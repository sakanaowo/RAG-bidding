================================================================================
SYSTEM ARCHITECTURE - RAG BIDDING SYSTEM v2.0
Generated: $(date '+%Y-%m-%d %H:%M:%S')
================================================================================

=== HIGH-LEVEL ARCHITECTURE ===

┌─────────────────────────────────────────────────────────────────────┐
│                         CLIENT LAYER                                 │
│  - Web Browser / Postman / cURL                                     │
│  - HTTP/REST API requests                                           │
└────────────────────────────────┬────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────┐
│                      API LAYER (FastAPI)                             │
│  Port: 8000                                                          │
│  Endpoints:                                                          │
│    - POST /ask (Question Answering)                                 │
│    - POST /api/upload/files (Document Upload)                       │
│    - GET  /api/upload/status (Processing Status)                    │
│    - GET  /health, /stats, /features                                │
│    - Chat Sessions (/api/chat/sessions/*)                           │
└────────────────────────────────┬────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    RAG PIPELINE ORCHESTRATION                        │
│                                                                      │
│  ┌──────────────────┐    ┌────────────────┐    ┌─────────────────┐│
│  │ Query Enhancement│ -> │ Vector Search  │ -> │   Reranking     ││
│  │  - Multi-Query   │    │  (PGVector)    │    │   (BGE-M3)      ││
│  │  - HyDE          │    │  - Cosine Sim  │    │   - Cross-Enc   ││
│  │  - Step-Back     │    │  - Top-K       │    │   - Singleton   ││
│  └──────────────────┘    └────────────────┘    └─────────────────┘│
│                                 │                                    │
│                                 ▼                                    │
│                         ┌─────────────────┐                         │
│                         │   LLM Answer    │                         │
│                         │  (GPT-4o-mini)  │                         │
│                         └─────────────────┘                         │
└─────────────────────────────────────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────┐
│                       STORAGE LAYER                                  │
│                                                                      │
│  ┌──────────────────┐    ┌────────────────┐    ┌─────────────────┐│
│  │   PostgreSQL 18  │    │   Redis Cache  │    │  In-Memory LRU  ││
│  │   + pgvector     │    │   (Optional)   │    │   (L1 Cache)    ││
│  │   7,892 chunks   │    │   - DB 0: Cache│    │   Max: 500 keys ││
│  │   149 MB         │    │   - DB 1: Sess │    │                 ││
│  └──────────────────┘    └────────────────┘    └─────────────────┘│
└─────────────────────────────────────────────────────────────────────┘

=== RAG PIPELINE MODES ===

MODE        | Enhancement | Reranking | Latency | Use Case
------------|-------------|-----------|---------|---------------------------
fast        | None        | No        | ~1s     | Quick lookups, high volume
balanced ⭐ | Multi-Query | BGE       | ~2-3s   | Default, production ready
            | + Step-Back |           |         |
quality     | All 4       | BGE       | ~3-5s   | Complex queries, accuracy
            | strategies  |           |         |
adaptive    | Dynamic     | BGE       | ~2-4s   | Auto-adjust based on query

Enhancement Strategies:
  1. Multi-Query: Generate 3 variations of original query
  2. HyDE (Hypothetical Document Embeddings): Generate ideal answer
  3. Step-Back: Generate broader context query
  4. Original: Keep original query

Fusion: RRF (Reciprocal Rank Fusion) for combining results

=== RERANKING ARCHITECTURE ===

Singleton Pattern (Production):
  - Model: BAAI/bge-reranker-v2-m3 (multilingual cross-encoder)
  - Device: Auto-detect (CUDA/CPU)
  - Batch Size: 32 (GPU) / 16 (CPU)
  - Lifecycle: Load once, reuse across requests
  - Memory: ~1.2GB model cache
  - Latency: ~100-150ms for 10 docs

Alternative (Development):
  - OpenAI Reranker (parallel API calls)
  - 8.38x faster with max_workers=10
  - Higher cost, no local model

=== CACHE ARCHITECTURE (3-TIER) ===

L1 Cache (In-Memory LRU):
  - Location: Python process memory
  - Technology: functools.lru_cache
  - Max Size: 500 queries
  - Hit Rate: ~40-60% (single-user)
  - Latency: <1ms
  - Persistence: No (lost on restart)

L2 Cache (Redis):
  - Location: Redis server (localhost:6379)
  - DB 0: Retrieval cache
  - DB 1: Chat sessions (TO BE DEPRECATED)
  - TTL: 3600s (1 hour)
  - Hit Rate: ~20-30% (multi-user)
  - Latency: ~5-10ms
  - Persistence: Optional (configurable)

L3 Cache (PostgreSQL):
  - Location: Native database storage
  - Always enabled (fallback)
  - No TTL (permanent)
  - Latency: ~50ms
  - Persistence: Yes

Cache Invalidation:
  - Automatic: TTL expiration
  - Manual: clear_cache() API
  - Triggers: Document updates, status changes

=== EMBEDDING ARCHITECTURE ===

Model: text-embedding-3-large (OpenAI)
  - Dimensions: 3,072 (native, no reduction)
  - Context Window: 8,191 tokens
  - Cost: $0.13 per 1M tokens
  - Latency: ~100-200ms per chunk

Storage: PGVector
  - Index: IVFFlat / HNSW (auto-selected)
  - Distance: Cosine similarity
  - Query Performance: ~50ms for 7,892 vectors
  - Precision: Float32

Chunking Strategy:
  - Size: 1,000 characters
  - Overlap: 200 characters
  - Preserves: Legal hierarchy, section structure
  - Metadata: 12 fields (JSONB)

=== DOCUMENT PROCESSING PIPELINE ===

Step 1: Upload & Classification
  - Input: DOCX/PDF files
  - Auto-classify: Law, Decree, Circular, Bidding
  - Background processing: Async with progress tracking

Step 2: Hierarchical Parsing
  - Extract: Chapters, Articles, Sections, Clauses
  - Preserve: Legal numbering, hierarchy
  - Handle: Tables, lists, special formatting

Step 3: Semantic Enrichment
  - NER: Laws, decrees, dates, organizations
  - Keywords: TF-IDF with legal term boosting
  - Concepts: Bidding terms, legal concepts

Step 4: Chunking
  - Method: Hierarchical (respect structure)
  - Output: UniversalChunk (JSONL)
  - Metadata: Rich context for retrieval

Step 5: Embedding & Storage
  - Generate: OpenAI embeddings (3,072-dim)
  - Store: PostgreSQL + pgvector
  - Index: Create vector index for fast search

=== DATABASE SCHEMA ===

documents ⭐ PRIMARY TABLE:
  - id (UUID, PK)
  - document_id (VARCHAR, UNIQUE) - Document identifier
  - document_name (TEXT) - Document name
  - category (VARCHAR) - Category (e.g., legal, bidding)
  - document_type (VARCHAR) - Type (law, decree, circular, bidding_form, etc.)
  - source_file (TEXT) - Source file path
  - file_name (TEXT) - Original filename
  - total_chunks (INT) - Number of chunks
  - status (VARCHAR) - active/expired (for visibility control)
  - created_at, updated_at (TIMESTAMP) - Timestamps
  Indexes:
    - document_id (UNIQUE)
    - category, document_type, status, source_file (B-tree for filtering)
  Current Data:
    - Total: 64 documents
    - Types: bidding_form (37), report_template (10), law (6), etc.

langchain_pg_embedding:
  - id (UUID, PK)
  - collection_id (UUID, FK to langchain_pg_collection)
  - embedding (VECTOR(3072)) - Vector embedding
  - document (TEXT) - Chunk content
  - cmetadata (JSONB) - Chunk metadata
    {
      "chunk_id": "doc_001_chunk_005",
      "document_id": "doc_001",
      "document_type": "Law",
      "hierarchy": "[\"Chapter 1\", \"Article 5\"]",
      "level": 2,
      "section_title": "Regulations on...",
      "char_count": 850,
      "chunk_index": 5,
      "total_chunks": 45,
      "is_complete_unit": true,
      "has_table": false,
      "has_list": true,
      "extra_metadata": "{...}"
    }
  Current Data:
    - Total: 7,892 chunks
    - Embeddings: 3,072 dimensions each

langchain_pg_collection (INTERNAL - LangChain use only):
  - id (UUID, PK)
  - name (VARCHAR) - Collection name ("docs")
  - cmetadata (JSONB) - Collection metadata
  Note: Internal table for LangChain PGVector
        Use 'documents' table for application-level document management

Indexes:
  - Primary: B-tree on id (all tables)
  - Vector: IVFFlat/HNSW on embedding (langchain_pg_embedding)
  - JSONB: GIN index on cmetadata (for metadata filtering)
  - Documents: B-tree on document_id, category, type, status

=== PERFORMANCE BENCHMARKS ===

Operation                  | Time      | Notes
---------------------------|-----------|--------------------------------
Document Processing        | ~0.35s    | Per file with enrichment
Chunk Import               | ~20/s     | With embedding generation
Vector Search (no cache)   | ~50ms     | 7,892 vectors
Vector Search (L1 hit)     | <1ms      | In-memory cache
Vector Search (L2 hit)     | ~5-10ms   | Redis cache
Reranking (BGE, 10 docs)   | ~150ms    | CPU mode
Full Pipeline (balanced)   | ~2-3s     | End-to-end
Full Pipeline (quality)    | ~3-5s     | With all enhancements

Concurrent Users (estimate):
  - No pooling (current): ~10 users
  - With pgBouncer: ~100+ users
  - With Redis cache: 2-3x improvement

=== PRODUCTION DEPLOYMENT CHECKLIST ===

Phase 1: Connection Pooling
  [ ] Install pgBouncer
  [ ] Configure /etc/pgbouncer/pgbouncer.ini
  [ ] Set USE_PGBOUNCER=True in feature_flags.py
  [ ] Monitor pool status

Phase 2: Cache Optimization
  [ ] Enable Redis cache (ENABLE_REDIS_CACHE=true)
  [ ] Configure Redis max memory (2GB)
  [ ] Monitor hit rates
  [ ] Test cache invalidation

Phase 3: Vector Index Optimization
  [ ] Create HNSW index (vs IVFFlat)
  [ ] Tune index parameters (m=16, ef_construction=64)
  [ ] Vacuum analyze tables
  [ ] Monitor query performance

Phase 4: Load Testing
  [ ] Test with 50+ concurrent users
  [ ] Monitor database connections
  [ ] Monitor Redis memory usage
  [ ] Check response times

Phase 5: Monitoring
  [ ] Set up /metrics endpoint
  [ ] Dashboard for pool/cache stats
  [ ] Alerts for high pool usage
  [ ] Log aggregation

=== SECURITY CONSIDERATIONS ===

API Security:
  - CORS: Configure allowed origins
  - Rate Limiting: Implement per-IP limits
  - Authentication: Add JWT/OAuth (TODO)
  - Input Validation: Sanitize all inputs

Database Security:
  - Connection: SSL/TLS for production
  - Credentials: Environment variables only
  - Permissions: Principle of least privilege
  - Backup: Regular automated backups

API Keys:
  - OpenAI: Store in .env, never commit
  - Rotate: Regular key rotation
  - Monitor: Usage and costs

=== MONITORING & OBSERVABILITY ===

Logs:
  - Location: logs/server-log.txt
  - Format: Structured (timestamp, level, message)
  - Rotation: Daily (TODO: implement)
  - Levels: DEBUG, INFO, WARNING, ERROR

Metrics (Available):
  - Query latency (by mode)
  - Cache hit rates (L1, L2, L3)
  - Database query times
  - Reranker performance
  - API endpoint usage

Health Checks:
  - /health: Database connectivity
  - /stats: System configuration
  - /features: Feature flags status

=== TECH STACK SUMMARY ===

Language & Framework:
  - Python 3.10.18
  - FastAPI 0.112.4
  - LangChain 0.3.x ecosystem

Database & Storage:
  - PostgreSQL 18
  - pgvector 0.8.1
  - Redis 7.0+ (optional)

AI/ML:
  - OpenAI API (embeddings + LLM)
  - BGE-reranker-v2-m3 (multilingual)
  - sentence-transformers 5.1.2
  - PyTorch 2.8.0

Dependencies:
  - psycopg 3.2.10 (async PostgreSQL driver)
  - uvicorn 0.30.6 (ASGI server)
  - pydantic 2.11.9 (data validation)

Development:
  - pytest (testing)
  - conda (environment management)
  - git (version control)

=== DIRECTORY STRUCTURE ===

RAG-bidding/
├── src/                    # Source code
│   ├── api/               # FastAPI endpoints
│   │   ├── main.py       # Main app
│   │   └── routers/      # Modular routes
│   ├── config/           # Configuration
│   │   ├── models.py     # Settings & presets
│   │   └── feature_flags.py
│   ├── embedding/        # Vector store
│   │   └── store/
│   ├── retrieval/        # Retrieval pipeline
│   │   ├── ranking/      # Rerankers (BGE)
│   │   ├── query_processing/
│   │   └── cached_retrieval.py
│   └── generation/       # LLM generation
│
├── scripts/               # Utility scripts
│   ├── bootstrap_db.py
│   ├── import_processed_chunks.py
│   ├── batch_reprocess_all.py
│   └── test/             # Test suite
│
├── data/                  # Data storage
│   ├── raw/              # Original documents
│   ├── processed/        # Enriched chunks
│   └── outputs/          # Reports
│
├── documents/             # Documentation
│   ├── setup/
│   ├── technical/
│   └── verification/
│
├── .env                   # Environment variables
├── environment.yaml       # Conda environment
├── setup_db.sh           # Database setup
└── start_server.sh       # Server startup

=== RELATED DOCUMENTATION ===

Setup:
  - SETUP_ENVIRONMENT_DATABASE.md (This file)
  - documents/setup/QUICK_SETUP.md
  - documents/setup/DATABASE_SETUP.md

Technical:
  - documents/technical/PIPELINE_INTEGRATION_SUMMARY.md
  - documents/technical/POOLING_CACHE_PLAN.md
  - documents/technical/CACHE_AND_HNSW_EXPLAINED.md

Development:
  - README.md (Project overview)
  - .github/copilot-instructions.md
  - scripts/test/README.md

================================================================================
END OF ARCHITECTURE SUMMARY
================================================================================
