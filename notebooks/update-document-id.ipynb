{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da654a89",
   "metadata": {},
   "source": [
    "# Document ID Migration - Option 4 (Hybrid System)\n",
    "\n",
    "**M·ª•c ƒë√≠ch:** Migrate document IDs t·ª´ format c≈© sang format m·ªõi theo Hybrid System\n",
    "\n",
    "**Format m·ªõi:** `{type_code}-{s·ªë_hi·ªáu}/{nƒÉm}#{hash_short}`\n",
    "\n",
    "**V√≠ d·ª•:**\n",
    "- `bidding_untitled` ‚Üí `FORM-Bidding/2025#bee720`\n",
    "- `circular_untitled` ‚Üí `TT-Circular/2025#3be8b6`\n",
    "- `decree_untitled` ‚Üí `ND-Decree/2025#95b863`\n",
    "\n",
    "**L·ª£i √≠ch:**\n",
    "- ‚úÖ Human-readable + Machine-friendly\n",
    "- ‚úÖ T∆∞∆°ng th√≠ch chu·∫©n ph√°p l√Ω VN\n",
    "- ‚úÖ ƒê·∫£m b·∫£o uniqueness v·ªõi hash\n",
    "- ‚úÖ D·ªÖ query v√† maintain\n",
    "\n",
    "---\n",
    "\n",
    "## Quy tr√¨nh Migration\n",
    "\n",
    "1. **Setup & Connect** - K·∫øt n·ªëi database\n",
    "2. **Preview** - Xem tr∆∞·ªõc thay ƒë·ªïi\n",
    "3. **Backup** - T·∫°o backup metadata\n",
    "4. **Execute** - Th·ª±c hi·ªán migration\n",
    "5. **Verify** - Ki·ªÉm tra k·∫øt qu·∫£\n",
    "6. **Test API** - Test v·ªõi API endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd1ed2",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80179355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "üìÅ Project root: /home/sakana/Code/RAG-bidding\n",
      "üîó Database: localhost:5432/rag_bidding_v2\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.config.models import settings\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üîó Database: {settings.database_url.split('@')[1] if '@' in settings.database_url else 'configured'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611f37c",
   "metadata": {},
   "source": [
    "## Step 2: Define Migration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ff9f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Migration functions defined\n"
     ]
    }
   ],
   "source": [
    "def extract_metadata_from_old_id(old_id: str, metadata: dict) -> dict:\n",
    "    \"\"\"Extract ho·∫∑c infer metadata t·ª´ old_id v√† cmetadata\"\"\"\n",
    "    \n",
    "    # L·∫•y document_type t·ª´ metadata ho·∫∑c old_id\n",
    "    doc_type = metadata.get(\"document_type\")\n",
    "    if not doc_type and old_id:\n",
    "        if \"bidding\" in old_id.lower():\n",
    "            doc_type = \"bidding\"\n",
    "        elif \"circular\" in old_id.lower():\n",
    "            doc_type = \"circular\"\n",
    "        elif \"decree\" in old_id.lower():\n",
    "            doc_type = \"decree\"\n",
    "        elif \"law\" in old_id.lower():\n",
    "            doc_type = \"law\"\n",
    "        elif \"exam\" in old_id.lower():\n",
    "            doc_type = \"exam\"\n",
    "        elif \"report\" in old_id.lower():\n",
    "            doc_type = \"report\"\n",
    "        else:\n",
    "            doc_type = \"document\"\n",
    "    \n",
    "    # L·∫•y year t·ª´ metadata ho·∫∑c timestamp\n",
    "    year = metadata.get(\"year\")\n",
    "    if not year and metadata.get(\"processing_metadata\"):\n",
    "        processed_at = metadata[\"processing_metadata\"].get(\"last_processed_at\", \"\")\n",
    "        if processed_at:\n",
    "            year = processed_at[:4]\n",
    "    if not year:\n",
    "        year = \"2025\"\n",
    "    \n",
    "    # L·∫•y number - d√πng type name thay v√¨ \"Unknown\"\n",
    "    number = metadata.get(\"number\")\n",
    "    if not number:\n",
    "        import re\n",
    "        match = re.search(r'(\\d+)', old_id)\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "        else:\n",
    "            number = doc_type.title()\n",
    "    \n",
    "    return {\n",
    "        \"type\": doc_type or \"doc\",\n",
    "        \"year\": year,\n",
    "        \"number\": number\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_new_document_id(old_id: str, metadata: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate new document_id theo Hybrid System\n",
    "    Format: {type_code}-{number}/{year}#{hash_short}\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted = extract_metadata_from_old_id(old_id, metadata)\n",
    "    doc_type = extracted[\"type\"]\n",
    "    year = extracted[\"year\"]\n",
    "    number = extracted[\"number\"]\n",
    "    \n",
    "    # Type code mapping\n",
    "    type_code_map = {\n",
    "        \"law\": \"LAW\",\n",
    "        \"decree\": \"ND\",\n",
    "        \"circular\": \"TT\",\n",
    "        \"decision\": \"QD\",\n",
    "        \"bidding\": \"FORM\",\n",
    "        \"report\": \"RPT\",\n",
    "        \"exam\": \"EXAM\",\n",
    "        \"document\": \"DOC\"\n",
    "    }\n",
    "    \n",
    "    type_code = type_code_map.get(doc_type, \"DOC\")\n",
    "    \n",
    "    # Generate hash t·ª´ old_id ƒë·ªÉ ƒë·∫£m b·∫£o uniqueness v√† idempotent\n",
    "    hash_obj = hashlib.md5(old_id.encode())\n",
    "    hash_short = hash_obj.hexdigest()[:6]\n",
    "    \n",
    "    new_id = f\"{type_code}-{number}/{year}#{hash_short}\"\n",
    "    \n",
    "    return new_id\n",
    "\n",
    "\n",
    "print(\"‚úÖ Migration functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c1b6e",
   "metadata": {},
   "source": [
    "## Step 3: Connect to Database & Preview Current Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7b2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to database...\n",
      "üîó Connection: localhost:5432/rag_bidding_v2\n",
      "\n",
      "üìä Found 5 documents with 4708 total chunks\n",
      "\n",
      "====================================================================================================\n",
      "OLD DOCUMENT_ID                NEW DOCUMENT_ID                TYPE     CHUNKS  \n",
      "====================================================================================================\n",
      "bidding_untitled               FORM-Bidding/2025#bee720       unknown  2831    \n",
      "circular_untitled              TT-Circular/2025#3be8b6        unknown  123     \n",
      "decision_untitled              DOC-Document/2025#787999       unknown  5       \n",
      "decree_untitled                ND-Decree/2025#95b863          unknown  595     \n",
      "law_untitled                   LAW-Law/2025#cd5116            unknown  1154    \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Preview complete: 5 documents ready for migration\n",
      "üîå Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "# Use database_url from settings (convert SQLAlchemy format to psycopg format)\n",
    "conn_str = settings.database_url.replace(\"postgresql+psycopg://\", \"postgresql://\")\n",
    "\n",
    "print(\"üîå Connecting to database...\")\n",
    "print(f\"üîó Connection: {conn_str.split('@')[1] if '@' in conn_str else 'configured'}\")\n",
    "conn = psycopg.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query all unique document_ids with their chunk counts\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    cmetadata->>'document_id' as document_id,\n",
    "    COUNT(*) as chunk_count,\n",
    "    MIN(cmetadata->>'doc_type') as doc_type,\n",
    "    MIN(cmetadata->>'processing_metadata') as processing_metadata_sample\n",
    "FROM langchain_pg_embedding\n",
    "WHERE cmetadata->>'document_id' IS NOT NULL\n",
    "GROUP BY cmetadata->>'document_id'\n",
    "ORDER BY document_id;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "results = cursor.fetchall()\n",
    "\n",
    "print(f\"\\nüìä Found {len(results)} documents with {sum(r[1] for r in results)} total chunks\\n\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'OLD DOCUMENT_ID':<30} {'NEW DOCUMENT_ID':<30} {'TYPE':<8} {'CHUNKS':<8}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "migration_preview = []\n",
    "for old_id, chunk_count, doc_type, processing_metadata in results:\n",
    "    # Parse processing_metadata if it exists\n",
    "    metadata = {}\n",
    "    if processing_metadata:\n",
    "        try:\n",
    "            import json\n",
    "            metadata = json.loads(processing_metadata)\n",
    "        except:\n",
    "            metadata = {}\n",
    "    \n",
    "    # Generate new document_id\n",
    "    new_id = generate_new_document_id(old_id, metadata)\n",
    "    \n",
    "    # Determine doc type for display\n",
    "    inferred_meta = extract_metadata_from_old_id(old_id, metadata)\n",
    "    display_type = inferred_meta.get('doc_type', 'unknown')\n",
    "    \n",
    "    print(f\"{old_id:<30} {new_id:<30} {display_type:<8} {chunk_count:<8}\")\n",
    "    \n",
    "    migration_preview.append({\n",
    "        'old_id': old_id,\n",
    "        'new_id': new_id,\n",
    "        'doc_type': display_type,\n",
    "        'chunk_count': chunk_count\n",
    "    })\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n‚úÖ Preview complete: {len(migration_preview)} documents ready for migration\")\n",
    "\n",
    "# Close connection (we'll reopen for migration)\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"üîå Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a2ab5",
   "metadata": {},
   "source": [
    "## Step 4: Backup Current Data\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Tr∆∞·ªõc khi migrate, ch√∫ng ta s·∫Ω backup to√†n b·ªô cmetadata hi·ªán t·∫°i ƒë·ªÉ c√≥ th·ªÉ rollback n·∫øu c·∫ßn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56cdf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Creating backup at: /home/sakana/Code/RAG-bidding/notebooks/backups/document_id_backup_20251109_154532.json\n",
      "‚úÖ Backup created: 4708 chunks saved\n",
      "üìÅ Backup location: /home/sakana/Code/RAG-bidding/notebooks/backups/document_id_backup_20251109_154532.json\n",
      "\n",
      "‚ö†Ô∏è  To rollback, run this SQL:\n",
      "-- Restore from backup: document_id_backup_20251109_154532.json\n",
      "-- UPDATE langchain_pg_embedding SET cmetadata = backup_cmetadata WHERE id = backup_id\n",
      "‚úÖ Backup created: 4708 chunks saved\n",
      "üìÅ Backup location: /home/sakana/Code/RAG-bidding/notebooks/backups/document_id_backup_20251109_154532.json\n",
      "\n",
      "‚ö†Ô∏è  To rollback, run this SQL:\n",
      "-- Restore from backup: document_id_backup_20251109_154532.json\n",
      "-- UPDATE langchain_pg_embedding SET cmetadata = backup_cmetadata WHERE id = backup_id\n"
     ]
    }
   ],
   "source": [
    "# Create backup directory (s·ª≠ d·ª•ng Path.cwd() thay v√¨ __file__ trong notebook)\n",
    "backup_dir = Path.cwd() / \"backups\"\n",
    "backup_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate backup filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_file = backup_dir / f\"document_id_backup_{timestamp}.json\"\n",
    "\n",
    "print(f\"üíæ Creating backup at: {backup_file}\")\n",
    "\n",
    "# Connect and fetch all metadata\n",
    "conn = psycopg.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "backup_query = \"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    cmetadata->>'document_id' as document_id,\n",
    "    cmetadata\n",
    "FROM langchain_pg_embedding\n",
    "WHERE cmetadata->>'document_id' IS NOT NULL\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(backup_query)\n",
    "backup_data = cursor.fetchall()\n",
    "\n",
    "# Save to JSON\n",
    "backup_records = []\n",
    "for row_id, doc_id, cmetadata in backup_data:\n",
    "    backup_records.append({\n",
    "        'id': row_id,\n",
    "        'document_id': doc_id,\n",
    "        'cmetadata': cmetadata\n",
    "    })\n",
    "\n",
    "with open(backup_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'timestamp': timestamp,\n",
    "        'total_chunks': len(backup_records),\n",
    "        'records': backup_records\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"‚úÖ Backup created: {len(backup_records)} chunks saved\")\n",
    "print(f\"üìÅ Backup location: {backup_file}\")\n",
    "print(\"\\n‚ö†Ô∏è  To rollback, run this SQL:\")\n",
    "print(f\"-- Restore from backup: {backup_file.name}\")\n",
    "print(\"-- UPDATE langchain_pg_embedding SET cmetadata = backup_cmetadata WHERE id = backup_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0baa95",
   "metadata": {},
   "source": [
    "## Step 5: Execute Migration\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL:** ƒê√¢y l√† b∆∞·ªõc thay ƒë·ªïi database th·∫≠t. H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£:\n",
    "- ‚úÖ Review preview ·ªü Step 3\n",
    "- ‚úÖ ƒê√£ c√≥ backup ·ªü Step 4\n",
    "- ‚úÖ S·∫µn s√†ng th·ª±c hi·ªán migration\n",
    "\n",
    "**ƒê·ªÉ th·ª±c hi·ªán migration, set `CONFIRM_MIGRATION = True` trong cell d∆∞·ªõi.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5260d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting migration...\n",
      "\n",
      "üìù Migrating: bidding_untitled ‚Üí FORM-Bidding/2025#bee720 (2831 chunks)...\n",
      "   ‚úÖ Updated 2831 chunks\n",
      "üìù Migrating: circular_untitled ‚Üí TT-Circular/2025#3be8b6 (123 chunks)...\n",
      "   ‚úÖ Updated 123 chunks\n",
      "üìù Migrating: decision_untitled ‚Üí DOC-Document/2025#787999 (5 chunks)...\n",
      "   ‚úÖ Updated 5 chunks\n",
      "üìù Migrating: decree_untitled ‚Üí ND-Decree/2025#95b863 (595 chunks)...\n",
      "   ‚úÖ Updated 595 chunks\n",
      "üìù Migrating: law_untitled ‚Üí LAW-Law/2025#cd5116 (1154 chunks)...\n",
      "   ‚úÖ Updated 1154 chunks\n",
      "\n",
      "üéâ Migration complete! Total chunks updated: 4708\n",
      "‚úÖ All changes committed to database\n",
      "üîå Database connection closed\n",
      "   ‚úÖ Updated 2831 chunks\n",
      "üìù Migrating: circular_untitled ‚Üí TT-Circular/2025#3be8b6 (123 chunks)...\n",
      "   ‚úÖ Updated 123 chunks\n",
      "üìù Migrating: decision_untitled ‚Üí DOC-Document/2025#787999 (5 chunks)...\n",
      "   ‚úÖ Updated 5 chunks\n",
      "üìù Migrating: decree_untitled ‚Üí ND-Decree/2025#95b863 (595 chunks)...\n",
      "   ‚úÖ Updated 595 chunks\n",
      "üìù Migrating: law_untitled ‚Üí LAW-Law/2025#cd5116 (1154 chunks)...\n",
      "   ‚úÖ Updated 1154 chunks\n",
      "\n",
      "üéâ Migration complete! Total chunks updated: 4708\n",
      "‚úÖ All changes committed to database\n",
      "üîå Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è  SET THIS TO True TO CONFIRM MIGRATION\n",
    "CONFIRM_MIGRATION = True\n",
    "\n",
    "if not CONFIRM_MIGRATION:\n",
    "    print(\"‚ùå Migration NOT confirmed. Set CONFIRM_MIGRATION = True to proceed.\")\n",
    "else:\n",
    "    print(\"üöÄ Starting migration...\\n\")\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = psycopg.connect(conn_str)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        total_updated = 0\n",
    "        \n",
    "        for item in migration_preview:\n",
    "            old_id = item['old_id']\n",
    "            new_id = item['new_id']\n",
    "            chunk_count = item['chunk_count']\n",
    "            \n",
    "            print(f\"üìù Migrating: {old_id} ‚Üí {new_id} ({chunk_count} chunks)...\")\n",
    "            \n",
    "            # Update document_id in cmetadata\n",
    "            update_query = \"\"\"\n",
    "            UPDATE langchain_pg_embedding\n",
    "            SET cmetadata = jsonb_set(\n",
    "                cmetadata, \n",
    "                '{document_id}', \n",
    "                to_jsonb(%s::text)\n",
    "            )\n",
    "            WHERE cmetadata->>'document_id' = %s;\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(update_query, (new_id, old_id))\n",
    "            updated_count = cursor.rowcount\n",
    "            total_updated += updated_count\n",
    "            \n",
    "            print(f\"   ‚úÖ Updated {updated_count} chunks\")\n",
    "        \n",
    "        # Commit all changes\n",
    "        conn.commit()\n",
    "        print(f\"\\nüéâ Migration complete! Total chunks updated: {total_updated}\")\n",
    "        print(f\"‚úÖ All changes committed to database\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"\\n‚ùå Error during migration: {e}\")\n",
    "        print(\"üîÑ All changes rolled back\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"üîå Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e0a74",
   "metadata": {},
   "source": [
    "## Step 6: Verify Migration Results\n",
    "\n",
    "Ki·ªÉm tra xem migration ƒë√£ th√†nh c√¥ng ch∆∞a b·∫±ng c√°ch query database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6aa43e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying migration results...\n",
      "\n",
      "üìä Current database state: 5 documents\n",
      "\n",
      "================================================================================\n",
      "DOCUMENT_ID                         TYPE       CHUNKS    \n",
      "================================================================================\n",
      "DOC-Document/2025#787999            unknown    5         \n",
      "FORM-Bidding/2025#bee720            unknown    2831      \n",
      "LAW-Law/2025#cd5116                 unknown    1154      \n",
      "ND-Decree/2025#95b863               unknown    595       \n",
      "TT-Circular/2025#3be8b6             unknown    123       \n",
      "================================================================================\n",
      "Total: 4708 chunks across 5 documents\n",
      "\n",
      "‚úÖ All document_ids successfully migrated to new format!\n",
      "\n",
      "üìã Sample chunks (first 5):\n",
      "================================================================================\n",
      "ID: 4af1c105-4ce2-43a6-b304-244a6d16ee15 | Doc: FORM-Bidding/2025#bee720 | Type: None | Chunk: 102\n",
      "ID: 82d0d255-4401-470d-9805-02ffe2a677a9 | Doc: FORM-Bidding/2025#bee720 | Type: None | Chunk: 66\n",
      "ID: 748f9e32-7127-43c4-b575-71cea7d40e78 | Doc: FORM-Bidding/2025#bee720 | Type: None | Chunk: 46\n",
      "ID: 8e742f0e-8a04-4cfe-b7a7-2548a93179fa | Doc: FORM-Bidding/2025#bee720 | Type: None | Chunk: 1\n",
      "ID: ca5d8bad-ed7f-475b-ab69-44a317a611b1 | Doc: FORM-Bidding/2025#bee720 | Type: None | Chunk: 67\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Verification complete!\n"
     ]
    }
   ],
   "source": [
    "# Connect and verify\n",
    "conn = psycopg.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"üîç Verifying migration results...\\n\")\n",
    "\n",
    "# Check all document_ids now in database\n",
    "verify_query = \"\"\"\n",
    "SELECT \n",
    "    cmetadata->>'document_id' as document_id,\n",
    "    COUNT(*) as chunk_count,\n",
    "    MIN(cmetadata->>'doc_type') as doc_type\n",
    "FROM langchain_pg_embedding\n",
    "WHERE cmetadata->>'document_id' IS NOT NULL\n",
    "GROUP BY cmetadata->>'document_id'\n",
    "ORDER BY document_id;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(verify_query)\n",
    "results = cursor.fetchall()\n",
    "\n",
    "print(f\"üìä Current database state: {len(results)} documents\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'DOCUMENT_ID':<35} {'TYPE':<10} {'CHUNKS':<10}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_chunks = 0\n",
    "for doc_id, chunk_count, doc_type in results:\n",
    "    print(f\"{doc_id:<35} {doc_type or 'unknown':<10} {chunk_count:<10}\")\n",
    "    total_chunks += chunk_count\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total: {total_chunks} chunks across {len(results)} documents\\n\")\n",
    "\n",
    "# Check for any old-format IDs remaining\n",
    "check_old_format = \"\"\"\n",
    "SELECT COUNT(*) \n",
    "FROM langchain_pg_embedding\n",
    "WHERE cmetadata->>'document_id' LIKE '%_untitled'\n",
    "   OR cmetadata->>'document_id' NOT LIKE '%#%';\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(check_old_format)\n",
    "old_format_count = cursor.fetchone()[0]\n",
    "\n",
    "if old_format_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Warning: {old_format_count} chunks still have old-format document_ids\")\n",
    "else:\n",
    "    print(\"‚úÖ All document_ids successfully migrated to new format!\")\n",
    "\n",
    "# Sample some chunks to verify metadata integrity\n",
    "sample_query = \"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    cmetadata->>'document_id' as document_id,\n",
    "    cmetadata->>'doc_type' as doc_type,\n",
    "    cmetadata->>'chunk_index' as chunk_index\n",
    "FROM langchain_pg_embedding\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(sample_query)\n",
    "samples = cursor.fetchall()\n",
    "\n",
    "print(\"\\nüìã Sample chunks (first 5):\")\n",
    "print(\"=\" * 80)\n",
    "for row_id, doc_id, doc_type, chunk_idx in samples:\n",
    "    print(f\"ID: {row_id} | Doc: {doc_id} | Type: {doc_type} | Chunk: {chunk_idx}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n‚úÖ Verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32556e",
   "metadata": {},
   "source": [
    "## Step 7: Test API with New Document IDs\n",
    "\n",
    "Ki·ªÉm tra API endpoints ho·∫°t ƒë·ªông ƒë√∫ng v·ªõi document_id format m·ªõi.\n",
    "\n",
    "**L∆∞u √Ω:** Document IDs c√≥ k√Ω t·ª± `#` c·∫ßn ƒë∆∞·ª£c URL-encode khi g·ªçi API:\n",
    "- `ND-43/2022#a7f3c9` ‚Üí `ND-43/2022%23a7f3c9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c12ae628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing API with new document IDs...\n",
      "\n",
      "üìù Testing with document: FORM-Bidding/2025#bee720\n",
      "   (Originally: bidding_untitled)\n",
      "   Type: unknown, Chunks: 2831\n",
      "\n",
      "üîó URL-encoded: FORM-Bidding%2F2025%23bee720\n",
      "\n",
      "================================================================================\n",
      "TEST 1: GET /api/document-status?document_id={document_id}\n",
      "================================================================================\n",
      "Status Code: 200\n",
      "‚úÖ GET request successful!\n",
      "Response: {\n",
      "  \"document_id\": \"FORM-Bidding/2025#bee720\",\n",
      "  \"current_status\": \"active\",\n",
      "  \"chunk_count\": 2831,\n",
      "  \"last_updated\": \"2025-11-09T15:46:40.002664\",\n",
      "  \"superseded_by\": \"bidding_new_2024\"\n",
      "}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST 2: POST /api/document-status/update\n",
      "================================================================================\n",
      "Status Code: 200\n",
      "‚úÖ POST request successful!\n",
      "Response: {\n",
      "  \"success\": true,\n",
      "  \"message\": \"ƒê√£ c·∫≠p nh·∫≠t status cho 2831 chunks\",\n",
      "  \"document_id\": \"FORM-Bidding/2025#bee720\",\n",
      "  \"chunks_updated\": 2831,\n",
      "  \"old_status\": \"active\",\n",
      "  \"new_status\": \"active\"\n",
      "}\n",
      "================================================================================\n",
      "\n",
      "‚úÖ API testing complete!\n",
      "Status Code: 200\n",
      "‚úÖ POST request successful!\n",
      "Response: {\n",
      "  \"success\": true,\n",
      "  \"message\": \"ƒê√£ c·∫≠p nh·∫≠t status cho 2831 chunks\",\n",
      "  \"document_id\": \"FORM-Bidding/2025#bee720\",\n",
      "  \"chunks_updated\": 2831,\n",
      "  \"old_status\": \"active\",\n",
      "  \"new_status\": \"active\"\n",
      "}\n",
      "================================================================================\n",
      "\n",
      "‚úÖ API testing complete!\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "# Assume API is running at localhost:8000\n",
    "API_BASE_URL = \"http://localhost:8000/api\"\n",
    "\n",
    "print(\"üß™ Testing API with new document IDs...\\n\")\n",
    "\n",
    "# Get a sample new document_id from migration_preview\n",
    "if migration_preview:\n",
    "    sample_doc = migration_preview[0]\n",
    "    test_doc_id = sample_doc['new_id']\n",
    "    \n",
    "    print(f\"üìù Testing with document: {test_doc_id}\")\n",
    "    print(f\"   (Originally: {sample_doc['old_id']})\")\n",
    "    print(f\"   Type: {sample_doc['doc_type']}, Chunks: {sample_doc['chunk_count']}\\n\")\n",
    "    \n",
    "    # URL encode the document_id (important for # and / symbols)\n",
    "    encoded_doc_id = urllib.parse.quote(test_doc_id, safe='')\n",
    "    \n",
    "    print(f\"üîó URL-encoded: {encoded_doc_id}\\n\")\n",
    "    \n",
    "    # Test 1: GET document status (using query parameter)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TEST 1: GET /api/document-status?document_id={document_id}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{API_BASE_URL}/document-status\",\n",
    "            params={\"document_id\": test_doc_id}  # Use query param, requests will encode\n",
    "        )\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\"‚úÖ GET request successful!\")\n",
    "            print(f\"Response: {json.dumps(data, indent=2, ensure_ascii=False)}\")\n",
    "        else:\n",
    "            print(f\"‚ùå GET request failed: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calling API: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Make sure the API server is running: uvicorn src.api.main:app --reload\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Test 2: POST update document status\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TEST 2: POST /api/document-status/update\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        update_data = {\n",
    "            \"document_id\": test_doc_id,  # Don't encode in JSON body\n",
    "            \"new_status\": \"active\",\n",
    "            \"reason\": \"Testing new document_id format after migration\",\n",
    "            \"notes\": \"Migration successful - testing API compatibility\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{API_BASE_URL}/document-status/update\",\n",
    "            json=update_data\n",
    "        )\n",
    "        \n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\"‚úÖ POST request successful!\")\n",
    "            print(f\"Response: {json.dumps(data, indent=2, ensure_ascii=False)}\")\n",
    "        else:\n",
    "            print(f\"‚ùå POST request failed: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calling API: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Make sure the API server is running: uvicorn src.api.main:app --reload\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚úÖ API testing complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No migration preview data available. Run Step 3 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed476a",
   "metadata": {},
   "source": [
    "## üéâ Migration Complete!\n",
    "\n",
    "### T√≥m t·∫Øt quy tr√¨nh:\n",
    "\n",
    "1. ‚úÖ **Setup & Import** - ƒê√£ load c√°c th∆∞ vi·ªán v√† k·∫øt n·ªëi database\n",
    "2. ‚úÖ **Define Functions** - ƒê√£ t·∫°o c√°c h√†m migration logic\n",
    "3. ‚úÖ **Preview** - ƒê√£ xem tr∆∞·ªõc c√°c thay ƒë·ªïi (5 documents ‚Üí 4708 chunks)\n",
    "4. ‚úÖ **Backup** - ƒê√£ backup to√†n b·ªô metadata hi·ªán t·∫°i\n",
    "5. ‚úÖ **Execute** - ƒê√£ th·ª±c hi·ªán migration (n·∫øu CONFIRM_MIGRATION = True)\n",
    "6. ‚úÖ **Verify** - ƒê√£ ki·ªÉm tra k·∫øt qu·∫£ migration\n",
    "7. ‚úÖ **Test API** - ƒê√£ test API v·ªõi document_id m·ªõi\n",
    "\n",
    "### Document ID Format m·ªõi:\n",
    "\n",
    "```\n",
    "{TYPE_CODE}-{Number}/{Year}#{Hash}\n",
    "```\n",
    "\n",
    "**V√≠ d·ª•:**\n",
    "- `ND-43/2022#a7f3c9` - Ngh·ªã ƒë·ªãnh 43/2022\n",
    "- `TT-15/2023#65aabb` - Th√¥ng t∆∞ 15/2023\n",
    "- `LAW-Law/2025#cd5116` - Lu·∫≠t (kh√¥ng c√≥ s·ªë c·ª• th·ªÉ)\n",
    "\n",
    "### L∆∞u √Ω ti·∫øp theo:\n",
    "\n",
    "1. **C·∫≠p nh·∫≠t preprocessing pipeline** ƒë·ªÉ s·ª≠ d·ª•ng `DocumentIDGenerator` cho c√°c vƒÉn b·∫£n m·ªõi\n",
    "2. **Update documentation** v·ªÅ format document_id m·ªõi\n",
    "3. **Th√¥ng b√°o team** v·ªÅ s·ª± thay ƒë·ªïi n√†y\n",
    "4. **Monitor API** ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≥ issues v·ªõi URL encoding (#)\n",
    "\n",
    "### Rollback (n·∫øu c·∫ßn):\n",
    "\n",
    "N·∫øu c·∫ßn quay l·∫°i document_id c≈©, s·ª≠ d·ª•ng file backup trong `notebooks/backups/` v√† ch·∫°y:\n",
    "\n",
    "```python\n",
    "# Load backup file\n",
    "with open('backups/document_id_backup_TIMESTAMP.json', 'r') as f:\n",
    "    backup = json.load(f)\n",
    "\n",
    "# Restore each record\n",
    "for record in backup['records']:\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE langchain_pg_embedding\n",
    "        SET cmetadata = %s\n",
    "        WHERE id = %s\n",
    "    \"\"\", (json.dumps(record['cmetadata']), record['id']))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
