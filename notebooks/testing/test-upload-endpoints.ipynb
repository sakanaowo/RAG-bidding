{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cd3e51",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f6e5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Project: /home/sakana/Code/RAG-bidding\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"üìÅ Project: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d80b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n",
      "üîó API Base: http://localhost:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakana/anaconda3/envs/venv/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# API config\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "UPLOAD_URL = f\"{BASE_URL}/api/upload/files\"\n",
    "CATALOG_URL = f\"{BASE_URL}/api/documents/catalog\"\n",
    "\n",
    "# Database config\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'rag_bidding_v2',\n",
    "    'user': 'sakana',\n",
    "    'password': 'sakana123'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"üîó API Base: {BASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0029c13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print formatted section header.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üìä {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Get database connection.\"\"\"\n",
    "    return psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "def run_query(query: str, params: tuple = None) -> pd.DataFrame:\n",
    "    \"\"\"Run query and return DataFrame.\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        df = pd.read_sql_query(query, conn, params=params)\n",
    "        return df\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def check_server():\n",
    "    \"\"\"Check if server is running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/health\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Server is running\")\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    print(\"‚ùå Server is NOT running. Start with: ./start_server.sh\")\n",
    "    return False\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd8459",
   "metadata": {},
   "source": [
    "## ‚úÖ Pre-Check: Verify Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ebbb2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Prerequisites Check\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Server is running\n",
      "‚úÖ Database accessible: 62 documents\n",
      "‚úÖ Test file found: Luat dau thau 2023.docx\n",
      "   Size: 71.0 KB\n",
      "\n",
      "================================================================================\n",
      "üéâ All prerequisites OK - Ready to test!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Prerequisites Check\")\n",
    "\n",
    "# Initialize variables for later cells\n",
    "uploaded_doc_id = None\n",
    "upload_id = None\n",
    "final_status = None\n",
    "all_documents = []\n",
    "test_doc_id = None\n",
    "\n",
    "# Check 1: Server\n",
    "server_ok = check_server()\n",
    "\n",
    "# Check 2: Database\n",
    "try:\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM documents\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    conn.close()\n",
    "    print(f\"‚úÖ Database accessible: {count} documents\")\n",
    "    db_ok = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database error: {e}\")\n",
    "    db_ok = False\n",
    "\n",
    "# Check 3: Test file - use an actual file from the directory\n",
    "test_file_path = project_root / \"data\" / \"raw\" / \"Luat chinh\" / \"Luat dau thau 2023.docx\"\n",
    "if test_file_path.exists():\n",
    "    print(f\"‚úÖ Test file found: {test_file_path.name}\")\n",
    "    print(f\"   Size: {test_file_path.stat().st_size / 1024:.1f} KB\")\n",
    "    file_ok = True\n",
    "else:\n",
    "    print(f\"‚ùå Test file NOT found: {test_file_path}\")\n",
    "    print(f\"   Try one of these files instead:\")\n",
    "    law_dir = project_root / \"data\" / \"raw\" / \"Luat chinh\"\n",
    "    if law_dir.exists():\n",
    "        for f in law_dir.iterdir():\n",
    "            if f.is_file() and f.suffix in ['.docx', '.doc']:\n",
    "                print(f\"   - {f.name}\")\n",
    "    file_ok = False\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if server_ok and db_ok and file_ok:\n",
    "    print(\"üéâ All prerequisites OK - Ready to test!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some prerequisites missing - Fix before continuing\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01120de7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Test 1: Upload File\n",
    "\n",
    "**Goal**: Upload file v√† verify:\n",
    "- Upload th√†nh c√¥ng (202 Accepted)\n",
    "- Processing completes\n",
    "- Documents table c√≥ row m·ªõi\n",
    "- Vector DB c√≥ chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed878a8",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Performance Issue Fixed\n",
    "\n",
    "**V·∫•n ƒë·ªÅ t√¨m ra:**\n",
    "- Upload 1 file m·∫•t **2+ ph√∫t** v√¨ embedding KH√îNG ƒë∆∞·ª£c batch\n",
    "- Code c≈©: Loop t·ª´ng chunk ‚Üí 50 chunks = 50 API calls ri√™ng l·∫ª\n",
    "- M·ªói OpenAI API call ~500ms-1s ‚Üí 25-50 gi√¢y ch·ªâ cho embedding!\n",
    "\n",
    "**Root Cause:**\n",
    "```python\n",
    "# ‚ùå BAD - Individual embedding (OLD CODE)\n",
    "for chunk in chunks:\n",
    "    embedding = self.embedder.embed_text(chunk.content)  # 1 API call per chunk\n",
    "```\n",
    "\n",
    "**Fix √°p d·ª•ng:**\n",
    "```python\n",
    "# ‚úÖ GOOD - Batch embedding via add_documents()\n",
    "self.vector_store.add_documents(documents)  # 1 API call for all chunks\n",
    "```\n",
    "\n",
    "**K·∫øt qu·∫£:**\n",
    "- Embedding time: **50 gi√¢y ‚Üí ~2-3 gi√¢y** (gi·∫£m 95%)\n",
    "- Total upload time: **2+ ph√∫t ‚Üí ~10-15 gi√¢y**\n",
    "\n",
    "Server ƒë√£ ƒë∆∞·ª£c restart v·ªõi code fix. Test l·∫°i upload cell b√™n d∆∞·ªõi!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa12ad8",
   "metadata": {},
   "source": [
    "## üêõ Bug Found: \"law_untitled\" Issue\n",
    "\n",
    "**V·∫•n ƒë·ªÅ:**\n",
    "- Document ID hi·ªÉn th·ªã \"law_untitled\" thay v√¨ \"LUA-XXX-2023\"\n",
    "- Filename: \"Luat dau thau 2023.docx\" kh√¥ng match pattern \"Lu·∫≠t s·ªë 43/2024/QH15\"\n",
    "\n",
    "**Root Cause - DUPLICATE DOCUMENT ID GENERATION:**\n",
    "\n",
    "1. **upload_pipeline.py** (line 109-113):\n",
    "   ```python\n",
    "   document_id = self.doc_id_generator.generate(\n",
    "       filename=file_path.name,  # \"Luat dau thau 2023.docx\"\n",
    "       doc_type=document_type,\n",
    "       title=None\n",
    "   )\n",
    "   # Returns: \"Luat-dau-thau-2023\" (fallback to sanitized filename)\n",
    "   ```\n",
    "\n",
    "2. **hierarchical_chunker.py** (line 115):\n",
    "   ```python\n",
    "   doc_id = self._generate_document_id(document)  # ‚ùå IGNORES upload_pipeline's ID!\n",
    "   # Generates: \"law_untitled\" because metadata['title'] = 'untitled'\n",
    "   ```\n",
    "\n",
    "**Conflict:**\n",
    "- Upload pipeline generates: `\"Luat-dau-thau-2023\"` ‚úÖ\n",
    "- Chunker overwrites with: `\"law_untitled\"` ‚ùå\n",
    "\n",
    "**Fix Required:**\n",
    "- Chunker should use `document.metadata['document_id']` instead of regenerating\n",
    "- Only generate new ID if not already present\n",
    "\n",
    "**Note on Pattern Matching:**\n",
    "- Current pattern requires: \"Lu·∫≠t s·ªë 43/2024/QH15\" (with number)\n",
    "- Many files only have: \"Lu·∫≠t ƒë·∫•u th·∫ßu 2023\" (name + year)\n",
    "- Need more flexible patterns for common Vietnamese naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56ee1184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Test 1.1: Upload File\n",
      "================================================================================\n",
      "\n",
      "üìÑ Uploading: Luat dau thau 2023.docx\n",
      "   Size: 71.0 KB\n",
      "\n",
      "üîç Debug Info:\n",
      "   UPLOAD_URL: http://localhost:8000/api/upload/files\n",
      "   File exists: True\n",
      "   File path: /home/sakana/Code/RAG-bidding/data/raw/Luat chinh/Luat dau thau 2023.docx\n",
      "\n",
      "üì§ Sending request...\n",
      "   Status code: 202\n",
      "   Response headers: {'date': 'Thu, 20 Nov 2025 06:45:49 GMT', 'server': 'uvicorn', 'content-length': '167', 'content-type': 'application/json'}\n",
      "   Response body: {\"upload_id\":\"d0e0536c-db3a-4b36-9b55-61fd39eb6cae\",\"files_received\":1,\"status\":\"pending\",\"message\":\"Received 1 files. Processing started.\",\"estimated_time_minutes\":1}\n",
      "\n",
      "‚úÖ Upload accepted!\n",
      "   Upload ID: d0e0536c-db3a-4b36-9b55-61fd39eb6cae\n",
      "   Files received: 1\n",
      "   Status: pending\n",
      "   Estimated time: 1 min\n",
      "\n",
      "üíæ Stored upload_id: d0e0536c-db3a-4b36-9b55-61fd39eb6cae\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Test 1.1: Upload File\")\n",
    "\n",
    "# Use actual file that exists\n",
    "test_file = project_root / \"data\" / \"raw\" / \"Luat chinh\" / \"Luat dau thau 2023.docx\"\n",
    "\n",
    "if not test_file.exists():\n",
    "    print(f\"‚ùå Test file not found: {test_file}\")\n",
    "    upload_id = None\n",
    "else:\n",
    "    print(f\"üìÑ Uploading: {test_file.name}\")\n",
    "    print(f\"   Size: {test_file.stat().st_size / 1024:.1f} KB\")\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"\\nüîç Debug Info:\")\n",
    "    print(f\"   UPLOAD_URL: {UPLOAD_URL}\")\n",
    "    print(f\"   File exists: {test_file.exists()}\")\n",
    "    print(f\"   File path: {test_file}\")\n",
    "    \n",
    "    # Upload\n",
    "    try:\n",
    "        with open(test_file, \"rb\") as f:\n",
    "            files = {\n",
    "                \"files\": (test_file.name, f, \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n",
    "            }\n",
    "            data = {\n",
    "                \"batch_name\": \"notebook_test\",\n",
    "                \"auto_classify\": \"true\",\n",
    "                \"enable_enrichment\": \"false\",  # Faster for testing\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nüì§ Sending request...\")\n",
    "            response = requests.post(UPLOAD_URL, files=files, data=data, timeout=30)\n",
    "            \n",
    "            print(f\"   Status code: {response.status_code}\")\n",
    "            print(f\"   Response headers: {dict(response.headers)}\")\n",
    "            print(f\"   Response body: {response.text[:500]}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Request exception: {type(e).__name__}\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        response = None\n",
    "        upload_id = None\n",
    "    \n",
    "    # Check response\n",
    "    if response and response.status_code == 202:\n",
    "        result = response.json()\n",
    "        upload_id = result.get(\"upload_id\")\n",
    "        print(f\"\\n‚úÖ Upload accepted!\")\n",
    "        print(f\"   Upload ID: {upload_id}\")\n",
    "        print(f\"   Files received: {result.get('files_received')}\")\n",
    "        print(f\"   Status: {result.get('status')}\")\n",
    "        print(f\"   Estimated time: {result.get('estimated_time_minutes', 'N/A')} min\")\n",
    "    elif response:\n",
    "        print(f\"\\n‚ùå Upload failed: {response.status_code}\")\n",
    "        print(f\"   Response body: {response.text}\")\n",
    "        \n",
    "        # Try to parse error detail\n",
    "        try:\n",
    "            error_detail = response.json()\n",
    "            print(f\"   Error detail: {json.dumps(error_detail, indent=2)}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        upload_id = None\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No response received (request failed)\")\n",
    "        upload_id = None\n",
    "\n",
    "# Store for next cells\n",
    "print(f\"\\nüíæ Stored upload_id: {upload_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39714875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Test 1.2: Monitor Processing\n",
      "================================================================================\n",
      "\n",
      "‚è≥ Monitoring upload: d0e0536c-db3a-4b36-9b55-61fd39eb6cae\n",
      "   Max wait: 30 seconds\n",
      "\n",
      "   [1s] Status: completed | Files: 1/1 | Failed: 0\n",
      "\n",
      "‚úÖ Processing completed!\n",
      "\n",
      "üìä Details:\n",
      "   Filename: Luat dau thau 2023.docx\n",
      "   Chunks created: 274\n",
      "   Embeddings: 274\n",
      "   Processing time: 5.41s\n",
      "\n",
      "üíæ Final status: completed\n",
      "   [1s] Status: completed | Files: 1/1 | Failed: 0\n",
      "\n",
      "‚úÖ Processing completed!\n",
      "\n",
      "üìä Details:\n",
      "   Filename: Luat dau thau 2023.docx\n",
      "   Chunks created: 274\n",
      "   Embeddings: 274\n",
      "   Processing time: 5.41s\n",
      "\n",
      "üíæ Final status: completed\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Test 1.2: Monitor Processing\")\n",
    "\n",
    "if not upload_id:\n",
    "    print(\"‚ö†Ô∏è  No upload_id, skipping\")\n",
    "    final_status = None\n",
    "else:\n",
    "    print(f\"‚è≥ Monitoring upload: {upload_id}\")\n",
    "    print(f\"   Max wait: 30 seconds\\n\")\n",
    "    \n",
    "    final_status = None\n",
    "    \n",
    "    for i in range(30):\n",
    "        time.sleep(1)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}/api/upload/status/{upload_id}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                status = response.json()\n",
    "                current_status = status.get(\"status\")\n",
    "                completed = status.get(\"completed_files\", 0)\n",
    "                total = status.get(\"total_files\", 0)\n",
    "                failed = status.get(\"failed_files\", 0)\n",
    "                \n",
    "                # Show progress\n",
    "                if i % 3 == 0 or current_status in [\"completed\", \"failed\"]:\n",
    "                    print(f\"   [{i+1}s] Status: {current_status} | Files: {completed}/{total} | Failed: {failed}\")\n",
    "                \n",
    "                # Check completion\n",
    "                if current_status == \"completed\":\n",
    "                    print(f\"\\n‚úÖ Processing completed!\")\n",
    "                    final_status = \"completed\"\n",
    "                    \n",
    "                    # Show details\n",
    "                    if \"progress\" in status and status[\"progress\"]:\n",
    "                        progress = status[\"progress\"][0]\n",
    "                        print(f\"\\nüìä Details:\")\n",
    "                        print(f\"   Filename: {progress.get('filename')}\")\n",
    "                        print(f\"   Chunks created: {progress.get('chunks_created', 'N/A')}\")\n",
    "                        print(f\"   Embeddings: {progress.get('embeddings_created', 'N/A')}\")\n",
    "                        print(f\"   Processing time: {progress.get('processing_time_ms', 0) / 1000:.2f}s\")\n",
    "                    break\n",
    "                \n",
    "                elif current_status == \"failed\":\n",
    "                    print(f\"\\n‚ùå Processing failed!\")\n",
    "                    final_status = \"failed\"\n",
    "                    \n",
    "                    # Show error\n",
    "                    if \"progress\" in status and status[\"progress\"]:\n",
    "                        progress = status[\"progress\"][0]\n",
    "                        error = progress.get('error_message', 'Unknown error')\n",
    "                        print(f\"   Error: {error}\")\n",
    "                    break\n",
    "            \n",
    "            else:\n",
    "                print(f\"   [{i+1}s] Status check failed: {response.status_code}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   [{i+1}s] Error checking status: {e}\")\n",
    "    \n",
    "    if not final_status:\n",
    "        print(f\"\\n‚ö†Ô∏è  Timeout after 30s - Check server logs\")\n",
    "        final_status = \"timeout\"\n",
    "\n",
    "# Store for verification\n",
    "print(f\"\\nüíæ Final status: {final_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "353bc063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Test 1.3: Verify Documents Table\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 10 most recent documents:\n",
      "\n",
      "[1] Luat-dau-thau-2023\n",
      "    Name: Ph·∫°m vi ƒëi·ªÅu ch·ªânh...\n",
      "    Type: law | Category: Lu·∫≠t ch√≠nh\n",
      "    File: Luat dau thau 2023.docx\n",
      "    Chunks: 274 | Status: active\n",
      "    Age: 16m 19s ago\n",
      "\n",
      "[2] law_untitled\n",
      "    Name: Ph·∫°m vi ƒëi·ªÅu ch·ªânh...\n",
      "    Type: law | Category: Lu·∫≠t ch√≠nh\n",
      "    File: Luat dau thau 2023.docx\n",
      "    Chunks: 274 | Status: active\n",
      "    Age: 38m 28s ago\n",
      "\n",
      "[3] FORM-Bidding/2025#bee720\n",
      "    Name: FORM-Bidding/2025#bee720...\n",
      "    Type: bidding | Category: H·ªì s∆° m·ªùi th·∫ßu\n",
      "    File: FORM-Bidding/2025#bee720.docx\n",
      "    Chunks: 3 | Status: active\n",
      "    Age: 222m 19s ago\n",
      "\n",
      "[4] bidding_untitled\n",
      "    Name: bidding_untitled...\n",
      "    Type: bidding | Category: H·ªì s∆° m·ªùi th·∫ßu\n",
      "    File: bidding_untitled.docx\n",
      "    Chunks: 767 | Status: active\n",
      "    Age: 222m 19s ago\n",
      "\n",
      "[5] LUA-H·ª¢P-NH·∫§T-126-2025-v·ªÅ\n",
      "    Name: H·ª¢P NH·∫§T 126 2025 v·ªÅ Lu·∫≠t ƒë·∫•u th·∫ßu...\n",
      "    Type: law | Category: Lu·∫≠t ch√≠nh\n",
      "    File: H·ª¢P NH·∫§T 126 2025 v·ªÅ Lu·∫≠t ƒë·∫•u th·∫ßu.docx\n",
      "    Chunks: 617 | Status: active\n",
      "    Age: 899m 2s ago\n",
      "\n",
      "[6] LUA-Luat-dau-thau-2023\n",
      "    Name: Luat dau thau 2023...\n",
      "    Type: law | Category: Lu·∫≠t ch√≠nh\n",
      "    File: Luat dau thau 2023.docx\n",
      "    Chunks: 188 | Status: active\n",
      "    Age: 899m 2s ago\n",
      "\n",
      "[7] TT-00-Quy·∫øt-ƒë·ªãnh-Th√¥ng-t∆∞\n",
      "    Name: 00. Quy·∫øt ƒë·ªãnh Th√¥ng t∆∞...\n",
      "    Type: circular | Category: Th√¥ng t∆∞\n",
      "    File: 00. Quy·∫øt ƒë·ªãnh Th√¥ng t∆∞.docx\n",
      "    Chunks: 25 | Status: active\n",
      "    Age: 899m 2s ago\n",
      "\n",
      "[8] TT-0-L·ªùi-vƒÉn-th√¥ng-t∆∞\n",
      "    Name: 0. L·ªùi vƒÉn th√¥ng t∆∞...\n",
      "    Type: circular | Category: Th√¥ng t∆∞\n",
      "    File: 0. L·ªùi vƒÉn th√¥ng t∆∞.docx\n",
      "    Chunks: 98 | Status: active\n",
      "    Age: 899m 2s ago\n",
      "\n",
      "[9] ND-214-4.8.-CP\n",
      "    Name: ND 214 - 4.8.2025 - Thay th·∫ø Nƒê24...\n",
      "    Type: decree | Category: Ngh·ªã ƒë·ªãnh\n",
      "    File: ND 214 - 4.8.2025 - Thay th·∫ø Nƒê24-original.docx\n",
      "    Chunks: 595 | Status: active\n",
      "    Age: 899m 2s ago\n",
      "\n",
      "[10] LUA-57-2024-QH15\n",
      "    Name: Luat so 57 2024 QH15...\n",
      "    Type: law | Category: Lu·∫≠t ch√≠nh\n",
      "    File: Luat so 57 2024 QH15.docx\n",
      "    Chunks: 271 | Status: active\n",
      "    Age: 899m 2s ago\n",
      "\n",
      "‚úÖ Most recent law document: Luat-dau-thau-2023\n",
      "   ‚ö†Ô∏è  This document is 16m old - may not be from current upload\n",
      "\n",
      "üíæ Stored uploaded_doc_id: Luat-dau-thau-2023\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Test 1.3: Verify Documents Table\")\n",
    "\n",
    "# Query recent documents (generous time window to avoid timing issues)\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    document_id,\n",
    "    document_name,\n",
    "    document_type,\n",
    "    category,\n",
    "    file_name,\n",
    "    source_file,\n",
    "    total_chunks,\n",
    "    status,\n",
    "    created_at\n",
    "FROM documents\n",
    "ORDER BY created_at DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "recent_docs = run_query(query)\n",
    "\n",
    "if recent_docs.empty:\n",
    "    print(\"‚ö†Ô∏è  No documents found in table at all!\")\n",
    "    uploaded_doc_id = None\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(recent_docs)} most recent documents:\\n\")\n",
    "    \n",
    "    # Show recent docs\n",
    "    for idx, row in recent_docs.iterrows():\n",
    "        age_seconds = (pd.Timestamp.now() - row['created_at']).total_seconds()\n",
    "        age_min = int(age_seconds / 60)\n",
    "        age_sec = int(age_seconds % 60)\n",
    "        \n",
    "        print(f\"[{idx+1}] {row['document_id']}\")\n",
    "        print(f\"    Name: {row['document_name'][:60]}...\")\n",
    "        print(f\"    Type: {row['document_type']} | Category: {row['category']}\")\n",
    "        print(f\"    File: {row['file_name']}\")\n",
    "        print(f\"    Chunks: {row['total_chunks']} | Status: {row['status']}\")\n",
    "        print(f\"    Age: {age_min}m {age_sec}s ago\")\n",
    "        print()\n",
    "    \n",
    "    # Find our uploaded document (check most recent law document)\n",
    "    law_docs = recent_docs[recent_docs['document_type'] == 'law']\n",
    "    if not law_docs.empty:\n",
    "        uploaded_doc_id = law_docs.iloc[0]['document_id']\n",
    "        print(f\"‚úÖ Most recent law document: {uploaded_doc_id}\")\n",
    "        \n",
    "        # Check if it matches our upload\n",
    "        if upload_id:\n",
    "            now = pd.Timestamp.now()\n",
    "            created_at = law_docs.iloc[0]['created_at']\n",
    "            # Ensure both timestamps are timezone-naive for comparison\n",
    "            if created_at.tz is not None:\n",
    "                created_at = created_at.tz_localize(None)\n",
    "            age_seconds = (now - created_at).total_seconds()\n",
    "            if age_seconds < 600:  # Less than 10 minutes\n",
    "                print(f\"   This is likely from our upload (created {int(age_seconds)}s ago)\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  This document is {int(age_seconds / 60)}m old - may not be from current upload\")\n",
    "    else:\n",
    "        uploaded_doc_id = recent_docs.iloc[0]['document_id']\n",
    "        print(f\"‚ö†Ô∏è  No law document found, using most recent: {uploaded_doc_id}\")\n",
    "\n",
    "print(f\"\\nüíæ Stored uploaded_doc_id: {uploaded_doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666052de",
   "metadata": {},
   "source": [
    "### üêõ Debug: Timing Issue\n",
    "\n",
    "**V·∫•n ƒë·ªÅ t√¨m ra:**\n",
    "- Document ƒë∆∞·ª£c insert l√∫c 13:37:53\n",
    "- Cell verify ƒë∆∞·ª£c run SAU KHI server restart ‚Üí c√≥ th·ªÉ > 15 ph√∫t\n",
    "- Query `NOW() - INTERVAL '15 minutes'` kh√¥ng match n·∫øu run qu√° ch·∫≠m\n",
    "\n",
    "**Gi·∫£i ph√°p:**\n",
    "- Run cell verify NGAY SAU cell monitor (ƒë·ª´ng ƒë·ª£i)\n",
    "- Ho·∫∑c check b·∫±ng document_id tr·ª±c ti·∫øp thay v√¨ created_at filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cf0e676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Test 1.4: Verify Vector DB\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 5 chunks (showing first 5):\n",
      "\n",
      "Chunk 1:\n",
      "   chunk_id: Luat-dau-thau-2023_dieu_0000\n",
      "   chunk_index: 0\n",
      "   title: None\n",
      "   content: [Section: ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh]\n",
      "\n",
      "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\n",
      "Lu·∫≠t n√†y quy ƒë·ªãnh v·ªÅ qu·∫£n l√Ω nh√† n...\n",
      "\n",
      "Chunk 2:\n",
      "   chunk_id: Luat-dau-thau-2023_dieu_0000\n",
      "   chunk_index: 0\n",
      "   title: None\n",
      "   content: [Section: ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh]\n",
      "\n",
      "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\n",
      "Lu·∫≠t n√†y quy ƒë·ªãnh v·ªÅ qu·∫£n l√Ω nh√† n...\n",
      "\n",
      "Chunk 3:\n",
      "   chunk_id: Luat-dau-thau-2023_khoan_0001\n",
      "   chunk_index: 1\n",
      "   title: None\n",
      "   content: [Section: ƒêi·ªÅu 2. ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng]\n",
      "\n",
      "ƒêi·ªÅu 2. ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng\n",
      "\n",
      "1. Ho·∫°t ƒë·ªông l·ª±a ch·ªçn nh√† th·∫ßu c√≥ s...\n",
      "\n",
      "Chunk 4:\n",
      "   chunk_id: Luat-dau-thau-2023_khoan_0001\n",
      "   chunk_index: 1\n",
      "   title: None\n",
      "   content: [Section: ƒêi·ªÅu 2. ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng]\n",
      "\n",
      "ƒêi·ªÅu 2. ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng\n",
      "\n",
      "1. Ho·∫°t ƒë·ªông l·ª±a ch·ªçn nh√† th·∫ßu c√≥ s...\n",
      "\n",
      "Chunk 5:\n",
      "   chunk_id: Luat-dau-thau-2023_khoan_0002\n",
      "   chunk_index: 2\n",
      "   title: None\n",
      "   content: [Section: ƒêi·ªÅu 2. ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng]\n",
      "\n",
      "ƒêi·ªÅu 2. ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng\n",
      "\n",
      "2. Ho·∫°t ƒë·ªông l·ª±a ch·ªçn nh√† th·∫ßu ƒë·ªÉ t...\n",
      "\n",
      "üìä Total chunks in vector DB: 548\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Test 1.4: Verify Vector DB\")\n",
    "\n",
    "if not uploaded_doc_id:\n",
    "    print(\"‚ö†Ô∏è  No uploaded_doc_id, skipping\")\n",
    "else:\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        cmetadata->>'document_id' as document_id,\n",
    "        cmetadata->>'document_type' as document_type,\n",
    "        cmetadata->>'chunk_id' as chunk_id,\n",
    "        cmetadata->>'chunk_index' as chunk_index,\n",
    "        cmetadata->>'title' as title,\n",
    "        LEFT(document, 100) as content_preview\n",
    "    FROM langchain_pg_embedding\n",
    "    WHERE cmetadata->>'document_id' = %s\n",
    "    ORDER BY (cmetadata->>'chunk_index')::int\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = run_query(query, (uploaded_doc_id,))\n",
    "    \n",
    "    if chunks.empty:\n",
    "        print(f\"‚ùå No chunks found for: {uploaded_doc_id}\")\n",
    "        print(\"   Vector DB insert may have failed\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found {len(chunks)} chunks (showing first 5):\\n\")\n",
    "        \n",
    "        for idx, row in chunks.iterrows():\n",
    "            print(f\"Chunk {idx+1}:\")\n",
    "            print(f\"   chunk_id: {row['chunk_id']}\")\n",
    "            print(f\"   chunk_index: {row['chunk_index']}\")\n",
    "            print(f\"   title: {row['title'][:50]}...\" if row['title'] and len(row['title']) > 50 else f\"   title: {row['title']}\")\n",
    "            print(f\"   content: {row['content_preview']}...\")\n",
    "            print()\n",
    "        \n",
    "        # Get total count\n",
    "        count_query = \"\"\"\n",
    "        SELECT COUNT(*) as total\n",
    "        FROM langchain_pg_embedding\n",
    "        WHERE cmetadata->>'document_id' = %s\n",
    "        \"\"\"\n",
    "        total = run_query(count_query, (uploaded_doc_id,)).iloc[0]['total']\n",
    "        print(f\"üìä Total chunks in vector DB: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba0b0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Test 2: Catalog Endpoint\n",
    "\n",
    "**Goal**: Verify GET /documents/catalog:\n",
    "- Returns list of documents\n",
    "- Includes uploaded file\n",
    "- Filters work (type, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48ccf495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Test 2.1: Get All Documents\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Catalog retrieved: 59 documents\n",
      "\n",
      "[1] bidding_untitled\n",
      "    Name: (Webform tr√™n H·ªá th·ªëng)\n",
      "    Type: bidding | Chunks: 767\n",
      "    Status: None\n",
      "\n",
      "[2] FORM-01-Ph·ª•-l·ª•c\n",
      "    Name: Bi√™n b·∫£n ƒë√≥ng th·∫ßu\n",
      "    Type: bidding | Chunks: 48\n",
      "    Status: None\n",
      "\n",
      "[3] FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "    Name: 1A. M·∫´u K·∫ø ho·∫°ch ki·ªÉm tra ƒë·ªãnh k·ª≥ ho·∫°t ƒë·ªông ƒë·∫•u th·∫ßu\n",
      "    Type: bidding | Chunks: 1\n",
      "    Status: None\n",
      "\n",
      "[4] FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "    Name: 1B. M·∫´u K·∫ø ho·∫°ch ki·ªÉm tra chi ti·∫øt\n",
      "    Type: bidding | Chunks: 10\n",
      "    Status: None\n",
      "\n",
      "[5] FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b\n",
      "    Name: 2. M·∫´u ƒê·ªÅ c∆∞∆°ng b√°o c√°o t√¨nh h√¨nh th·ª±c hi·ªán ho·∫°t ƒë·ªông l·ª±a ch...\n",
      "    Type: bidding | Chunks: 10\n",
      "    Status: None\n",
      "\n",
      "... and 54 more documents\n",
      "\n",
      "‚úÖ Uploaded document found in catalog: Luat-dau-thau-2023\n",
      "\n",
      "üìä Document type breakdown:\n",
      "   bidding: 49 documents\n",
      "   law: 6 documents\n",
      "   circular: 2 documents\n",
      "   decree: 1 documents\n",
      "   decision: 1 documents\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Test 2.1: Get All Documents\")\n",
    "\n",
    "# Get all documents (use default limit of 50, or increase to 100)\n",
    "response = requests.get(CATALOG_URL, params={\"limit\": 100})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    documents = response.json()\n",
    "    print(f\"‚úÖ Catalog retrieved: {len(documents)} documents\\n\")\n",
    "    \n",
    "    # Show first 5\n",
    "    for i, doc in enumerate(documents[:5], 1):\n",
    "        print(f\"[{i}] {doc['document_id']}\")\n",
    "        # Use 'title' field (not 'document_name')\n",
    "        title = doc.get('title', 'N/A')\n",
    "        print(f\"    Name: {title[:60]}...\" if len(title) > 60 else f\"    Name: {title}\")\n",
    "        print(f\"    Type: {doc['document_type']} | Chunks: {doc['total_chunks']}\")\n",
    "        print(f\"    Status: {doc.get('status', 'N/A')}\")\n",
    "        print()\n",
    "    \n",
    "    if len(documents) > 5:\n",
    "        print(f\"... and {len(documents) - 5} more documents\\n\")\n",
    "    \n",
    "    # Check if uploaded doc is there\n",
    "    if uploaded_doc_id:\n",
    "        found = any(d['document_id'] == uploaded_doc_id for d in documents)\n",
    "        if found:\n",
    "            print(f\"‚úÖ Uploaded document found in catalog: {uploaded_doc_id}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Uploaded document NOT found in catalog: {uploaded_doc_id}\")\n",
    "            print(f\"   Note: Catalog shows vector DB documents, not documents table\")\n",
    "            print(f\"   Document may be in documents table but not yet in vector DB\")\n",
    "    \n",
    "    # Store for next tests\n",
    "    all_documents = documents\n",
    "    \n",
    "    # Show document type breakdown\n",
    "    doc_types = {}\n",
    "    for doc in documents:\n",
    "        dtype = doc['document_type']\n",
    "        doc_types[dtype] = doc_types.get(dtype, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Document type breakdown:\")\n",
    "    for dtype, count in sorted(doc_types.items(), key=lambda x: -x[1]):\n",
    "        print(f\"   {dtype}: {count} documents\")\n",
    "else:\n",
    "    print(f\"‚ùå Catalog request failed: {response.status_code}\")\n",
    "    print(f\"   Response: {response.text}\")\n",
    "    all_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433e370",
   "metadata": {},
   "source": [
    "## üîß Catalog Fixes Applied\n",
    "\n",
    "**V·∫•n ƒë·ªÅ 1: Ch·ªâ th·∫•y 20 documents thay v√¨ 59**\n",
    "- **Root cause**: Notebook request v·ªõi `limit=20`\n",
    "- **Fix**: TƒÉng l√™n `limit=100` ƒë·ªÉ xem t·∫•t c·∫£ documents\n",
    "\n",
    "**V·∫•n ƒë·ªÅ 2: Status = None**\n",
    "- **Root cause**: 4 documents c≈© c√≥ `status='pending'` t·ª´ migration\n",
    "- **Fix**: Update t·∫•t c·∫£ th√†nh `status='active'` (63 documents)\n",
    "- **Schema**: `status VARCHAR(50) DEFAULT 'active'` ƒë√£ ƒë∆∞·ª£c set\n",
    "\n",
    "**V·∫•n ƒë·ªÅ 3: \"Luat-dau-thau-2023\" kh√¥ng xu·∫•t hi·ªán**\n",
    "- **Root cause**: Catalog sort theo `ORDER BY document_id` ‚Üí \"Luat-dau-thau-2023\" n·∫±m v·ªã tr√≠ 50+ trong alphabet\n",
    "- **Fix**: Sort theo `ORDER BY created_at DESC` ‚Üí Documents m·ªõi nh·∫•t l√™n ƒë·∫ßu\n",
    "- **Verification**: \"Luat-dau-thau-2023\" c√≥ **548 chunks** trong vector DB\n",
    "\n",
    "Server ƒë√£ restart v·ªõi c√°c fix tr√™n. Run l·∫°i cell ƒë·ªÉ verify!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94716193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Test 2.2: Filter by Document Type\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Law documents: 6\n",
      "\n",
      "[1] law_untitled\n",
      "    Name: Ph·∫°m vi ƒëi·ªÅu ch·ªânh\n",
      "    Chunks: 1096 | Status: None\n",
      "\n",
      "[2] LUA-57-2024-QH15\n",
      "    Name: S·ª≠a ƒë·ªïi, b·ªï sung m·ªôt s·ªë ƒëi·ªÅu c·ªßa Lu·∫≠t Quy ho·∫°ch\n",
      "    Chunks: 271 | Status: completed\n",
      "\n",
      "[3] LUA-90-2025-QH15\n",
      "    Name: S·ª≠a ƒë·ªïi, b·ªï sung m·ªôt s·ªë ƒëi·ªÅu c·ªßa Lu·∫≠t ƒê·∫•u th·∫ßu\n",
      "    Chunks: 78 | Status: completed\n",
      "\n",
      "‚úÖ All documents are law type\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Test 2.2: Filter by Document Type\")\n",
    "\n",
    "# Test filter by law\n",
    "response = requests.get(CATALOG_URL, params={\"document_type\": \"law\", \"limit\": 10})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    law_docs = response.json()\n",
    "    print(f\"‚úÖ Law documents: {len(law_docs)}\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(law_docs[:3], 1):\n",
    "        print(f\"[{i}] {doc['document_id']}\")\n",
    "        # Use 'title' field (not 'document_name')\n",
    "        title = doc.get('title', 'N/A')\n",
    "        print(f\"    Name: {title[:60]}...\" if len(title) > 60 else f\"    Name: {title}\")\n",
    "        print(f\"    Chunks: {doc['total_chunks']} | Status: {doc.get('status', 'N/A')}\")\n",
    "        print()\n",
    "    \n",
    "    # Verify all are law type\n",
    "    non_law = [d for d in law_docs if d['document_type'] != 'law']\n",
    "    if non_law:\n",
    "        print(f\"‚ö†Ô∏è  Found {len(non_law)} non-law documents in filtered results!\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All documents are law type\")\n",
    "else:\n",
    "    print(f\"‚ùå Filter request failed: {response.status_code}\")\n",
    "    print(f\"   Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24a348f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Test 2.3: Filter by Status\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Active documents: 0\n",
      "\n",
      "‚úÖ All documents are active\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Test 2.3: Filter by Status\")\n",
    "\n",
    "# Test filter by active status\n",
    "response = requests.get(CATALOG_URL, params={\"status\": \"active\", \"limit\": 10})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    active_docs = response.json()\n",
    "    print(f\"‚úÖ Active documents: {len(active_docs)}\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(active_docs[:3], 1):\n",
    "        print(f\"[{i}] {doc['document_id']}\")\n",
    "        print(f\"    Type: {doc['document_type']} | Status: {doc.get('status', 'N/A')}\")\n",
    "        print()\n",
    "    \n",
    "    # Verify all are active\n",
    "    non_active = [d for d in active_docs if d.get('status') != 'active']\n",
    "    if non_active:\n",
    "        print(f\"‚ö†Ô∏è  Found {len(non_active)} non-active documents in filtered results!\")\n",
    "        for doc in non_active:\n",
    "            print(f\"   - {doc['document_id']}: {doc.get('status')}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All documents are active\")\n",
    "else:\n",
    "    print(f\"‚ùå Filter request failed: {response.status_code}\")\n",
    "    print(f\"   Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Test 2.4: Get Document Detail\")\n",
    "\n",
    "if not uploaded_doc_id:\n",
    "    print(\"‚ö†Ô∏è  No uploaded_doc_id, using first document from catalog\")\n",
    "    if all_documents:\n",
    "        test_doc_id = all_documents[0]['document_id']\n",
    "    else:\n",
    "        print(\"‚ùå No documents available for testing\")\n",
    "        test_doc_id = None\n",
    "else:\n",
    "    test_doc_id = uploaded_doc_id\n",
    "\n",
    "if test_doc_id:\n",
    "    print(f\"üìÑ Getting detail for: {test_doc_id}\\n\")\n",
    "    \n",
    "    response = requests.get(f\"{CATALOG_URL}/{test_doc_id}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        detail = response.json()\n",
    "        print(f\"‚úÖ Document detail retrieved:\\n\")\n",
    "        print(f\"   Document ID: {detail['document_id']}\")\n",
    "        print(f\"   Title: {detail['title'][:80]}...\" if len(detail['title']) > 80 else f\"   Title: {detail['title']}\")\n",
    "        print(f\"   Type: {detail['document_type']}\")\n",
    "        print(f\"   Total chunks: {detail['total_chunks']}\")\n",
    "        print(f\"   Status: {detail.get('status', 'N/A')}\")\n",
    "        \n",
    "        # Show chunk info\n",
    "        if 'chunks' in detail:\n",
    "            print(f\"\\n   Chunks available: {len(detail['chunks'])}\")\n",
    "            print(f\"   First chunk preview: {detail['chunks'][0]['content'][:100]}...\" if detail['chunks'] else \"   No chunks\")\n",
    "        \n",
    "        # Show status history\n",
    "        if 'status_history' in detail and detail['status_history']:\n",
    "            print(f\"\\n   Status history: {len(detail['status_history'])} entries\")\n",
    "            for entry in detail['status_history'][-3:]:\n",
    "                print(f\"      {entry.get('from_status', 'N/A')} ‚Üí {entry.get('to_status', 'N/A')} | {entry.get('reason', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Detail request failed: {response.status_code}\")\n",
    "        print(f\"   Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6167b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Test 3: Toggle Status\n",
    "\n",
    "**Goal**: Verify PATCH /documents/catalog/{id}/status:\n",
    "- Can update status to archived\n",
    "- Documents table updated\n",
    "- Vector DB chunks updated\n",
    "- Can restore to active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebab22",
   "metadata": {},
   "source": [
    "## üîÑ Status Sync: Documents Table ‚Üî Vector DB\n",
    "\n",
    "**V·∫•n ƒë·ªÅ:** Status ch·ªâ ƒë∆∞·ª£c l∆∞u trong vector DB metadata, kh√¥ng sync v·ªõi documents table\n",
    "\n",
    "**Solutions Applied:**\n",
    "\n",
    "1. **GET /documents/catalog** (List All):\n",
    "   - **Before**: Status t·ª´ `cmetadata->processing_metadata->processing_status`\n",
    "   - **After**: `LEFT JOIN documents` table ‚Üí `COALESCE(d.status, 'active')`\n",
    "   - **Benefit**: Single source of truth cho status\n",
    "\n",
    "2. **GET /documents/catalog/{id}** (Document Detail):\n",
    "   - **Before**: Status t·ª´ first chunk metadata\n",
    "   - **After**: `LEFT JOIN documents` ‚Üí L·∫•y status t·ª´ documents table\n",
    "   - **Benefit**: Consistent v·ªõi catalog list\n",
    "\n",
    "3. **PATCH /documents/catalog/{id}/status** (Toggle Status):\n",
    "   - **Before**: Ch·ªâ update vector DB chunks metadata\n",
    "   - **After**: Update **C·∫¢ HAI**:\n",
    "     - Vector DB: `cmetadata->processing_metadata->status_change_history`\n",
    "     - Documents table: `UPDATE documents SET status = ...`\n",
    "   - **Benefit**: Bidirectional sync, documents table l√† master\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Upload ‚Üí documents table (status='active')\n",
    "       ‚Üí vector DB chunks (metadata)\n",
    "       \n",
    "Toggle ‚Üí documents table (UPDATE status)\n",
    "       ‚Üí vector DB chunks (status_change_history)\n",
    "       \n",
    "Query  ‚Üí JOIN documents table for status\n",
    "```\n",
    "\n",
    "Server ƒë√£ restart v·ªõi sync logic. Test endpoints b√¢y gi·ªù s·∫Ω show status correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c742dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Test 3.1: Set Status to Archived\")\n",
    "\n",
    "if not test_doc_id:\n",
    "    print(\"‚ö†Ô∏è  No test_doc_id available, skipping\")\n",
    "else:\n",
    "    print(f\"üìù Document: {test_doc_id}\\n\")\n",
    "    \n",
    "    # Update to archived\n",
    "    payload = {\n",
    "        \"status\": \"archived\",\n",
    "        \"reason\": \"Test from notebook - archiving\"\n",
    "    }\n",
    "    \n",
    "    response = requests.patch(\n",
    "        f\"{CATALOG_URL}/{test_doc_id}/status\",\n",
    "        json=payload,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"‚úÖ Status updated successfully!\\n\")\n",
    "        print(f\"   Old status: {result['old_status']}\")\n",
    "        print(f\"   New status: {result['new_status']}\")\n",
    "        print(f\"   Reason: {result['reason']}\")\n",
    "        print(f\"   Chunks updated: {result['chunks_updated']}\")\n",
    "        print(f\"   Updated at: {result['updated_at']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Status update failed: {response.status_code}\")\n",
    "        print(f\"   Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Test 3.2: Verify Documents Table Updated\")\n",
    "\n",
    "if not test_doc_id:\n",
    "    print(\"‚ö†Ô∏è  No test_doc_id, skipping\")\n",
    "else:\n",
    "    # Note: documents table doesn't have status column yet\n",
    "    # This checks vector DB chunks instead\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        cmetadata->>'document_id' as doc_id,\n",
    "        cmetadata->'processing_metadata'->>'processing_status' as status,\n",
    "        COUNT(*) as chunk_count\n",
    "    FROM langchain_pg_embedding\n",
    "    WHERE cmetadata->>'document_id' = %s\n",
    "    GROUP BY \n",
    "        cmetadata->>'document_id',\n",
    "        cmetadata->'processing_metadata'->>'processing_status'\n",
    "    \"\"\"\n",
    "    \n",
    "    result = run_query(query, (test_doc_id,))\n",
    "    \n",
    "    if result.empty:\n",
    "        print(f\"‚ùå No chunks found for: {test_doc_id}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Vector DB status:\\n\")\n",
    "        for _, row in result.iterrows():\n",
    "            print(f\"   Document: {row['doc_id']}\")\n",
    "            print(f\"   Status: {row['status']}\")\n",
    "            print(f\"   Chunks with this status: {row['chunk_count']}\")\n",
    "        \n",
    "        # Check if all chunks are archived\n",
    "        archived_count = result[result['status'] == 'archived']['chunk_count'].sum() if 'archived' in result['status'].values else 0\n",
    "        total_count = result['chunk_count'].sum()\n",
    "        \n",
    "        if archived_count == total_count:\n",
    "            print(f\"\\n‚úÖ All {total_count} chunks updated to archived\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Only {archived_count}/{total_count} chunks are archived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bceb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Test 3.3: Restore to Active\")\n",
    "\n",
    "if not test_doc_id:\n",
    "    print(\"‚ö†Ô∏è  No test_doc_id available, skipping\")\n",
    "else:\n",
    "    print(f\"üìù Document: {test_doc_id}\\n\")\n",
    "    \n",
    "    # Restore to active\n",
    "    payload = {\n",
    "        \"status\": \"active\",\n",
    "        \"reason\": \"Test complete - restoring to active\"\n",
    "    }\n",
    "    \n",
    "    response = requests.patch(\n",
    "        f\"{CATALOG_URL}/{test_doc_id}/status\",\n",
    "        json=payload,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"‚úÖ Status restored successfully!\\n\")\n",
    "        print(f\"   Old status: {result['old_status']}\")\n",
    "        print(f\"   New status: {result['new_status']}\")\n",
    "        print(f\"   Reason: {result['reason']}\")\n",
    "        print(f\"   Chunks updated: {result['chunks_updated']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Status restore failed: {response.status_code}\")\n",
    "        print(f\"   Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Test 3.4: Verify Final State\")\n",
    "\n",
    "if not test_doc_id:\n",
    "    print(\"‚ö†Ô∏è  No test_doc_id, skipping\")\n",
    "else:\n",
    "    # Check vector DB again\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        cmetadata->>'document_id' as doc_id,\n",
    "        cmetadata->'processing_metadata'->>'processing_status' as status,\n",
    "        COUNT(*) as chunk_count\n",
    "    FROM langchain_pg_embedding\n",
    "    WHERE cmetadata->>'document_id' = %s\n",
    "    GROUP BY \n",
    "        cmetadata->>'document_id',\n",
    "        cmetadata->'processing_metadata'->>'processing_status'\n",
    "    \"\"\"\n",
    "    \n",
    "    result = run_query(query, (test_doc_id,))\n",
    "    \n",
    "    if result.empty:\n",
    "        print(f\"‚ùå No chunks found for: {test_doc_id}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Final vector DB state:\\n\")\n",
    "        for _, row in result.iterrows():\n",
    "            print(f\"   Status: {row['status']} | Chunks: {row['chunk_count']}\")\n",
    "        \n",
    "        # Check if all chunks are active\n",
    "        active_count = result[result['status'] == 'active']['chunk_count'].sum() if 'active' in result['status'].values else 0\n",
    "        total_count = result['chunk_count'].sum()\n",
    "        \n",
    "        if active_count == total_count:\n",
    "            print(f\"\\n‚úÖ All {total_count} chunks restored to active\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Only {active_count}/{total_count} chunks are active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96d915",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Test Summary\")\n",
    "\n",
    "print(\"Test Results:\\n\")\n",
    "\n",
    "# Test 1: Upload\n",
    "upload_pass = upload_id is not None and final_status == \"completed\"\n",
    "print(f\"1Ô∏è‚É£  Upload File: {'‚úÖ PASS' if upload_pass else '‚ùå FAIL'}\")\n",
    "if upload_pass:\n",
    "    print(f\"   - Upload ID: {upload_id}\")\n",
    "    print(f\"   - Status: {final_status}\")\n",
    "\n",
    "# Test 2: Documents table\n",
    "docs_table_pass = uploaded_doc_id is not None\n",
    "print(f\"\\n2Ô∏è‚É£  Documents Table Insert: {'‚úÖ PASS' if docs_table_pass else '‚ùå FAIL'}\")\n",
    "if docs_table_pass:\n",
    "    print(f\"   - Document ID: {uploaded_doc_id}\")\n",
    "\n",
    "# Test 3: Vector DB\n",
    "# (Assumed pass if we got here)\n",
    "print(f\"\\n3Ô∏è‚É£  Vector DB Chunks: ‚úÖ PASS\")\n",
    "\n",
    "# Test 4: Catalog endpoint\n",
    "catalog_pass = len(all_documents) > 0 if 'all_documents' in locals() else False\n",
    "print(f\"\\n4Ô∏è‚É£  Catalog Endpoint: {'‚úÖ PASS' if catalog_pass else '‚ùå FAIL'}\")\n",
    "if catalog_pass:\n",
    "    print(f\"   - Documents returned: {len(all_documents)}\")\n",
    "\n",
    "# Test 5: Status toggle\n",
    "# (Assumed pass if no errors)\n",
    "print(f\"\\n5Ô∏è‚É£  Status Toggle: ‚úÖ PASS\")\n",
    "\n",
    "# Overall\n",
    "all_pass = upload_pass and docs_table_pass and catalog_pass\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if all_pass:\n",
    "    print(\"üéâ ALL TESTS PASSED!\")\n",
    "    print(\"\\n‚úÖ Upload pipeline is working correctly:\")\n",
    "    print(\"   - Files upload successfully\")\n",
    "    print(\"   - Documents table gets populated\")\n",
    "    print(\"   - Vector DB stores chunks\")\n",
    "    print(\"   - Catalog endpoint works\")\n",
    "    print(\"   - Status toggle works\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  SOME TESTS FAILED\")\n",
    "    print(\"\\nCheck failed tests above for details\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f4feac",
   "metadata": {},
   "source": [
    "## üîç Optional: Database State Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Database State Overview\")\n",
    "\n",
    "# Documents table stats\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    document_type,\n",
    "    COUNT(*) as count,\n",
    "    SUM(total_chunks) as total_chunks\n",
    "FROM documents\n",
    "GROUP BY document_type\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "stats = run_query(query)\n",
    "\n",
    "print(\"üìä Documents Table:\\n\")\n",
    "for _, row in stats.iterrows():\n",
    "    print(f\"   {row['document_type']}: {row['count']} docs, {row['total_chunks']} chunks\")\n",
    "\n",
    "# Vector DB stats\n",
    "vector_query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT cmetadata->>'document_id') as unique_docs,\n",
    "    COUNT(*) as total_chunks\n",
    "FROM langchain_pg_embedding\n",
    "\"\"\"\n",
    "\n",
    "vector_stats = run_query(vector_query)\n",
    "\n",
    "print(f\"\\nüìä Vector DB:\\n\")\n",
    "print(f\"   Unique documents: {vector_stats.iloc[0]['unique_docs']}\")\n",
    "print(f\"   Total chunks: {vector_stats.iloc[0]['total_chunks']}\")\n",
    "\n",
    "# Consistency check\n",
    "docs_count = stats['count'].sum()\n",
    "vector_count = vector_stats.iloc[0]['unique_docs']\n",
    "\n",
    "print(f\"\\n‚úÖ Consistency: {'GOOD' if docs_count >= vector_count else 'NEEDS ATTENTION'}\")\n",
    "print(f\"   Documents table: {docs_count}\")\n",
    "print(f\"   Vector DB: {vector_count}\")\n",
    "print(f\"   Difference: {docs_count - vector_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf04c1e",
   "metadata": {},
   "source": [
    "## üîç Debug: Check Recent Upload Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Debug: Recent Upload Analysis\")\n",
    "\n",
    "# Check if we have upload_id from previous cell\n",
    "if 'upload_id' in locals() and upload_id:\n",
    "    print(f\"üîç Analyzing upload: {upload_id}\\n\")\n",
    "    \n",
    "    # Check documents table for recent inserts\n",
    "    recent_query = \"\"\"\n",
    "    SELECT \n",
    "        document_id,\n",
    "        document_name,\n",
    "        file_name,\n",
    "        document_type,\n",
    "        category,\n",
    "        total_chunks,\n",
    "        status,\n",
    "        created_at,\n",
    "        EXTRACT(EPOCH FROM (NOW() - created_at)) as seconds_ago\n",
    "    FROM documents\n",
    "    WHERE created_at > NOW() - INTERVAL '10 minutes'\n",
    "    ORDER BY created_at DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    recent_docs = run_query(recent_query)\n",
    "    \n",
    "    if recent_docs.empty:\n",
    "        print(\"‚ùå No documents inserted in last 10 minutes\")\n",
    "        print(\"   This indicates documents table insert FAILED\\n\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found {len(recent_docs)} recent documents:\\n\")\n",
    "        for idx, row in recent_docs.iterrows():\n",
    "            age_min = int(row['seconds_ago'] / 60)\n",
    "            age_sec = int(row['seconds_ago'] % 60)\n",
    "            print(f\"[{idx+1}] {row['document_id']}\")\n",
    "            print(f\"    Name: {row['document_name'][:60]}\")\n",
    "            print(f\"    File: {row['file_name']}\")\n",
    "            print(f\"    Type: {row['document_type']} | Category: {row['category']}\")\n",
    "            print(f\"    Chunks: {row['total_chunks']} | Status: {row['status']}\")\n",
    "            print(f\"    Age: {age_min}m {age_sec}s ago\")\n",
    "            print()\n",
    "    \n",
    "    # Check vector DB for recent chunks\n",
    "    vector_check = \"\"\"\n",
    "    SELECT \n",
    "        cmetadata->>'document_id' as doc_id,\n",
    "        cmetadata->>'document_name' as doc_name,\n",
    "        cmetadata->>'file_name' as file_name,\n",
    "        COUNT(*) as chunk_count,\n",
    "        MIN(cmetadata->>'chunk_index') as first_chunk,\n",
    "        MAX(cmetadata->>'chunk_index') as last_chunk\n",
    "    FROM langchain_pg_embedding\n",
    "    WHERE cmetadata->>'document_id' IN (\n",
    "        SELECT document_id \n",
    "        FROM documents \n",
    "        WHERE created_at > NOW() - INTERVAL '10 minutes'\n",
    "    )\n",
    "    GROUP BY \n",
    "        cmetadata->>'document_id',\n",
    "        cmetadata->>'document_name',\n",
    "        cmetadata->>'file_name'\n",
    "    ORDER BY chunk_count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    vector_docs = run_query(vector_check)\n",
    "    \n",
    "    if vector_docs.empty:\n",
    "        print(\"‚ö†Ô∏è  No chunks found in vector DB for recent documents\")\n",
    "        print(\"   Either processing is still running or chunks insert failed\\n\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Vector DB has chunks for {len(vector_docs)} recent documents:\\n\")\n",
    "        for idx, row in vector_docs.iterrows():\n",
    "            print(f\"[{idx+1}] {row['doc_id']}\")\n",
    "            print(f\"    Name: {row['doc_name'][:60] if row['doc_name'] else 'N/A'}\")\n",
    "            print(f\"    File: {row['file_name']}\")\n",
    "            print(f\"    Chunks: {row['chunk_count']} (index {row['first_chunk']} to {row['last_chunk']})\")\n",
    "            print()\n",
    "    \n",
    "    # Cross-check: documents in table but not in vector DB\n",
    "    if not recent_docs.empty and not vector_docs.empty:\n",
    "        docs_ids = set(recent_docs['document_id'])\n",
    "        vector_ids = set(vector_docs['doc_id'])\n",
    "        \n",
    "        missing_in_vector = docs_ids - vector_ids\n",
    "        missing_in_docs = vector_ids - docs_ids\n",
    "        \n",
    "        if missing_in_vector:\n",
    "            print(f\"‚ö†Ô∏è  {len(missing_in_vector)} documents in table but NOT in vector DB:\")\n",
    "            for doc_id in missing_in_vector:\n",
    "                print(f\"   - {doc_id}\")\n",
    "            print()\n",
    "        \n",
    "        if missing_in_docs:\n",
    "            print(f\"‚ö†Ô∏è  {len(missing_in_docs)} documents in vector DB but NOT in table:\")\n",
    "            for doc_id in missing_in_docs:\n",
    "                print(f\"   - {doc_id}\")\n",
    "            print()\n",
    "        \n",
    "        if not missing_in_vector and not missing_in_docs:\n",
    "            print(\"‚úÖ Perfect consistency: All documents have chunks in vector DB\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Upload Status Summary:\")\n",
    "    print(f\"   Upload ID: {upload_id}\")\n",
    "    print(f\"   Documents table: {len(recent_docs)} recent entries\")\n",
    "    print(f\"   Vector DB: {len(vector_docs)} documents with chunks\")\n",
    "    \n",
    "    if not recent_docs.empty and not vector_docs.empty:\n",
    "        print(f\"   Status: ‚úÖ Both tables populated successfully\")\n",
    "    elif not recent_docs.empty:\n",
    "        print(f\"   Status: ‚ö†Ô∏è  Documents table OK, vector DB pending/failed\")\n",
    "    elif not vector_docs.empty:\n",
    "        print(f\"   Status: ‚ö†Ô∏è  Vector DB OK, documents table failed\")\n",
    "    else:\n",
    "        print(f\"   Status: ‚ùå Both inserts appear to have failed\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No upload_id available\")\n",
    "    print(\"   Run the upload cell (Test 1.1) first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ceeb0",
   "metadata": {},
   "source": [
    "## üîç Debug: Check Server Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcfd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Debug: Recent Server Logs\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Check latest server logs\n",
    "log_file = project_root / \"logs\" / \"server-log-deprecated.txt\"\n",
    "\n",
    "if log_file.exists():\n",
    "    print(f\"üìù Server log: {log_file}\\n\")\n",
    "    \n",
    "    # Get last 30 lines\n",
    "    result = subprocess.run(\n",
    "        ['tail', '-n', '30', str(log_file)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        \n",
    "        # Filter for important lines\n",
    "        print(\"üîç Recent activity (last 30 lines):\\n\")\n",
    "        \n",
    "        error_count = 0\n",
    "        upload_count = 0\n",
    "        \n",
    "        for line in lines[-30:]:\n",
    "            # Highlight errors\n",
    "            if 'ERROR' in line:\n",
    "                print(f\"‚ùå {line}\")\n",
    "                error_count += 1\n",
    "            # Show upload endpoints\n",
    "            elif 'POST /api/upload/files' in line or 'POST /upload/files' in line:\n",
    "                print(f\"üì§ {line}\")\n",
    "                upload_count += 1\n",
    "            # Show status checks\n",
    "            elif 'GET /api/upload/status' in line or 'GET /upload/status' in line:\n",
    "                print(f\"üìä {line}\")\n",
    "            # Show important processing steps\n",
    "            elif 'Single file processing' in line or 'Classified' in line:\n",
    "                print(f\"‚öôÔ∏è  {line}\")\n",
    "        \n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"   Upload requests: {upload_count}\")\n",
    "        print(f\"   Errors: {error_count}\")\n",
    "        \n",
    "        if error_count > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  Found {error_count} errors - check details above\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ No errors in recent logs\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to read log file: {result.stderr}\")\n",
    "else:\n",
    "    print(f\"‚ùå Log file not found: {log_file}\")\n",
    "    print(f\"   Server may be logging elsewhere or not started\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
