{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ad7378",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# NG√ÄY 1: Setup Documents Table + Update Chunks\n",
    "\n",
    "**Th·ªùi gian:** 4 gi·ªù  \n",
    "**M·ª•c ti√™u:** Create documents table (70 records) + Update chunk metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffcbb8",
   "metadata": {},
   "source": [
    "## Step 1: Extract Metadata t·ª´ 70 Files\n",
    "\n",
    "**Th·ªùi gian:** 30 ph√∫t  \n",
    "**Input:** 70 files trong data/raw/  \n",
    "**Output:** documents_metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efa8c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing to project root: /home/sakana/Code/RAG-bidding\n",
      "Current directory: /home/sakana/Code/RAG-bidding\n",
      "\n",
      "üîç Scanning data/raw folders...\n",
      "\n",
      "‚úÖ Found 68 documents\n",
      "\n",
      "üìÅ C√¢u h·ªèi thi: 5 documents\n",
      "   - EXAM-NHCH_30925_bo_sung: NHCH 30.9.25 bo sung theo TB1952 qldt (1)\n",
      "   - EXAM-Ng√¢n-h√†ng-c√¢u-h·ªèi-CC: Ng√¢n h√†ng c√¢u h·ªèi CCDT ƒë·ª£t 2\n",
      "   ... and 3 more\n",
      "\n",
      "üìÅ H·ªì s∆° m·ªùi th·∫ßu: 45 documents\n",
      "   - FORM-15-Phu-luc: 15. Phu luc\n",
      "   - FORM-01-Ph·ª•-l·ª•c: 01. Ph·ª• l·ª•c\n",
      "   ... and 43 more\n",
      "\n",
      "üìÅ Lu·∫≠t ch√≠nh: 4 documents\n",
      "   - LUA-90-2025-QH15: Luat so 90 2025-qh15\n",
      "   - LUA-57-2024-QH15: Luat so 57 2024 QH15\n",
      "   ... and 2 more\n",
      "\n",
      "üìÅ M·∫´u b√°o c√°o: 10 documents\n",
      "   - TEMPLATE-BC-03B: 03B. M·∫´u BCTƒê danh s√°ch ƒë√°p ·ª©ng v·ªÅ k·ªπ thu·∫≠t\n",
      "   - TEMPLATE-BC-03C: 03C. M·∫´u BCTƒê KQLCNT\n",
      "   ... and 8 more\n",
      "\n",
      "üìÅ Ngh·ªã ƒë·ªãnh: 1 documents\n",
      "   - ND-214-4.8.-CP: ND 214 - 4.8.2025 - Thay th·∫ø Nƒê24\n",
      "\n",
      "üìÅ Quy·∫øt ƒë·ªãnh: 1 documents\n",
      "   - QD-1667-BYT: Quy·∫øt ƒë·ªãnh-1667-Qƒê-BYT\n",
      "\n",
      "üìÅ Th√¥ng t∆∞: 2 documents\n",
      "   - TT-0-L·ªùi-vƒÉn-th√¥ng-t∆∞: 0. L·ªùi vƒÉn th√¥ng t∆∞\n",
      "   - TT-00-Quy·∫øt-ƒë·ªãnh-Th√¥ng-t∆∞: 00. Quy·∫øt ƒë·ªãnh Th√¥ng t∆∞\n",
      "\n",
      "üíæ Saved to: data/processed/documents_metadata.json\n",
      "üìä Total documents: 68\n"
     ]
    }
   ],
   "source": [
    "# Run extraction script - navigate to project root\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Hardcode project root path\n",
    "project_root = \"/home/sakana/Code/RAG-bidding\"\n",
    "\n",
    "print(f\"Changing to project root: {project_root}\")\n",
    "os.chdir(project_root)\n",
    "print(f\"Current directory: {os.getcwd()}\\n\")\n",
    "\n",
    "# Run the script\n",
    "!python scripts/migration/002_extract_simple_metadata.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fde719",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "‚úÖ Found 70 documents\n",
    "\n",
    "üìÅ H·ªì s∆° m·ªùi th·∫ßu: 46 documents\n",
    "üìÅ M·∫´u b√°o c√°o: 10 documents\n",
    "üìÅ C√¢u h·ªèi thi: 6 documents\n",
    "üìÅ Lu·∫≠t ch√≠nh: 4 documents\n",
    "üìÅ Th√¥ng t∆∞: 2 documents\n",
    "üìÅ Ngh·ªã ƒë·ªãnh: 1 documents\n",
    "üìÅ Quy·∫øt ƒë·ªãnh: 1 documents\n",
    "\n",
    "üíæ Saved to: data/processed/documents_metadata.json\n",
    "```\n",
    "\n",
    "**Verify:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7edd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 68\n",
      "\n",
      "By category:\n",
      "  H·ªì s∆° m·ªùi th·∫ßu: 45 documents\n",
      "  M·∫´u b√°o c√°o: 10 documents\n",
      "  C√¢u h·ªèi thi: 5 documents\n",
      "  Lu·∫≠t ch√≠nh: 4 documents\n",
      "  Th√¥ng t∆∞: 2 documents\n",
      "  Ngh·ªã ƒë·ªãnh: 1 documents\n",
      "  Quy·∫øt ƒë·ªãnh: 1 documents\n",
      "\n",
      "Sample documents:\n",
      "  LUA-90-2025-QH15: Luat so 90 2025-qh15...\n",
      "  LUA-57-2024-QH15: Luat so 57 2024 QH15...\n",
      "  LUA-H·ª¢P-NH·∫§T-126-2025-v·ªÅ: H·ª¢P NH·∫§T 126 2025 v·ªÅ Lu·∫≠t ƒë·∫•u th·∫ßu...\n",
      "  LUA-Luat-dau-thau-2023: Luat dau thau 2023...\n",
      "  ND-214-4.8.-CP: ND 214 - 4.8.2025 - Thay th·∫ø Nƒê24...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load metadata (now we're in project root)\n",
    "metadata_file = Path(\"data/processed/documents_metadata.json\")\n",
    "with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "\n",
    "# Group by category\n",
    "by_category = {}\n",
    "for doc in documents:\n",
    "    cat = doc['category']\n",
    "    if cat not in by_category:\n",
    "        by_category[cat] = []\n",
    "    by_category[cat].append(doc)\n",
    "\n",
    "print(\"\\nBy category:\")\n",
    "for cat, docs in sorted(by_category.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    print(f\"  {cat}: {len(docs)} documents\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample documents:\")\n",
    "for doc in documents[:5]:\n",
    "    print(f\"  {doc['document_id']}: {doc['document_name'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202f807",
   "metadata": {},
   "source": [
    "## Step 2: Create Documents Table & Populate\n",
    "\n",
    "**Th·ªùi gian:** 1 gi·ªù  \n",
    "**Input:** documents_metadata.json  \n",
    "**Output:** documents table v·ªõi 70 records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837683f0",
   "metadata": {},
   "source": [
    "### üîç Pre-Migration: Visualize Current Database State\n",
    "\n",
    "Ki·ªÉm tra database tr∆∞·ªõc khi ch·∫°y migration:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b33eb",
   "metadata": {},
   "source": [
    "### üìù Step 2A: Load Metadata\n",
    "\n",
    "Load documents t·ª´ metadata file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120446f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 68 documents from metadata file\n",
      "\n",
      "üìã Sample document structure:\n",
      "{\n",
      "  \"document_id\": \"LUA-90-2025-QH15\",\n",
      "  \"document_name\": \"Luat so 90 2025-qh15\",\n",
      "  \"category\": \"Lu·∫≠t ch√≠nh\",\n",
      "  \"document_type\": \"law\",\n",
      "  \"source_file\": \"data/raw/Luat chinh/Luat so 90 2025-qh15.docx\",\n",
      "  \"file_name\": \"Luat so 90 2025-qh15.docx\",\n",
      "  \"total_chunks\": 0,\n",
      "  \"status\": \"active\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load documents metadata\n",
    "metadata_file = Path(\"data/processed/documents_metadata.json\")\n",
    "\n",
    "with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "    documents_to_insert = json.load(f)\n",
    "\n",
    "print(f\"üìÇ Loaded {len(documents_to_insert)} documents from metadata file\")\n",
    "print(f\"\\nüìã Sample document structure:\")\n",
    "print(json.dumps(documents_to_insert[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3ffb2",
   "metadata": {},
   "source": [
    "### üìù Step 2B: Create Documents Table (Split SQL)\n",
    "\n",
    "Execute SQL schema t·ª´ng statement m·ªôt ƒë·ªÉ tr√°nh l·ªói asyncpg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd206f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating documents table...\n",
      "\n",
      "üìù Found 11 SQL statements to execute\n",
      "\n",
      "üìã Statement preview:\n",
      "   [1] CREATE TABLE IF NOT EXISTS documents (...\n",
      "   [2] CREATE INDEX idx_documents_category ON documents(category);...\n",
      "   [3] CREATE INDEX idx_documents_type ON documents(document_type);...\n",
      "   [4] CREATE INDEX idx_documents_status ON documents(status);...\n",
      "   [5] CREATE INDEX idx_documents_source ON documents(source_file);...\n",
      "   [6] CREATE OR REPLACE FUNCTION update_documents_updated_at()...\n",
      "   [7] CREATE TRIGGER trigger_documents_updated_at...\n",
      "   [8] COMMENT ON TABLE documents IS 'Master table for all legal documents re...\n",
      "   [9] COMMENT ON COLUMN documents.document_id IS 'Unique identifier followin...\n",
      "   [10] COMMENT ON COLUMN documents.category IS 'One of 7 categories mapping t...\n",
      "   [11] COMMENT ON COLUMN documents.status IS 'Document status for toggle func...\n",
      "\n",
      "   [1/11] Executing: CREATE TABLE IF NOT EXISTS documents (...\n",
      "   [2/11] Executing: CREATE INDEX idx_documents_category ON documents(category);...\n",
      "      ‚è≠Ô∏è  Already exists, skipping...\n",
      "   [3/11] Executing: CREATE INDEX idx_documents_type ON documents(document_type);...\n",
      "      ‚è≠Ô∏è  Already exists, skipping...\n",
      "   [4/11] Executing: CREATE INDEX idx_documents_status ON documents(status);...\n",
      "      ‚è≠Ô∏è  Already exists, skipping...\n",
      "   [5/11] Executing: CREATE INDEX idx_documents_source ON documents(source_file);...\n",
      "      ‚è≠Ô∏è  Already exists, skipping...\n",
      "   [6/11] Executing: CREATE OR REPLACE FUNCTION update_documents_updated_at()...\n",
      "   [7/11] Executing: CREATE TRIGGER trigger_documents_updated_at...\n",
      "      ‚è≠Ô∏è  Already exists, skipping...\n",
      "   [8/11] Executing: COMMENT ON TABLE documents IS 'Master table for all legal do...\n",
      "   [9/11] Executing: COMMENT ON COLUMN documents.document_id IS 'Unique identifie...\n",
      "   [10/11] Executing: COMMENT ON COLUMN documents.category IS 'One of 7 categories...\n",
      "   [11/11] Executing: COMMENT ON COLUMN documents.status IS 'Document status for t...\n",
      "\n",
      "‚úÖ Executed 6/11 statements successfully\n",
      "‚è≠Ô∏è  Skipped 5 statements (already exist)\n",
      "\n",
      "‚úÖ Documents table setup complete!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "\n",
    "print(\"üìä Creating documents table...\\n\")\n",
    "\n",
    "# Get DATABASE_URL if not already defined\n",
    "try:\n",
    "    DATABASE_URL\n",
    "except NameError:\n",
    "    DATABASE_URL = os.getenv(\n",
    "        \"DATABASE_URL_ASYNC\",\n",
    "        \"postgresql+asyncpg://sakana:sakana123@localhost/rag_bidding_v2\"\n",
    "    )\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "# Read SQL schema file\n",
    "schema_file = Path(\"scripts/migration/001_simple_documents_schema.sql\")\n",
    "\n",
    "with open(schema_file, \"r\") as f:\n",
    "    sql = f.read()\n",
    "\n",
    "# Improved SQL parsing: handle comments properly\n",
    "statements = []\n",
    "current = \"\"\n",
    "in_function = False\n",
    "in_comment = False\n",
    "\n",
    "for line in sql.split(\"\\n\"):\n",
    "    # Skip pure comment lines at the start\n",
    "    stripped = line.strip()\n",
    "    if stripped.startswith(\"--\") and not current.strip():\n",
    "        continue\n",
    "    \n",
    "    # Track if we're inside a function definition with $$\n",
    "    if \"$$\" in line:\n",
    "        in_function = not in_function\n",
    "    \n",
    "    # Add line to current statement\n",
    "    current += line + \"\\n\"\n",
    "    \n",
    "    # Split on semicolon only if:\n",
    "    # 1. NOT inside function body\n",
    "    # 2. Line actually ends with semicolon\n",
    "    if line.rstrip().endswith(\";\") and not in_function:\n",
    "        # Clean up statement\n",
    "        stmt = current.strip()\n",
    "        if stmt and not stmt.startswith(\"--\"):\n",
    "            statements.append(stmt)\n",
    "        current = \"\"\n",
    "\n",
    "# Add last statement if exists (might not have semicolon)\n",
    "if current.strip() and not current.strip().startswith(\"--\"):\n",
    "    statements.append(current.strip())\n",
    "\n",
    "print(f\"üìù Found {len(statements)} SQL statements to execute\")\n",
    "\n",
    "# Preview statements\n",
    "print(\"\\nüìã Statement preview:\")\n",
    "for i, stmt in enumerate(statements, 1):\n",
    "    first_line = stmt.split(\"\\n\")[0].strip()\n",
    "    if first_line.startswith(\"--\"):\n",
    "        first_line = [l.strip() for l in stmt.split(\"\\n\") if l.strip() and not l.strip().startswith(\"--\")][0]\n",
    "    print(f\"   [{i}] {first_line[:70]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Execute each statement in SEPARATE transactions (idempotent)\n",
    "async def create_table():\n",
    "    executed = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    for i, stmt in enumerate(statements, 1):\n",
    "        try:\n",
    "            # Show what we're executing\n",
    "            first_line = stmt.split(\"\\n\")[0].strip()\n",
    "            if first_line.startswith(\"--\"):\n",
    "                first_line = [l.strip() for l in stmt.split(\"\\n\") if l.strip() and not l.strip().startswith(\"--\")][0]\n",
    "            \n",
    "            print(f\"   [{i}/{len(statements)}] Executing: {first_line[:60]}...\")\n",
    "            \n",
    "            # IMPORTANT: Each statement in its own transaction\n",
    "            async with engine.begin() as conn:\n",
    "                await conn.execute(text(stmt))\n",
    "            \n",
    "            executed += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"already exists\" in error_msg or \"duplicate\" in error_msg:\n",
    "                # Ignore \"already exists\" errors - idempotent\n",
    "                print(f\"      ‚è≠Ô∏è  Already exists, skipping...\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "                print(f\"      Statement preview: {stmt[:200]}...\")\n",
    "                # Don't raise - continue with other statements\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Executed {executed}/{len(statements)} statements successfully\")\n",
    "    if skipped > 0:\n",
    "        print(f\"‚è≠Ô∏è  Skipped {skipped} statements (already exist)\")\n",
    "\n",
    "await create_table()\n",
    "print(\"\\n‚úÖ Documents table setup complete!\")\n",
    "\n",
    "await engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e44e6",
   "metadata": {},
   "source": [
    "### üìù Step 2C: Insert Documents\n",
    "\n",
    "Insert 68 documents v√†o table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485ed39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Inserting 68 documents...\n",
      "\n",
      "   ‚è≠Ô∏è  Skipped (exists): LUA-90-2025-QH15\n",
      "   ‚è≠Ô∏è  Skipped (exists): LUA-57-2024-QH15\n",
      "   ‚è≠Ô∏è  Skipped (exists): LUA-H·ª¢P-NH·∫§T-126-2025-v·ªÅ\n",
      "   ‚è≠Ô∏è  Skipped (exists): LUA-Luat-dau-thau-2023\n",
      "   ‚è≠Ô∏è  Skipped (exists): ND-214-4.8.-CP\n",
      "\n",
      "============================================================\n",
      "‚úÖ Inserted: 0 documents\n",
      "‚è≠Ô∏è  Skipped: 68 documents (already exist)\n",
      "üìä Total: 68 documents in database\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "\n",
    "print(f\"üì• Inserting {len(documents_to_insert)} documents...\\n\")\n",
    "\n",
    "# Get DATABASE_URL if not already defined\n",
    "try:\n",
    "    DATABASE_URL\n",
    "except NameError:\n",
    "    DATABASE_URL = os.getenv(\n",
    "        \"DATABASE_URL_ASYNC\",\n",
    "        \"postgresql+asyncpg://sakana:sakana123@localhost/rag_bidding_v2\"\n",
    "    )\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "async def insert_documents():\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        inserted = 0\n",
    "        skipped = 0\n",
    "        \n",
    "        for doc in documents_to_insert:\n",
    "            try:\n",
    "                # Check if exists\n",
    "                result = await session.execute(\n",
    "                    text(\"SELECT id FROM documents WHERE document_id = :doc_id\"),\n",
    "                    {\"doc_id\": doc[\"document_id\"]}\n",
    "                )\n",
    "                existing = result.fetchone()\n",
    "                \n",
    "                if existing:\n",
    "                    skipped += 1\n",
    "                    if skipped <= 5:  # Show first 5\n",
    "                        print(f\"   ‚è≠Ô∏è  Skipped (exists): {doc['document_id']}\")\n",
    "                    continue\n",
    "                \n",
    "                # Insert new document\n",
    "                await session.execute(\n",
    "                    text(\"\"\"\n",
    "                        INSERT INTO documents (\n",
    "                            document_id, document_name, category, document_type,\n",
    "                            source_file, file_name, total_chunks, status\n",
    "                        ) VALUES (\n",
    "                            :document_id, :document_name, :category, :document_type,\n",
    "                            :source_file, :file_name, :total_chunks, :status\n",
    "                        )\n",
    "                    \"\"\"),\n",
    "                    {\n",
    "                        \"document_id\": doc[\"document_id\"],\n",
    "                        \"document_name\": doc[\"document_name\"],\n",
    "                        \"category\": doc[\"category\"],\n",
    "                        \"document_type\": doc[\"document_type\"],\n",
    "                        \"source_file\": doc[\"source_file\"],\n",
    "                        \"file_name\": doc[\"file_name\"],\n",
    "                        \"total_chunks\": doc.get(\"total_chunks\", 0),\n",
    "                        \"status\": doc.get(\"status\", \"active\"),\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                inserted += 1\n",
    "                \n",
    "                if inserted % 10 == 0:\n",
    "                    print(f\"   ‚úÖ Inserted {inserted}/{len(documents_to_insert)}...\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error inserting {doc['document_id']}: {e}\")\n",
    "        \n",
    "        await session.commit()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚úÖ Inserted: {inserted} documents\")\n",
    "        print(f\"‚è≠Ô∏è  Skipped: {skipped} documents (already exist)\")\n",
    "        print(f\"üìä Total: {inserted + skipped} documents in database\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "await insert_documents()\n",
    "await engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a8ac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DATABASE PRE-MIGRATION CHECK\n",
      "============================================================\n",
      "\n",
      "üìä Tables in Database:\n",
      "   ‚Ä¢ documents\n",
      "   ‚Ä¢ langchain_pg_collection\n",
      "   ‚Ä¢ langchain_pg_embedding\n",
      "\n",
      "üìÅ Documents Table: ‚úÖ EXISTS\n",
      "   Records: 59\n",
      "\n",
      "üóÑÔ∏è  Vector Database: ‚úÖ EXISTS\n",
      "   Total chunks: 4708\n",
      "   Unique document_ids: 25\n",
      "\n",
      "   üìã Sample current document_ids:\n",
      "      FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-\n",
      "      FORM-10-M·∫´u-s·ªë-10A_E-HSM\n",
      "      FORM-10-M·∫´u-s·ªë-10B_E-HSM\n",
      "      FORM-11-M·∫´u-s·ªë-11A_E-HSM\n",
      "      FORM-11-M·∫´u-s·ªë-11B_E-HSM\n",
      "      FORM-15-Phu-luc\n",
      "      FORM-7-M·∫´u-s·ªë-7A-E-HSMT\n",
      "      FORM-7-M·∫´u-s·ªë-7B-E-HSMT\n",
      "      FORM-8-M·∫´u-s·ªë-8B-E-HSMT\n",
      "      FORM-9M·∫´u-s·ªë-9A_E-HSMT_P\n",
      "\n",
      "   ‚úÖ source_file metadata: 4705/4708 (99.9%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pre-migration check complete!\n",
      "============================================================\n",
      "   ‚Ä¢ documents\n",
      "   ‚Ä¢ langchain_pg_collection\n",
      "   ‚Ä¢ langchain_pg_embedding\n",
      "\n",
      "üìÅ Documents Table: ‚úÖ EXISTS\n",
      "   Records: 59\n",
      "\n",
      "üóÑÔ∏è  Vector Database: ‚úÖ EXISTS\n",
      "   Total chunks: 4708\n",
      "   Unique document_ids: 25\n",
      "\n",
      "   üìã Sample current document_ids:\n",
      "      FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-\n",
      "      FORM-10-M·∫´u-s·ªë-10A_E-HSM\n",
      "      FORM-10-M·∫´u-s·ªë-10B_E-HSM\n",
      "      FORM-11-M·∫´u-s·ªë-11A_E-HSM\n",
      "      FORM-11-M·∫´u-s·ªë-11B_E-HSM\n",
      "      FORM-15-Phu-luc\n",
      "      FORM-7-M·∫´u-s·ªë-7A-E-HSMT\n",
      "      FORM-7-M·∫´u-s·ªë-7B-E-HSMT\n",
      "      FORM-8-M·∫´u-s·ªë-8B-E-HSMT\n",
      "      FORM-9M·∫´u-s·ªë-9A_E-HSMT_P\n",
      "\n",
      "   ‚úÖ source_file metadata: 4705/4708 (99.9%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pre-migration check complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "\n",
    "print(\"üîç DATABASE PRE-MIGRATION CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get database URL\n",
    "DATABASE_URL = os.getenv(\n",
    "    \"DATABASE_URL_ASYNC\",\n",
    "    \"postgresql+asyncpg://sakana:sakana123@localhost/rag_bidding_v2\"\n",
    ")\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "async def check_database():\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        # 1. List all tables\n",
    "        print(\"\\nüìä Tables in Database:\")\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT tablename \n",
    "            FROM pg_tables \n",
    "            WHERE schemaname = 'public'\n",
    "            ORDER BY tablename\n",
    "        \"\"\"))\n",
    "        tables = [row[0] for row in result.fetchall()]\n",
    "        for table in tables:\n",
    "            print(f\"   ‚Ä¢ {table}\")\n",
    "        \n",
    "        # 2. Check if documents table exists\n",
    "        has_documents = 'documents' in tables\n",
    "        print(f\"\\nüìÅ Documents Table: {'‚úÖ EXISTS' if has_documents else '‚ùå NOT FOUND'}\")\n",
    "        \n",
    "        if has_documents:\n",
    "            # Count documents\n",
    "            result = await session.execute(text(\"SELECT COUNT(*) FROM documents\"))\n",
    "            doc_count = result.scalar()\n",
    "            print(f\"   Records: {doc_count}\")\n",
    "        \n",
    "        # 3. Check vector database (langchain_pg_embedding)\n",
    "        has_vectors = 'langchain_pg_embedding' in tables\n",
    "        print(f\"\\nüóÑÔ∏è  Vector Database: {'‚úÖ EXISTS' if has_vectors else '‚ùå NOT FOUND'}\")\n",
    "        \n",
    "        if has_vectors:\n",
    "            # Count chunks\n",
    "            result = await session.execute(text(\"\"\"\n",
    "                SELECT COUNT(*) FROM langchain_pg_embedding\n",
    "            \"\"\"))\n",
    "            chunk_count = result.scalar()\n",
    "            print(f\"   Total chunks: {chunk_count}\")\n",
    "            \n",
    "            # Count distinct document_ids\n",
    "            result = await session.execute(text(\"\"\"\n",
    "                SELECT COUNT(DISTINCT cmetadata->>'document_id')\n",
    "                FROM langchain_pg_embedding\n",
    "            \"\"\"))\n",
    "            unique_docs = result.scalar()\n",
    "            print(f\"   Unique document_ids: {unique_docs}\")\n",
    "            \n",
    "            # Sample current document_ids\n",
    "            result = await session.execute(text(\"\"\"\n",
    "                SELECT DISTINCT cmetadata->>'document_id'\n",
    "                FROM langchain_pg_embedding\n",
    "                ORDER BY cmetadata->>'document_id'\n",
    "                LIMIT 10\n",
    "            \"\"\"))\n",
    "            \n",
    "            print(f\"\\n   üìã Sample current document_ids:\")\n",
    "            for row in result.fetchall():\n",
    "                doc_id = row[0]\n",
    "                print(f\"      {doc_id}\")\n",
    "            \n",
    "            # Check for source_file metadata\n",
    "            result = await session.execute(text(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) FILTER (WHERE cmetadata->>'source_file' IS NOT NULL) as with_source,\n",
    "                    COUNT(*) as total\n",
    "                FROM langchain_pg_embedding\n",
    "            \"\"\"))\n",
    "            row = result.fetchone()\n",
    "            coverage = (row[0] / row[1] * 100) if row[1] > 0 else 0\n",
    "            status = \"‚úÖ\" if coverage > 90 else \"‚ö†Ô∏è\"\n",
    "            print(f\"\\n   {status} source_file metadata: {row[0]}/{row[1]} ({coverage:.1f}%)\")\n",
    "\n",
    "await check_database()\n",
    "await engine.dispose()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Pre-migration check complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af33cfa",
   "metadata": {},
   "source": [
    "### ‚úÖ Expected Output (Step 2)\n",
    "\n",
    "Sau khi ch·∫°y c√°c cell tr√™n, b·∫°n s·∫Ω th·∫•y:\n",
    "\n",
    "**Step 2A - Load Metadata:**\n",
    "```\n",
    "üìÇ Loaded 68 documents from metadata file\n",
    "```\n",
    "\n",
    "**Step 2B - Create Table:**\n",
    "```\n",
    "üìä Creating documents table...\n",
    "üìù Found 15 SQL statements to execute\n",
    "\n",
    "   [1/15] CREATE TABLE IF NOT EXISTS documents (...\n",
    "   [2/15] CREATE INDEX idx_documents_category ON documents(category...\n",
    "   ...\n",
    "‚úÖ Executed 15 statements successfully\n",
    "‚úÖ Documents table created!\n",
    "```\n",
    "\n",
    "**Step 2C - Insert Documents:**\n",
    "```\n",
    "üì• Inserting 68 documents...\n",
    "   ‚úÖ Inserted 10/68...\n",
    "   ‚úÖ Inserted 20/68...\n",
    "   ...\n",
    "‚úÖ Inserted: 68 documents\n",
    "‚è≠Ô∏è  Skipped: 0 documents\n",
    "üìä Total: 68 documents in database\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Verify k·∫øt qu·∫£:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112f518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total documents in table: 59\n",
      "\n",
      "üìÅ By category:\n",
      "   H·ªì s∆° m·ªùi th·∫ßu: 37 documents\n",
      "   M·∫´u b√°o c√°o: 10 documents\n",
      "   C√¢u h·ªèi thi: 4 documents\n",
      "   Lu·∫≠t ch√≠nh: 4 documents\n",
      "   Th√¥ng t∆∞: 2 documents\n",
      "   Ngh·ªã ƒë·ªãnh: 1 documents\n",
      "   Quy·∫øt ƒë·ªãnh: 1 documents\n",
      "\n",
      "üìÑ Sample:\n",
      "   EXAM-Ng√¢n-h√†ng-c√¢u-h·ªèi-CC: Ng√¢n h√†ng c√¢u h·ªèi CCDT ƒë·ª£t 2... (C√¢u h·ªèi thi)\n",
      "   EXAM-Ng√¢n-h√†ng-c√¢u-h·ªèi-th: Ng√¢n h√†ng c√¢u h·ªèi thi CCDT ƒë·ª£t 1... (C√¢u h·ªèi thi)\n",
      "   EXAM-NHCH_2692025_dot-2: NHCH 26.9.2025 dot 2- b·ªï sung... (C√¢u h·ªèi thi)\n",
      "   EXAM-NHCH_30925_bo_sung: NHCH 30.9.25 bo sung theo TB1952 qldt (1... (C√¢u h·ªèi thi)\n",
      "   FORM-01-Ph·ª•-l·ª•c: 01. Ph·ª• l·ª•c... (H·ªì s∆° m·ªùi th·∫ßu)\n",
      "   FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-: 04.1A. M·∫´u K·∫ø ho·∫°ch ki·ªÉm tra ƒë·ªãnh k·ª≥... (H·ªì s∆° m·ªùi th·∫ßu)\n",
      "   FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-: 04.1B. M·∫´u K·∫ø ho·∫°ch ki·ªÉm tra chi ti·∫øt... (H·ªì s∆° m·ªùi th·∫ßu)\n",
      "   FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b: 04.2. M·∫´u ƒê·ªÅ c∆∞∆°ng b√°o c√°o ƒë·∫•u th·∫ßu l·ª±a ... (H·ªì s∆° m·ªùi th·∫ßu)\n",
      "   FORM-043-M·∫´u-B√°o-c√°o-ki: 04.3. M·∫´u B√°o c√°o ki·ªÉm tra ƒë·ªëi v·ªõi l·ª±a c... (H·ªì s∆° m·ªùi th·∫ßu)\n",
      "   FORM-044-M·∫´u-K·∫øt-lu·∫≠n-k: 04.4. M·∫´u K·∫øt lu·∫≠n ki·ªÉm tra ƒë·ªëi v·ªõi l·ª±a ... (H·ªì s∆° m·ªùi th·∫ßu)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "\n",
    "# Get database URL with password\n",
    "DATABASE_URL = os.getenv(\n",
    "    \"DATABASE_URL_ASYNC\",\n",
    "    \"postgresql+asyncpg://sakana:sakana123@localhost/rag_bidding_v2\"\n",
    ")\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "async def verify_documents_table():\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        # Count total\n",
    "        result = await session.execute(text(\"SELECT COUNT(*) FROM documents\"))\n",
    "        total = result.scalar()\n",
    "        print(f\"‚úÖ Total documents in table: {total}\")\n",
    "        \n",
    "        # Count by category\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT category, COUNT(*) as count\n",
    "            FROM documents\n",
    "            GROUP BY category\n",
    "            ORDER BY count DESC\n",
    "        \"\"\"))\n",
    "        \n",
    "        print(\"\\nüìÅ By category:\")\n",
    "        for row in result.fetchall():\n",
    "            print(f\"   {row[0]}: {row[1]} documents\")\n",
    "        \n",
    "        # Sample documents\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT document_id, document_name, category\n",
    "            FROM documents\n",
    "            ORDER BY category, document_id\n",
    "            LIMIT 10\n",
    "        \"\"\"))\n",
    "        \n",
    "        print(\"\\nüìÑ Sample:\")\n",
    "        for row in result.fetchall():\n",
    "            print(f\"   {row[0]}: {row[1][:40]}... ({row[2]})\")\n",
    "\n",
    "await verify_documents_table()\n",
    "await engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97bdd2",
   "metadata": {},
   "source": [
    "## Step 3: Build Chunk ‚Üí Document Mapping\n",
    "\n",
    "**Th·ªùi gian:** 30 ph√∫t  \n",
    "**Input:** metadata files + documents table  \n",
    "**Output:** chunk_to_document_mapping.json\n",
    "\n",
    "**Strategy:**\n",
    "- S·ª≠ d·ª•ng metadata files (63 files) l√†m bridge\n",
    "- Map: chunk_file ‚Üí source_file ‚Üí document_id\n",
    "- M·ªói chunk trong file s·∫Ω c√≥ c√πng document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f2edb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Building chunk ‚Üí document mapping...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 59 documents from database\n",
      "‚úÖ Found 63 metadata files\n",
      "\n",
      "‚úÖ Built mapping for 55 chunk files\n",
      "\n",
      "üìã Sample mappings:\n",
      "   05._M·∫´u_B√°o_c√°o_ƒë·∫•u_th·∫ßu.jsonl ‚Üí FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-\n",
      "   14A._M·∫´u_BCƒêG_PTV_HH_XL_hop_hop_TBYT_CGTT._quy_tr√¨nh_1_1_tui.jsonl ‚Üí TEMPLATE-BC-14A\n",
      "   4._M·∫´u_s·ªë_4B_E-HSMT_h√†ng_h√≥a_2_t√∫i.jsonl ‚Üí FORM-HSYC-HANGHOA-4\n",
      "   8._M·∫´u_s·ªë_8B._E-HSMT_EC_02_t√∫i.jsonl ‚Üí FORM-8-M·∫´u-s·ªë-8B-E-HSMT\n",
      "   01D._M·∫´u_HSYC_T∆∞_v·∫•n.jsonl ‚Üí FORM-HSYC-TUVAN-01D\n",
      "\n",
      "üíæ Saved to: data/processed/chunk_to_document_mapping.json\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üî® Building chunk ‚Üí document mapping...\\n\")\n",
    "\n",
    "# Get DATABASE_URL if not already defined\n",
    "try:\n",
    "    DATABASE_URL\n",
    "except NameError:\n",
    "    DATABASE_URL = os.getenv(\n",
    "        \"DATABASE_URL_ASYNC\",\n",
    "        \"postgresql+asyncpg://sakana:sakana123@localhost/rag_bidding_v2\"\n",
    "    )\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "# Load documents from database\n",
    "async def load_documents():\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT document_id, source_file, file_name\n",
    "            FROM documents\n",
    "            ORDER BY document_id\n",
    "        \"\"\"))\n",
    "        return result.fetchall()\n",
    "\n",
    "db_documents = await load_documents()\n",
    "print(f\"‚úÖ Loaded {len(db_documents)} documents from database\")\n",
    "\n",
    "# Load metadata files (now in project root)\n",
    "METADATA_DIR = Path(\"data/processed/metadata\")\n",
    "metadata_files = list(METADATA_DIR.glob(\"*.json\"))\n",
    "print(f\"‚úÖ Found {len(metadata_files)} metadata files\")\n",
    "\n",
    "# Build mapping: chunk_file ‚Üí document_id\n",
    "chunk_mapping = {}  # {chunk_file_path: document_id}\n",
    "source_to_doc = {doc[1]: doc[0] for doc in db_documents}  # {source_file: document_id}\n",
    "\n",
    "for meta_file in metadata_files:\n",
    "    with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    source_file = metadata.get(\"source_file\")\n",
    "    chunk_file = metadata.get(\"output_file\")\n",
    "    \n",
    "    if source_file in source_to_doc:\n",
    "        document_id = source_to_doc[source_file]\n",
    "        chunk_mapping[chunk_file] = document_id\n",
    "\n",
    "print(f\"\\n‚úÖ Built mapping for {len(chunk_mapping)} chunk files\")\n",
    "print(f\"\\nüìã Sample mappings:\")\n",
    "for i, (chunk_file, doc_id) in enumerate(list(chunk_mapping.items())[:5]):\n",
    "    chunk_name = Path(chunk_file).name\n",
    "    print(f\"   {chunk_name} ‚Üí {doc_id}\")\n",
    "\n",
    "# Save mapping (project root)\n",
    "mapping_output = Path(\"data/processed/chunk_to_document_mapping.json\")\n",
    "with open(mapping_output, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Saved to: {mapping_output}\")\n",
    "\n",
    "await engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ae3e6",
   "metadata": {},
   "source": [
    "## Step 4: Update Chunk Document IDs in Database\n",
    "\n",
    "**Th·ªùi gian:** 2 gi·ªù  \n",
    "**Input:** chunk_to_document_mapping.json  \n",
    "**Output:** Updated 4708 chunks with new document_ids\n",
    "\n",
    "**Strategy:**\n",
    "1. Read chunks t·ª´ chunk files\n",
    "2. Generate new document_id v√† chunk_id\n",
    "3. Update database chunks (keep embeddings!)\n",
    "4. Add source_file metadata\n",
    "\n",
    "**‚ö†Ô∏è Important:** Kh√¥ng ƒë·ªông v√†o embeddings, ch·ªâ update cmetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e1099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Updating chunk document_ids in database...\n",
      "\n",
      "üìÇ Loaded mapping for 55 chunk files\n",
      "üìã Loaded metadata for 63 files\n",
      "\n",
      "‚úÖ Connected to database\n",
      "\n",
      "  üìä Processed 100 chunks, updated 0...\n",
      "  üìä Processed 100 chunks, updated 0...\n",
      "  üìä Processed 200 chunks, updated 0...\n",
      "  üìä Processed 200 chunks, updated 0...\n",
      "  üìä Processed 300 chunks, updated 0...\n",
      "  üìä Processed 300 chunks, updated 0...\n",
      "  üìä Processed 400 chunks, updated 0...\n",
      "  üìä Processed 400 chunks, updated 0...\n",
      "  üìä Processed 500 chunks, updated 0...\n",
      "  üìä Processed 500 chunks, updated 0...\n",
      "  üìä Processed 600 chunks, updated 0...\n",
      "  üìä Processed 600 chunks, updated 0...\n",
      "  üìä Processed 700 chunks, updated 0...\n",
      "  üìä Processed 700 chunks, updated 0...\n",
      "  üìä Processed 800 chunks, updated 0...\n",
      "  üìä Processed 800 chunks, updated 0...\n",
      "  üìä Processed 900 chunks, updated 0...\n",
      "  üìä Processed 900 chunks, updated 0...\n",
      "  üìä Processed 1000 chunks, updated 0...\n",
      "  üìä Processed 1000 chunks, updated 0...\n",
      "  üìä Processed 1100 chunks, updated 0...\n",
      "  üìä Processed 1100 chunks, updated 0...\n",
      "  üìä Processed 1200 chunks, updated 0...\n",
      "  üìä Processed 1200 chunks, updated 0...\n",
      "  üìä Processed 1300 chunks, updated 0...\n",
      "  üìä Processed 1300 chunks, updated 0...\n",
      "  üìä Processed 1400 chunks, updated 0...\n",
      "  üìä Processed 1400 chunks, updated 0...\n",
      "  üìä Processed 1500 chunks, updated 0...\n",
      "  üìä Processed 1500 chunks, updated 0...\n",
      "  üìä Processed 1600 chunks, updated 0...\n",
      "  üìä Processed 1600 chunks, updated 0...\n",
      "  üìä Processed 1700 chunks, updated 0...\n",
      "  üìä Processed 1700 chunks, updated 0...\n",
      "  üìä Processed 1800 chunks, updated 0...\n",
      "  üìä Processed 1800 chunks, updated 0...\n",
      "  üìä Processed 1900 chunks, updated 0...\n",
      "  üìä Processed 1900 chunks, updated 0...\n",
      "  üìä Processed 2000 chunks, updated 0...\n",
      "  üìä Processed 2000 chunks, updated 0...\n",
      "  üìä Processed 2100 chunks, updated 0...\n",
      "  üìä Processed 2100 chunks, updated 0...\n",
      "  üìä Processed 2200 chunks, updated 0...\n",
      "  üìä Processed 2200 chunks, updated 0...\n",
      "  üìä Processed 2300 chunks, updated 0...\n",
      "  üìä Processed 2300 chunks, updated 0...\n",
      "  üìä Processed 2400 chunks, updated 0...\n",
      "  üìä Processed 2400 chunks, updated 0...\n",
      "  üìä Processed 2500 chunks, updated 0...\n",
      "  üìä Processed 2500 chunks, updated 0...\n",
      "  üìä Processed 2600 chunks, updated 0...\n",
      "  üìä Processed 2600 chunks, updated 0...\n",
      "  üìä Processed 2700 chunks, updated 0...\n",
      "  üìä Processed 2700 chunks, updated 0...\n",
      "  üìä Processed 2800 chunks, updated 0...\n",
      "  üìä Processed 2800 chunks, updated 0...\n",
      "  üìä Processed 2900 chunks, updated 0...\n",
      "  üìä Processed 2900 chunks, updated 0...\n",
      "  üìä Processed 3000 chunks, updated 0...\n",
      "  üìä Processed 3000 chunks, updated 0...\n",
      "  üìä Processed 3100 chunks, updated 0...\n",
      "  üìä Processed 3100 chunks, updated 0...\n",
      "  üìä Processed 3200 chunks, updated 0...\n",
      "  üìä Processed 3200 chunks, updated 0...\n",
      "  üìä Processed 3300 chunks, updated 0...\n",
      "  üìä Processed 3300 chunks, updated 0...\n",
      "  üìä Processed 3400 chunks, updated 0...\n",
      "  üìä Processed 3400 chunks, updated 0...\n",
      "  üìä Processed 3500 chunks, updated 0...\n",
      "  üìä Processed 3500 chunks, updated 0...\n",
      "  üìä Processed 3600 chunks, updated 0...\n",
      "  üìä Processed 3600 chunks, updated 0...\n",
      "  üìä Processed 3700 chunks, updated 0...\n",
      "  üìä Processed 3700 chunks, updated 0...\n",
      "  üìä Processed 3800 chunks, updated 0...\n",
      "  üìä Processed 3800 chunks, updated 0...\n",
      "  üìä Processed 3900 chunks, updated 0...\n",
      "  üìä Processed 3900 chunks, updated 0...\n",
      "  üìä Processed 4000 chunks, updated 0...\n",
      "  üìä Processed 4000 chunks, updated 0...\n",
      "  üìä Processed 4100 chunks, updated 0...\n",
      "\n",
      "‚úÖ All chunks processed!\n",
      "‚úÖ Database connection closed\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ Processed 4114 chunks\n",
      "‚úÖ Updated 0 chunks in database\n",
      "  üìä Processed 4100 chunks, updated 0...\n",
      "\n",
      "‚úÖ All chunks processed!\n",
      "‚úÖ Database connection closed\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ Processed 4114 chunks\n",
      "‚úÖ Updated 0 chunks in database\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import asyncpg\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üöÄ Updating chunk document_ids in database...\\n\")\n",
    "\n",
    "# Load mapping\n",
    "with open(mapping_output, 'r', encoding='utf-8') as f:\n",
    "    chunk_mapping = json.load(f)\n",
    "\n",
    "print(f\"üìÇ Loaded mapping for {len(chunk_mapping)} chunk files\")\n",
    "\n",
    "# Load metadata for source_file info\n",
    "metadata_map = {}  # {chunk_file: metadata}\n",
    "for meta_file in metadata_files:\n",
    "    with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    chunk_file = metadata.get(\"output_file\")\n",
    "    if chunk_file:\n",
    "        metadata_map[chunk_file] = metadata\n",
    "\n",
    "print(f\"üìã Loaded metadata for {len(metadata_map)} files\\n\")\n",
    "\n",
    "# Database connection params\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"rag_bidding_v2\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"sakana\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"sakana123\")\n",
    "\n",
    "async def update_chunks_v5():\n",
    "    \"\"\"Version 5 - Pure asyncpg (no SQLAlchemy) - simpler parameter binding\"\"\"\n",
    "    total_updated = 0\n",
    "    total_chunks = 0\n",
    "    errors = []\n",
    "    \n",
    "    # Connect directly with asyncpg\n",
    "    conn = await asyncpg.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        database=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(f\"‚úÖ Connected to database\\n\")\n",
    "        \n",
    "        for chunk_file, new_doc_id in chunk_mapping.items():\n",
    "            # Get metadata\n",
    "            metadata = metadata_map.get(chunk_file)\n",
    "            if not metadata:\n",
    "                errors.append(f\"No metadata for {chunk_file}\")\n",
    "                continue\n",
    "            \n",
    "            source_file = metadata.get(\"source_file\")\n",
    "            chunk_file_path = Path(chunk_file)\n",
    "            \n",
    "            if not chunk_file_path.exists():\n",
    "                errors.append(f\"Chunk file not found: {chunk_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Read chunks from file\n",
    "            with open(chunk_file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "                        old_chunk_id = chunk.get(\"chunk_id\")\n",
    "                        \n",
    "                        if not old_chunk_id:\n",
    "                            continue\n",
    "                        \n",
    "                        # Generate new chunk_id\n",
    "                        parts = old_chunk_id.split(\"_\")\n",
    "                        if len(parts) >= 3:\n",
    "                            chunk_suffix = \"_\".join(parts[2:])\n",
    "                        else:\n",
    "                            chunk_suffix = old_chunk_id\n",
    "                        \n",
    "                        new_chunk_id = f\"{new_doc_id}_{chunk_suffix}\"\n",
    "                        \n",
    "                        # Update with asyncpg - uses $1, $2, $3, $4 positional params\n",
    "                        query = \"\"\"\n",
    "                            UPDATE langchain_pg_embedding\n",
    "                            SET cmetadata = jsonb_set(\n",
    "                                jsonb_set(\n",
    "                                    jsonb_set(\n",
    "                                        cmetadata,\n",
    "                                        '{document_id}',\n",
    "                                        to_jsonb($1::text)\n",
    "                                    ),\n",
    "                                    '{chunk_id}',\n",
    "                                    to_jsonb($2::text)\n",
    "                                ),\n",
    "                                '{source_file}',\n",
    "                                to_jsonb($3::text)\n",
    "                            )\n",
    "                            WHERE cmetadata->>'chunk_id' = $4\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        result = await conn.execute(\n",
    "                            query,\n",
    "                            new_doc_id,\n",
    "                            new_chunk_id,\n",
    "                            source_file,\n",
    "                            old_chunk_id\n",
    "                        )\n",
    "                        \n",
    "                        # result format: \"UPDATE 1\" or \"UPDATE 0\"\n",
    "                        rows_affected = int(result.split()[-1])\n",
    "                        if rows_affected > 0:\n",
    "                            total_updated += 1\n",
    "                        \n",
    "                        total_chunks += 1\n",
    "                        \n",
    "                        # Progress update\n",
    "                        if total_chunks % 100 == 0:\n",
    "                            print(f\"  üìä Processed {total_chunks} chunks, updated {total_updated}...\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        errors.append(f\"Error processing chunk in {chunk_file}: {e}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ All chunks processed!\")\n",
    "        \n",
    "    finally:\n",
    "        await conn.close()\n",
    "        print(f\"‚úÖ Database connection closed\\n\")\n",
    "    \n",
    "    return total_updated, total_chunks, errors\n",
    "\n",
    "# Run the update\n",
    "updated, processed, errors = await update_chunks_v5()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ Processed {processed} chunks\")\n",
    "print(f\"‚úÖ Updated {updated} chunks in database\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\n‚ö†Ô∏è  Errors: {len(errors)}\")\n",
    "    for err in errors[:5]:\n",
    "        print(f\"   {err}\")\n",
    "    if len(errors) > 5:\n",
    "        print(f\"   ... and {len(errors) - 5} more errors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e43d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# NG√ÄY 2: Verification + API Updates\n",
    "\n",
    "**Th·ªùi gian:** 4 gi·ªù  \n",
    "**M·ª•c ti√™u:** Verify migration + Update API endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2703a8e",
   "metadata": {},
   "source": [
    "## Step 5: Verify Migration\n",
    "\n",
    "**Th·ªùi gian:** 1 gi·ªù  \n",
    "**Ki·ªÉm tra:**\n",
    "1. S·ªë l∆∞·ª£ng documents trong database\n",
    "2. source_file ƒë∆∞·ª£c populate\n",
    "3. document_id format ƒë√∫ng\n",
    "4. chunk_id format ƒë√∫ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b04d18c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying migration...\n",
      "\n",
      "‚úÖ Distinct document_ids: 25\n",
      "   Expected: 70, Actual: 25\n",
      "\n",
      "‚úÖ Source file tracking:\n",
      "   With source_file: 4705 / 4708\n",
      "   Coverage: 99.9%\n",
      "\n",
      "üìã Sample documents in database:\n",
      "   FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-       |  130 chunks | 05. M·∫´u B√°o c√°o ƒë·∫•u th·∫ßu.docx\n",
      "   FORM-10-M·∫´u-s·ªë-10A_E-HSM       |    6 chunks | 10. M·∫´u s·ªë 10A_E-HSMT_EPC 01 t√∫i.docx\n",
      "   FORM-10-M·∫´u-s·ªë-10B_E-HSM       |  104 chunks | 10. M·∫´u s·ªë 10B_E-HSMT_EPC 2 t√∫i.docx\n",
      "   FORM-11-M·∫´u-s·ªë-11A_E-HSM       |   23 chunks | 11. M·∫´u s·ªë 11A_E-HSMT_M√°y ƒë·∫∑t m√°y m∆∞·ª£n 1\n",
      "   FORM-11-M·∫´u-s·ªë-11B_E-HSM       |    9 chunks | 11. M·∫´u s·ªë 11B_E-HSMT_M√°y ƒë·∫∑t m√°y m∆∞·ª£n 2\n",
      "   FORM-15-Phu-luc                |  153 chunks | 15. Phu luc.docx\n",
      "   FORM-7-M·∫´u-s·ªë-7A-E-HSMT        |    9 chunks | 7. M·∫´u s·ªë 7A. E-HSMT_EP qua m·∫°ng 01 t√∫i.\n",
      "   FORM-7-M·∫´u-s·ªë-7B-E-HSMT        |  264 chunks | 7. M·∫´u s·ªë 7B. E-HSMT_EP qua m·∫°ng 2 t√∫i.d\n",
      "   FORM-8-M·∫´u-s·ªë-8B-E-HSMT        |  166 chunks | 8. M·∫´u s·ªë 8B. E-HSMT_EC 02 t√∫i.docx\n",
      "   FORM-9M·∫´u-s·ªë-9A_E-HSMT_P       |   16 chunks | 9.M·∫´u s·ªë 9A_E-HSMT_PC 1 t√∫i.docx\n",
      "   FORM-9-M·∫´u-s·ªë-9B_-E-HSMT       |    4 chunks | 9. M·∫´u s·ªë 9B_ E-HSMT_PC 2 t√∫i.docx\n",
      "   FORM-Bidding/2025#bee720       |    3 chunks | (NULL)\n",
      "   FORM-HSYC-HANGHOA-4            | 1293 chunks | 4. M·∫´u s·ªë 4B E-HSMT h√†ng h√≥a 2 t√∫i.docx\n",
      "   FORM-HSYC-TUVAN-01D            |  134 chunks | 01D. M·∫´u HSYC T∆∞ v·∫•n.docx\n",
      "   FORM-HSYC-XAYLAP-01A           |    3 chunks | 01A. M·∫´u HSYC X√¢y l·∫Øp.docx\n",
      "\n",
      "üìÑ Sample chunk_ids:\n",
      "   LUA-57-2024-QH15_dieu_0143\n",
      "   FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-_section_0000\n",
      "   FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-_section_0000\n",
      "   FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-_section_0000\n",
      "   FORM-HSYC-TUVAN-01D_form_0004\n",
      "\n",
      "‚ö†Ô∏è  Old format document_ids: 0\n",
      "\n",
      "üìã Sample documents in database:\n",
      "   FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-       |  130 chunks | 05. M·∫´u B√°o c√°o ƒë·∫•u th·∫ßu.docx\n",
      "   FORM-10-M·∫´u-s·ªë-10A_E-HSM       |    6 chunks | 10. M·∫´u s·ªë 10A_E-HSMT_EPC 01 t√∫i.docx\n",
      "   FORM-10-M·∫´u-s·ªë-10B_E-HSM       |  104 chunks | 10. M·∫´u s·ªë 10B_E-HSMT_EPC 2 t√∫i.docx\n",
      "   FORM-11-M·∫´u-s·ªë-11A_E-HSM       |   23 chunks | 11. M·∫´u s·ªë 11A_E-HSMT_M√°y ƒë·∫∑t m√°y m∆∞·ª£n 1\n",
      "   FORM-11-M·∫´u-s·ªë-11B_E-HSM       |    9 chunks | 11. M·∫´u s·ªë 11B_E-HSMT_M√°y ƒë·∫∑t m√°y m∆∞·ª£n 2\n",
      "   FORM-15-Phu-luc                |  153 chunks | 15. Phu luc.docx\n",
      "   FORM-7-M·∫´u-s·ªë-7A-E-HSMT        |    9 chunks | 7. M·∫´u s·ªë 7A. E-HSMT_EP qua m·∫°ng 01 t√∫i.\n",
      "   FORM-7-M·∫´u-s·ªë-7B-E-HSMT        |  264 chunks | 7. M·∫´u s·ªë 7B. E-HSMT_EP qua m·∫°ng 2 t√∫i.d\n",
      "   FORM-8-M·∫´u-s·ªë-8B-E-HSMT        |  166 chunks | 8. M·∫´u s·ªë 8B. E-HSMT_EC 02 t√∫i.docx\n",
      "   FORM-9M·∫´u-s·ªë-9A_E-HSMT_P       |   16 chunks | 9.M·∫´u s·ªë 9A_E-HSMT_PC 1 t√∫i.docx\n",
      "   FORM-9-M·∫´u-s·ªë-9B_-E-HSMT       |    4 chunks | 9. M·∫´u s·ªë 9B_ E-HSMT_PC 2 t√∫i.docx\n",
      "   FORM-Bidding/2025#bee720       |    3 chunks | (NULL)\n",
      "   FORM-HSYC-HANGHOA-4            | 1293 chunks | 4. M·∫´u s·ªë 4B E-HSMT h√†ng h√≥a 2 t√∫i.docx\n",
      "   FORM-HSYC-TUVAN-01D            |  134 chunks | 01D. M·∫´u HSYC T∆∞ v·∫•n.docx\n",
      "   FORM-HSYC-XAYLAP-01A           |    3 chunks | 01A. M·∫´u HSYC X√¢y l·∫Øp.docx\n",
      "\n",
      "üìÑ Sample chunk_ids:\n",
      "   LUA-57-2024-QH15_dieu_0143\n",
      "   FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-_section_0000\n",
      "   FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-_section_0000\n",
      "   FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-_section_0000\n",
      "   FORM-HSYC-TUVAN-01D_form_0004\n",
      "\n",
      "‚ö†Ô∏è  Old format document_ids: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Verifying migration...\\n\")\n",
    "\n",
    "# Get DATABASE_URL if not already defined\n",
    "try:\n",
    "    DATABASE_URL\n",
    "except NameError:\n",
    "    DATABASE_URL = os.getenv(\n",
    "        \"DATABASE_URL_ASYNC\",\n",
    "        \"postgresql+asyncpg://sakana:sakana123@localhost/rag_bidding_v2\"\n",
    "    )\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "async def verify_migration():\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        # 1. Count distinct document_ids\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT COUNT(DISTINCT cmetadata->>'document_id')\n",
    "            FROM langchain_pg_embedding\n",
    "        \"\"\"))\n",
    "        doc_count = result.scalar()\n",
    "        print(f\"‚úÖ Distinct document_ids: {doc_count}\")\n",
    "        print(f\"   Expected: 70, Actual: {doc_count}\")\n",
    "        \n",
    "        # 2. Check source_file populated\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) FILTER (WHERE cmetadata->>'source_file' IS NOT NULL) as with_source,\n",
    "                COUNT(*) as total\n",
    "            FROM langchain_pg_embedding\n",
    "        \"\"\"))\n",
    "        row = result.fetchone()\n",
    "        print(f\"\\n‚úÖ Source file tracking:\")\n",
    "        print(f\"   With source_file: {row[0]} / {row[1]}\")\n",
    "        print(f\"   Coverage: {row[0]/row[1]*100:.1f}%\")\n",
    "        \n",
    "        # 3. Sample document_ids\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                cmetadata->>'document_id' as doc_id,\n",
    "                cmetadata->>'source_file' as source,\n",
    "                COUNT(*) as chunks\n",
    "            FROM langchain_pg_embedding\n",
    "            GROUP BY cmetadata->>'document_id', cmetadata->>'source_file'\n",
    "            ORDER BY doc_id\n",
    "            LIMIT 15\n",
    "        \"\"\"))\n",
    "        \n",
    "        print(f\"\\nüìã Sample documents in database:\")\n",
    "        for row in result.fetchall():\n",
    "            doc_id = row[0]\n",
    "            source = row[1]\n",
    "            chunks = row[2]\n",
    "            source_display = source.split('/')[-1][:40] if source else \"(NULL)\"\n",
    "            print(f\"   {doc_id:30} | {chunks:4} chunks | {source_display}\")\n",
    "        \n",
    "        # 4. Sample chunk_ids\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT cmetadata->>'chunk_id'\n",
    "            FROM langchain_pg_embedding\n",
    "            LIMIT 5\n",
    "        \"\"\"))\n",
    "        \n",
    "        print(f\"\\nüìÑ Sample chunk_ids:\")\n",
    "        for row in result.fetchall():\n",
    "            print(f\"   {row[0]}\")\n",
    "        \n",
    "        # 5. Check for old format (should be 0)\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM langchain_pg_embedding\n",
    "            WHERE cmetadata->>'document_id' LIKE '%untitled%'\n",
    "        \"\"\"))\n",
    "        old_format = result.scalar()\n",
    "        print(f\"\\n‚ö†Ô∏è  Old format document_ids: {old_format}\")\n",
    "        if old_format > 0:\n",
    "            print(f\"   Warning: Still have {old_format} chunks with old format!\")\n",
    "\n",
    "await verify_migration()\n",
    "await engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24412eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DETAILED MIGRATION ANALYSIS\n",
      "\n",
      "============================================================\n",
      "üìä Vector Database Analysis:\n",
      "   Total distinct document_ids: 25\n",
      "\n",
      "üìã Document IDs by prefix:\n",
      "\n",
      "   FORM: 16 documents\n",
      "      FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-                 ( 130 chunks)\n",
      "      FORM-10-M·∫´u-s·ªë-10A_E-HSM                 (   6 chunks)\n",
      "      FORM-10-M·∫´u-s·ªë-10B_E-HSM                 ( 104 chunks)\n",
      "      FORM-11-M·∫´u-s·ªë-11A_E-HSM                 (  23 chunks)\n",
      "      FORM-11-M·∫´u-s·ªë-11B_E-HSM                 (   9 chunks)\n",
      "      ... and 11 more\n",
      "\n",
      "   LUA: 4 documents\n",
      "      LUA-57-2024-QH15                         ( 271 chunks)\n",
      "      LUA-90-2025-QH15                         (  78 chunks)\n",
      "      LUA-H·ª¢P-NH·∫§T-126-2025-v·ªÅ                 ( 617 chunks)\n",
      "      LUA-Luat-dau-thau-2023                   ( 188 chunks)\n",
      "\n",
      "   ND: 1 documents\n",
      "      ND-214-4.8.-CP                           ( 595 chunks)\n",
      "\n",
      "   QD: 1 documents\n",
      "      QD-1667-BYT                              (   5 chunks)\n",
      "\n",
      "   TEMPLATE: 1 documents\n",
      "      TEMPLATE-BC-14A                          ( 510 chunks)\n",
      "\n",
      "   TT: 2 documents\n",
      "      TT-00-Quy·∫øt-ƒë·ªãnh-Th√¥ng-t∆∞                (  25 chunks)\n",
      "      TT-0-L·ªùi-vƒÉn-th√¥ng-t∆∞                    (  98 chunks)\n",
      "\n",
      "============================================================\n",
      "üìÇ Mapping File Analysis:\n",
      "\n",
      "   Total chunk files in mapping: 55\n",
      "   Unique document_ids in mapping: 55\n",
      "\n",
      "   Top 10 document_ids by chunk file count:\n",
      "      FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-                 (1 chunk files)\n",
      "      TEMPLATE-BC-14A                          (1 chunk files)\n",
      "      FORM-HSYC-HANGHOA-4                      (1 chunk files)\n",
      "      FORM-8-M·∫´u-s·ªë-8B-E-HSMT                  (1 chunk files)\n",
      "      FORM-HSYC-TUVAN-01D                      (1 chunk files)\n",
      "      TT-00-Quy·∫øt-ƒë·ªãnh-Th√¥ng-t∆∞                (1 chunk files)\n",
      "      FORM-7-M·∫´u-s·ªë-7B-E-HSMT                  (1 chunk files)\n",
      "      TEMPLATE-BC-03C                          (1 chunk files)\n",
      "      FORM-15-Phu-luc                          (1 chunk files)\n",
      "      QD-1667-BYT                              (1 chunk files)\n",
      "\n",
      "============================================================\n",
      "üîç Missing Documents Analysis:\n",
      "\n",
      "   ‚ö†Ô∏è  31 document_ids in mapping but NOT in vector DB:\n",
      "      FORM-01-Ph·ª•-l·ª•c\n",
      "         Chunk files: 1\n",
      "         Example: 01._Ph·ª•_l·ª•c.jsonl\n",
      "      FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "         Chunk files: 1\n",
      "         Example: 04.1A._M·∫´u_K·∫ø_ho·∫°ch_ki·ªÉm_tra_ƒë·ªãnh_k·ª≥.jsonl\n",
      "      FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "         Chunk files: 1\n",
      "         Example: 04.1B._M·∫´u_K·∫ø_ho·∫°ch_ki·ªÉm_tra_chi_ti·∫øt.jsonl\n",
      "      FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b\n",
      "         Chunk files: 1\n",
      "         Example: 04.2._M·∫´u_ƒê·ªÅ_c∆∞∆°ng_b√°o_c√°o_ƒë·∫•u_th·∫ßu_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "      FORM-043-M·∫´u-B√°o-c√°o-ki\n",
      "         Chunk files: 1\n",
      "         Example: 04.3._M·∫´u_B√°o_c√°o_ki·ªÉm_tra_ƒë·ªëi_v·ªõi_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "      FORM-044-M·∫´u-K·∫øt-lu·∫≠n-k\n",
      "         Chunk files: 1\n",
      "         Example: 04.4._M·∫´u_K·∫øt_lu·∫≠n_ki·ªÉm_tra_ƒë·ªëi_v·ªõi_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "      FORM-045-M·∫´u-B√°o-c√°o-ph\n",
      "         Chunk files: 1\n",
      "         Example: 04.5._M·∫´u_B√°o_c√°o_ph·∫£n_h·ªìi_v·ªÅ_K·∫øt_lu·∫≠n_ki·ªÉm_tra.jsonl\n",
      "      FORM-1-M·∫´u-01A-02C_K·∫ø-ho\n",
      "         Chunk files: 1\n",
      "         Example: 1._M·∫´u_01A-02C_K·∫ø_ho·∫°ch_t·ªïng_th·ªÉ_LCNT.jsonl\n",
      "      FORM-10-M·∫´u-s·ªë-10C_E-HSM\n",
      "         Chunk files: 1\n",
      "         Example: 10._M·∫´u_s·ªë_10C_E-HSMST_EPC_s∆°_tuy·ªÉn.jsonl\n",
      "      FORM-12-M·∫´u-s·ªë-12C-CGTT-\n",
      "         Chunk files: 1\n",
      "         Example: 12._M·∫´u_s·ªë_12C_CGTT_HH_r√∫t_g·ªçn.jsonl\n",
      "\n",
      "   ‚ÑπÔ∏è  1 document_ids in vector DB but NOT in mapping:\n",
      "      FORM-Bidding/2025#bee720\n",
      "\n",
      "============================================================\n",
      "üìù Chunk File Processing Status:\n",
      "\n",
      "   ‚ö†Ô∏è  Found 3 chunks with OLD format chunk_ids:\n",
      "      bidding_untitled_form_0122                         ‚Üí FORM-Bidding/2025#bee720\n",
      "      bidding_untitled_form_0123                         ‚Üí FORM-Bidding/2025#bee720\n",
      "      bidding_untitled_form_0124                         ‚Üí FORM-Bidding/2025#bee720\n",
      "\n",
      "============================================================\n",
      "‚úÖ Analysis complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import asyncpg\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"üîç DETAILED MIGRATION ANALYSIS\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Database connection params\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"rag_bidding_v2\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"sakana\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"sakana123\")\n",
    "\n",
    "async def analyze_migration():\n",
    "    conn = await asyncpg.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        database=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 1. Get all distinct document_ids from vector DB\n",
    "        result = await conn.fetch(\"\"\"\n",
    "            SELECT \n",
    "                cmetadata->>'document_id' as doc_id,\n",
    "                COUNT(*) as chunk_count\n",
    "            FROM langchain_pg_embedding\n",
    "            GROUP BY cmetadata->>'document_id'\n",
    "            ORDER BY doc_id\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"üìä Vector Database Analysis:\")\n",
    "        print(f\"   Total distinct document_ids: {len(result)}\\n\")\n",
    "        \n",
    "        # Group by prefix to see patterns\n",
    "        by_prefix = defaultdict(list)\n",
    "        for row in result:\n",
    "            doc_id = row['doc_id']\n",
    "            if doc_id:\n",
    "                prefix = doc_id.split('-')[0]\n",
    "                by_prefix[prefix].append((doc_id, row['chunk_count']))\n",
    "        \n",
    "        print(f\"üìã Document IDs by prefix:\")\n",
    "        for prefix, docs in sorted(by_prefix.items()):\n",
    "            print(f\"\\n   {prefix}: {len(docs)} documents\")\n",
    "            for doc_id, count in docs[:5]:  # Show first 5\n",
    "                print(f\"      {doc_id:40} ({count:4} chunks)\")\n",
    "            if len(docs) > 5:\n",
    "                print(f\"      ... and {len(docs) - 5} more\")\n",
    "        \n",
    "        # 2. Check mapping file\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìÇ Mapping File Analysis:\\n\")\n",
    "        \n",
    "        import json\n",
    "        from pathlib import Path\n",
    "        \n",
    "        mapping_file = Path(\"data/processed/chunk_to_document_mapping.json\")\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            chunk_mapping = json.load(f)\n",
    "        \n",
    "        print(f\"   Total chunk files in mapping: {len(chunk_mapping)}\")\n",
    "        print(f\"   Unique document_ids in mapping: {len(set(chunk_mapping.values()))}\")\n",
    "        \n",
    "        # Count by document_id\n",
    "        doc_counts = defaultdict(int)\n",
    "        for doc_id in chunk_mapping.values():\n",
    "            doc_counts[doc_id] += 1\n",
    "        \n",
    "        print(f\"\\n   Top 10 document_ids by chunk file count:\")\n",
    "        for doc_id, count in sorted(doc_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"      {doc_id:40} ({count} chunk files)\")\n",
    "        \n",
    "        # 3. Find missing documents\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîç Missing Documents Analysis:\\n\")\n",
    "        \n",
    "        mapping_doc_ids = set(chunk_mapping.values())\n",
    "        vector_doc_ids = {row['doc_id'] for row in result if row['doc_id']}\n",
    "        \n",
    "        missing_in_vector = mapping_doc_ids - vector_doc_ids\n",
    "        extra_in_vector = vector_doc_ids - mapping_doc_ids\n",
    "        \n",
    "        if missing_in_vector:\n",
    "            print(f\"   ‚ö†Ô∏è  {len(missing_in_vector)} document_ids in mapping but NOT in vector DB:\")\n",
    "            for doc_id in sorted(missing_in_vector)[:10]:\n",
    "                # Find which chunk files map to this\n",
    "                chunk_files = [cf for cf, did in chunk_mapping.items() if did == doc_id]\n",
    "                print(f\"      {doc_id}\")\n",
    "                print(f\"         Chunk files: {len(chunk_files)}\")\n",
    "                if chunk_files:\n",
    "                    print(f\"         Example: {Path(chunk_files[0]).name}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All mapping document_ids found in vector DB\")\n",
    "        \n",
    "        if extra_in_vector:\n",
    "            print(f\"\\n   ‚ÑπÔ∏è  {len(extra_in_vector)} document_ids in vector DB but NOT in mapping:\")\n",
    "            for doc_id in sorted(extra_in_vector)[:5]:\n",
    "                print(f\"      {doc_id}\")\n",
    "        \n",
    "        # 4. Check which chunk files were actually processed\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìù Chunk File Processing Status:\\n\")\n",
    "        \n",
    "        # Get sample of old chunk_ids still in DB\n",
    "        old_chunks = await conn.fetch(\"\"\"\n",
    "            SELECT DISTINCT \n",
    "                cmetadata->>'chunk_id' as chunk_id,\n",
    "                cmetadata->>'document_id' as doc_id\n",
    "            FROM langchain_pg_embedding\n",
    "            WHERE cmetadata->>'chunk_id' LIKE '%untitled%'\n",
    "               OR cmetadata->>'chunk_id' LIKE '%bidding_%'\n",
    "               OR cmetadata->>'chunk_id' LIKE '%law_%'\n",
    "            LIMIT 20\n",
    "        \"\"\")\n",
    "        \n",
    "        if old_chunks:\n",
    "            print(f\"   ‚ö†Ô∏è  Found {len(old_chunks)} chunks with OLD format chunk_ids:\")\n",
    "            for row in old_chunks[:10]:\n",
    "                print(f\"      {row['chunk_id']:50} ‚Üí {row['doc_id']}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ No old format chunk_ids found\")\n",
    "        \n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "await analyze_migration()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ Analysis complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de38ef",
   "metadata": {},
   "source": [
    "## üéØ Ph√¢n T√≠ch Nguy√™n Nh√¢n & Gi·∫£i Ph√°p\n",
    "\n",
    "### ‚ùå **V·∫•n ƒê·ªÅ Ph√°t Hi·ªán**: 31 Documents Ch∆∞a ƒê∆∞·ª£c Import V√†o Database\n",
    "\n",
    "**K·∫øt Qu·∫£ Ph√¢n T√≠ch:**\n",
    "1. ‚úÖ **Chunk files**: C√≥ 63 files trong `data/processed/chunks/`\n",
    "2. ‚úÖ **Mapping file**: C√≥ 55 chunk files ƒë∆∞·ª£c map sang document_ids\n",
    "3. ‚úÖ **Format trong database**: Chunks ƒê√É c√≥ format m·ªõi (`FORM-05-..._section_0000`)\n",
    "4. ‚ùå **Thi·∫øu chunks**: 31 document_ids c√≥ trong mapping nh∆∞ng KH√îNG c√≥ trong vector DB\n",
    "\n",
    "**Nguy√™n Nh√¢n:**\n",
    "- 31 documents b·ªã thi·∫øu **ch∆∞a bao gi·ªù ƒë∆∞·ª£c import** v√†o vector database\n",
    "- ƒê√¢y KH√îNG ph·∫£i l·ªói migration (chunks ƒë√£ c√≥ format ƒë√∫ng r·ªìi)\n",
    "- ƒê√¢y l√† **l·ªó h·ªïng data ingestion** t·ª´ giai ƒëo·∫°n x·ª≠ l√Ω ban ƒë·∫ßu\n",
    "\n",
    "**B·∫±ng Ch·ª©ng:**\n",
    "```sql\n",
    "-- Database ƒë√£ c√≥ format m·ªõi:\n",
    "LUA-57-2024-QH15_dieu_0143\n",
    "FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-_section_0000\n",
    "FORM-HSYC-TUVAN-01D_form_0004\n",
    "\n",
    "-- Nh∆∞ng documents b·ªã thi·∫øu nh∆∞ FORM-01-Ph·ª•-l·ª•c c√≥ 0 chunks trong DB\n",
    "```\n",
    "\n",
    "### ‚úÖ **C√°c Ph∆∞∆°ng √Ån Gi·∫£i Quy·∫øt:**\n",
    "\n",
    "**Ph∆∞∆°ng √Ån 1: Ho√†n Th√†nh Migration (Khuy√™n D√πng)** üåü\n",
    "- **L√† g√¨**: Import l·∫°i 31 chunk files b·ªã thi·∫øu v√†o vector database\n",
    "- **C√°ch th·ª±c hi·ªán**: D√πng `scripts/import_processed_chunks.py` ƒë·ªÉ import c√°c files c√≤n thi·∫øu\n",
    "- **K·∫øt qu·∫£**: TƒÉng t·ª´ 25 ‚Üí 55 documents (tr·∫°ng th√°i mong mu·ªën)\n",
    "- **Th·ªùi gian**: 10-15 ph√∫t\n",
    "\n",
    "**Ph∆∞∆°ng √Ån 2: Ch·ªâ C·∫≠p Nh·∫≠t Mapping**\n",
    "- **L√† g√¨**: X√≥a 31 documents b·ªã thi·∫øu kh·ªèi mapping file\n",
    "- **C√°ch th·ª±c hi·ªán**: C·∫≠p nh·∫≠t `chunk_to_document_mapping.json` ƒë·ªÉ ch·ªâ ch·ª©a 25 documents\n",
    "- **K·∫øt qu·∫£**: Gi·ªØ nguy√™n tr·∫°ng th√°i hi·ªán t·∫°i (25 documents), c·∫≠p nh·∫≠t mapping cho kh·ªõp\n",
    "- **Th·ªùi gian**: 2 ph√∫t\n",
    "\n",
    "**Khuy·∫øn Ngh·ªã**: **Ph∆∞∆°ng √Ån 1** - Ho√†n th√†nh vi·ªác import data ƒë·ªÉ c√≥ ƒë·ªß 55 documents.\n",
    "\n",
    "---\n",
    "\n",
    "### üìã C√°c B∆∞·ªõc Ti·∫øp Theo:\n",
    "\n",
    "1. **X√°c ƒë·ªãnh c√°c chunk files b·ªã thi·∫øu** (31 files)\n",
    "2. **Ch·∫°y import script** cho c√°c files thi·∫øu\n",
    "3. **Verify**: Ph·∫£i c√≥ 55 documents trong database\n",
    "4. **Ti·∫øp t·ª•c** v·ªõi Steps 6-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c03499cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç X√°c ƒê·ªãnh C√°c Chunk Files B·ªã Thi·∫øu ƒê·ªÉ Import\n",
      "\n",
      "============================================================\n",
      "üìä T·ªïng K·∫øt:\n",
      "   Documents trong mapping: 55\n",
      "   Documents trong vector DB: 25\n",
      "   Documents b·ªã thi·∫øu: 31\n",
      "\n",
      "üìÇ C√°c Chunk Files B·ªã Thi·∫øu (31):\n",
      "\n",
      "   [ 1] ‚úÖ FORM-01-Ph·ª•-l·ª•c\n",
      "        File: 01._Ph·ª•_l·ª•c.jsonl\n",
      "        S·ªë chunks: 48\n",
      "\n",
      "   [ 2] ‚úÖ FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "        File: 04.1A._M·∫´u_K·∫ø_ho·∫°ch_ki·ªÉm_tra_ƒë·ªãnh_k·ª≥.jsonl\n",
      "        S·ªë chunks: 1\n",
      "\n",
      "   [ 3] ‚úÖ FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "        File: 04.1B._M·∫´u_K·∫ø_ho·∫°ch_ki·ªÉm_tra_chi_ti·∫øt.jsonl\n",
      "        S·ªë chunks: 10\n",
      "\n",
      "   [ 4] ‚úÖ FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b\n",
      "        File: 04.2._M·∫´u_ƒê·ªÅ_c∆∞∆°ng_b√°o_c√°o_ƒë·∫•u_th·∫ßu_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "        S·ªë chunks: 10\n",
      "\n",
      "   [ 5] ‚úÖ FORM-043-M·∫´u-B√°o-c√°o-ki\n",
      "        File: 04.3._M·∫´u_B√°o_c√°o_ki·ªÉm_tra_ƒë·ªëi_v·ªõi_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "        S·ªë chunks: 8\n",
      "\n",
      "   [ 6] ‚úÖ FORM-044-M·∫´u-K·∫øt-lu·∫≠n-k\n",
      "        File: 04.4._M·∫´u_K·∫øt_lu·∫≠n_ki·ªÉm_tra_ƒë·ªëi_v·ªõi_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "        S·ªë chunks: 4\n",
      "\n",
      "   [ 7] ‚úÖ FORM-045-M·∫´u-B√°o-c√°o-ph\n",
      "        File: 04.5._M·∫´u_B√°o_c√°o_ph·∫£n_h·ªìi_v·ªÅ_K·∫øt_lu·∫≠n_ki·ªÉm_tra.jsonl\n",
      "        S·ªë chunks: 4\n",
      "\n",
      "   [ 8] ‚úÖ FORM-1-M·∫´u-01A-02C_K·∫ø-ho\n",
      "        File: 1._M·∫´u_01A-02C_K·∫ø_ho·∫°ch_t·ªïng_th·ªÉ_LCNT.jsonl\n",
      "        S·ªë chunks: 31\n",
      "\n",
      "   [ 9] ‚úÖ FORM-10-M·∫´u-s·ªë-10C_E-HSM\n",
      "        File: 10._M·∫´u_s·ªë_10C_E-HSMST_EPC_s∆°_tuy·ªÉn.jsonl\n",
      "        S·ªë chunks: 19\n",
      "\n",
      "   [10] ‚úÖ FORM-12-M·∫´u-s·ªë-12C-CGTT-\n",
      "        File: 12._M·∫´u_s·ªë_12C_CGTT_HH_r√∫t_g·ªçn.jsonl\n",
      "        S·ªë chunks: 11\n",
      "\n",
      "   [11] ‚úÖ FORM-12-M·∫´u-s·ªë-12D-CGTT-\n",
      "        File: 12._M·∫´u_s·ªë_12D_CGTT_PTV_r√∫t_g·ªçn.jsonl\n",
      "        S·ªë chunks: 12\n",
      "\n",
      "   [12] ‚úÖ FORM-12-M·∫´u-s·ªë-12G-CGTT-\n",
      "        File: 12._M·∫´u_s·ªë_12G_CGTT_r√∫t_g·ªçn_x·ª≠_l√Ω_t√¨nh_hu·ªëng_final.jsonl\n",
      "        S·ªë chunks: 4\n",
      "\n",
      "   [13] ‚úÖ FORM-13-M·∫´u-s·ªë-13_-Mua-s\n",
      "        File: 13._M·∫´u_s·ªë_13__Mua_s·∫Øm_tr·ª±c_tuy·∫øn.jsonl\n",
      "        S·ªë chunks: 21\n",
      "\n",
      "   [14] ‚úÖ FORM-7-M·∫´u-s·ªë-7C-E-HSMS\n",
      "        File: 7._M·∫´u_s·ªë_7C._E-HSMST_EP_s∆°_tuy·ªÉn.jsonl\n",
      "        S·ªë chunks: 15\n",
      "\n",
      "   [15] ‚úÖ FORM-8-M·∫´u-s·ªë-8A-E-HSMT\n",
      "        File: 8._M·∫´u_s·ªë_8A._E-HSMT__EC_01_t√∫i.jsonl\n",
      "        S·ªë chunks: 101\n",
      "\n",
      "   [16] ‚úÖ FORM-8-M·∫´u-s·ªë-8C-E-HSMS\n",
      "        File: 8._M·∫´u_s·ªë_8C._E-HSMST_EC_s∆°_tuy·ªÉn.jsonl\n",
      "        S·ªë chunks: 17\n",
      "\n",
      "   [17] ‚úÖ FORM-9-M·∫´u-s·ªë-9C_E-HSMST\n",
      "        File: 9._M·∫´u_s·ªë_9C_E-HSMST_PC_s∆°_tuy·ªÉn.jsonl\n",
      "        S·ªë chunks: 18\n",
      "\n",
      "   [18] ‚úÖ FORM-HSYC-HANGHOA-01B\n",
      "        File: 01B._M·∫´u_HSYC_h√†ng_h√≥a.jsonl\n",
      "        S·ªë chunks: 55\n",
      "\n",
      "   [19] ‚úÖ FORM-HSYC-TUVAN-01C\n",
      "        File: 01C._M·∫´u_HSYC_Phi_t∆∞_v·∫•n.jsonl\n",
      "        S·ªë chunks: 35\n",
      "\n",
      "   [20] ‚úÖ FORM-HSYC-TUVAN-5\n",
      "        File: 5._M·∫´u_s·ªë_5A_E-HSMT_Phi_t∆∞_v·∫•n_1_t√∫i.jsonl\n",
      "        S·ªë chunks: 68\n",
      "\n",
      "   [21] ‚úÖ FORM-HSYC-TUVAN-6\n",
      "        File: 6._M·∫´u_s·ªë_6B_E-HSMQT_t∆∞_v·∫•n.jsonl\n",
      "        S·ªë chunks: 9\n",
      "\n",
      "   [22] ‚úÖ FORM-HSYC-XAYLAP-12\n",
      "        File: 12._M·∫´u_s·ªë_12E_CGTT_x√¢y_l·∫Øp_r√∫t_g·ªçn_final.jsonl\n",
      "        S·ªë chunks: 54\n",
      "\n",
      "   [23] ‚úÖ TEMPLATE-BC-02A\n",
      "        File: 02A._M·∫´u_BCƒêG_cho_1_giai_ƒëo·∫°n_1_t√∫i.jsonl\n",
      "        S·ªë chunks: 27\n",
      "\n",
      "   [24] ‚úÖ TEMPLATE-BC-02B\n",
      "        File: 02B._M·∫´u_BCƒêG_cho_1_giai_ƒëo·∫°n_2_t√∫i_h·ªì_s∆°.jsonl\n",
      "        S·ªë chunks: 37\n",
      "\n",
      "   [25] ‚úÖ TEMPLATE-BC-02C\n",
      "        File: 02C._M·∫´u_BCƒêG_cho_g√≥i_th·∫ßu_t∆∞_v·∫•n.jsonl\n",
      "        S·ªë chunks: 38\n",
      "\n",
      "   [26] ‚úÖ TEMPLATE-BC-03A\n",
      "        File: 03A._M·∫´u_BCTƒê_HSMT.jsonl\n",
      "        S·ªë chunks: 8\n",
      "\n",
      "   [27] ‚úÖ TEMPLATE-BC-03B\n",
      "        File: 03B._M·∫´u_BCTƒê_danh_s√°ch_ƒë√°p_·ª©ng_v·ªÅ_k·ªπ_thu·∫≠t.jsonl\n",
      "        S·ªë chunks: 11\n",
      "\n",
      "   [28] ‚úÖ TEMPLATE-BC-03C\n",
      "        File: 03C._M·∫´u_BCTƒê_KQLCNT.jsonl\n",
      "        S·ªë chunks: 11\n",
      "\n",
      "   [29] ‚úÖ TEMPLATE-BC-14B\n",
      "        File: 14B._M·∫´u_BCƒêG_PTV_HH_TBYT_quy_tr√¨nh_2_1_t√∫i_.jsonl\n",
      "        S·ªë chunks: 17\n",
      "\n",
      "   [30] ‚úÖ TEMPLATE-BC-14C\n",
      "        File: 14C._M·∫´u_BCƒêG_HH_XL_PTV_hon_hop_TBYT_2_tui_.jsonl\n",
      "        S·ªë chunks: 43\n",
      "\n",
      "   [31] ‚úÖ TEMPLATE-BC-14D\n",
      "        File: 14D._Mau_BCƒêG_T∆∞_v·∫•n_.jsonl\n",
      "        S·ªë chunks: 20\n",
      "\n",
      "============================================================\n",
      "‚úÖ Files t·ªìn t·∫°i: 31/31\n",
      "üìä T·ªïng chunks c·∫ßn import: 767\n",
      "============================================================\n",
      "\n",
      "üíæ ƒê√£ l∆∞u danh s√°ch files b·ªã thi·∫øu v√†o: data/processed/missing_chunk_files.json\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import asyncpg\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç X√°c ƒê·ªãnh C√°c Chunk Files B·ªã Thi·∫øu ƒê·ªÉ Import\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# K·∫øt n·ªëi database\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"rag_bidding_v2\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"sakana\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"sakana123\")\n",
    "\n",
    "async def identify_missing_files():\n",
    "    conn = await asyncpg.connect(\n",
    "        host=DB_HOST, port=DB_PORT, database=DB_NAME,\n",
    "        user=DB_USER, password=DB_PASSWORD\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # L·∫•y t·∫•t c·∫£ document_ids hi·ªán c√≥ trong vector DB\n",
    "        result = await conn.fetch(\"\"\"\n",
    "            SELECT DISTINCT cmetadata->>'document_id' as doc_id\n",
    "            FROM langchain_pg_embedding\n",
    "        \"\"\")\n",
    "        vector_doc_ids = {row['doc_id'] for row in result if row['doc_id']}\n",
    "        \n",
    "        # Load mapping file\n",
    "        mapping_file = Path(\"data/processed/chunk_to_document_mapping.json\")\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            chunk_mapping = json.load(f)\n",
    "        \n",
    "        mapping_doc_ids = set(chunk_mapping.values())\n",
    "        \n",
    "        # T√¨m documents b·ªã thi·∫øu\n",
    "        missing_doc_ids = mapping_doc_ids - vector_doc_ids\n",
    "        \n",
    "        print(f\"üìä T·ªïng K·∫øt:\")\n",
    "        print(f\"   Documents trong mapping: {len(mapping_doc_ids)}\")\n",
    "        print(f\"   Documents trong vector DB: {len(vector_doc_ids)}\")\n",
    "        print(f\"   Documents b·ªã thi·∫øu: {len(missing_doc_ids)}\\n\")\n",
    "        \n",
    "        # T√¨m chunk files cho documents b·ªã thi·∫øu\n",
    "        missing_files = []\n",
    "        for chunk_file, doc_id in chunk_mapping.items():\n",
    "            if doc_id in missing_doc_ids:\n",
    "                chunk_path = Path(chunk_file)\n",
    "                if chunk_path.exists():\n",
    "                    # ƒê·∫øm chunks trong file\n",
    "                    chunk_count = sum(1 for line in open(chunk_path, 'r'))\n",
    "                    missing_files.append({\n",
    "                        'document_id': doc_id,\n",
    "                        'chunk_file': str(chunk_path),\n",
    "                        'file_name': chunk_path.name,\n",
    "                        'chunk_count': chunk_count,\n",
    "                        'exists': True\n",
    "                    })\n",
    "                else:\n",
    "                    missing_files.append({\n",
    "                        'document_id': doc_id,\n",
    "                        'chunk_file': str(chunk_path),\n",
    "                        'file_name': chunk_path.name,\n",
    "                        'chunk_count': 0,\n",
    "                        'exists': False\n",
    "                    })\n",
    "        \n",
    "        # S·∫Øp x·∫øp theo document_id\n",
    "        missing_files.sort(key=lambda x: x['document_id'])\n",
    "        \n",
    "        print(f\"üìÇ C√°c Chunk Files B·ªã Thi·∫øu ({len(missing_files)}):\\n\")\n",
    "        \n",
    "        total_chunks = 0\n",
    "        existing_files = 0\n",
    "        \n",
    "        for i, file_info in enumerate(missing_files, 1):\n",
    "            status = \"‚úÖ\" if file_info['exists'] else \"‚ùå\"\n",
    "            print(f\"   [{i:2d}] {status} {file_info['document_id']}\")\n",
    "            print(f\"        File: {file_info['file_name']}\")\n",
    "            print(f\"        S·ªë chunks: {file_info['chunk_count']}\")\n",
    "            print()\n",
    "            \n",
    "            if file_info['exists']:\n",
    "                total_chunks += file_info['chunk_count']\n",
    "                existing_files += 1\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"‚úÖ Files t·ªìn t·∫°i: {existing_files}/{len(missing_files)}\")\n",
    "        print(f\"üìä T·ªïng chunks c·∫ßn import: {total_chunks:,}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # L∆∞u danh s√°ch ƒë·ªÉ import\n",
    "        output_file = Path(\"data/processed/missing_chunk_files.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(missing_files, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"üíæ ƒê√£ l∆∞u danh s√°ch files b·ªã thi·∫øu v√†o: {output_file}\")\n",
    "        \n",
    "        return missing_files\n",
    "    \n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "missing_files = await identify_missing_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebd4dc",
   "metadata": {},
   "source": [
    "### üöÄ Import C√°c Chunk Files B·ªã Thi·∫øu\n",
    "\n",
    "**Chi·∫øn L∆∞·ª£c:**\n",
    "1. ƒê·ªçc c√°c chunk files b·ªã thi·∫øu (ƒë√£ c√≥ format ƒë√∫ng r·ªìi)\n",
    "2. Import tr·ª±c ti·∫øp v√†o vector database c√πng v·ªõi embeddings\n",
    "3. Verify: Ph·∫£i tƒÉng t·ª´ 25 ‚Üí 55 documents\n",
    "\n",
    "**‚ö†Ô∏è Quan Tr·ªçng:**\n",
    "- Chunks trong files ƒê√É c√≥ format ƒë√∫ng (`document_id` v√† `chunk_id`)\n",
    "- Ch√∫ng ta ch·ªâ c·∫ßn import ch√∫ng v√†o vector database\n",
    "- S·ª≠ d·ª•ng script import hi·ªán c√≥: `scripts/import_processed_chunks.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe16605",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Cleanup: X√≥a Chunks Import Sai\n",
    "\n",
    "**V·∫•n ƒë·ªÅ ph√°t hi·ªán:**\n",
    "- Cell 28 l·∫ßn tr∆∞·ªõc import 767 chunks v·ªõi `document_id: bidding_untitled` (old format)\n",
    "- C·∫ßn x√≥a ch√∫ng tr∆∞·ªõc khi re-import v·ªõi document_ids ƒë√∫ng\n",
    "\n",
    "**Gi·∫£i ph√°p:**\n",
    "- X√≥a c√°c chunks c√≥ `document_id: bidding_untitled` (ho·∫∑c gi·ªëng old format)\n",
    "- Re-import v·ªõi document_ids m·ªõi t·ª´ mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import asyncpg\n",
    "import os\n",
    "\n",
    "print(\"üóëÔ∏è Cleanup: X√≥a Chunks Import Sai\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# K·∫øt n·ªëi database\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"rag_bidding_v2\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"sakana\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"sakana123\")\n",
    "\n",
    "async def cleanup_wrong_imports():\n",
    "    conn = await asyncpg.connect(\n",
    "        host=DB_HOST, port=DB_PORT, database=DB_NAME,\n",
    "        user=DB_USER, password=DB_PASSWORD\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Ki·ªÉm tra chunks v·ªõi bidding_untitled (import sai l·∫ßn tr∆∞·ªõc)\n",
    "        result = await conn.fetchrow(\"\"\"\n",
    "            SELECT COUNT(*) as count\n",
    "            FROM langchain_pg_embedding\n",
    "            WHERE cmetadata->>'document_id' = 'bidding_untitled'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"üìä Chunks v·ªõi document_id='bidding_untitled': {result['count']}\")\n",
    "        \n",
    "        if result['count'] > 0:\n",
    "            print(f\"   üóëÔ∏è ƒêang x√≥a {result['count']} chunks...\\n\")\n",
    "            \n",
    "            # X√≥a chunks\n",
    "            deleted = await conn.execute(\"\"\"\n",
    "                DELETE FROM langchain_pg_embedding\n",
    "                WHERE cmetadata->>'document_id' = 'bidding_untitled'\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"   ‚úÖ ƒê√£ x√≥a: {deleted}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Kh√¥ng c√≥ chunks c·∫ßn x√≥a\")\n",
    "        \n",
    "        # Ki·ªÉm tra l·∫°i sau khi x√≥a\n",
    "        result = await conn.fetchrow(\"\"\"\n",
    "            SELECT COUNT(DISTINCT cmetadata->>'document_id') as doc_count,\n",
    "                   COUNT(*) as total_chunks\n",
    "            FROM langchain_pg_embedding\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"\\nüìä Sau khi cleanup:\")\n",
    "        print(f\"   S·ªë document_ids: {result['doc_count']}\")\n",
    "        print(f\"   T·ªïng chunks: {result['total_chunks']:,}\")\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        \n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "await cleanup_wrong_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "132ed056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ƒêang Import C√°c Chunk Files B·ªã Thi·∫øu (V·ªõi Document ID M·ªõi)\n",
      "\n",
      "============================================================\n",
      "üìÇ Files c·∫ßn import: 31\n",
      "üìä T·ªïng chunks: 767\n",
      "\n",
      "üìã Loaded metadata for 63 files\n",
      "\n",
      "‚úÖ ƒê√£ k·∫øt n·ªëi vector store: docs\n",
      "\n",
      "[1/31] ƒêang import: FORM-01-Ph·ª•-l·ª•c\n",
      "   File: 01._Ph·ª•_l·ª•c.jsonl\n",
      "   Chunks: 48\n",
      "   ‚úÖ ƒê√£ import 48 chunks v·ªõi document_id: FORM-01-Ph·ª•-l·ª•c\n",
      "\n",
      "[2/31] ƒêang import: FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "   File: 04.1A._M·∫´u_K·∫ø_ho·∫°ch_ki·ªÉm_tra_ƒë·ªãnh_k·ª≥.jsonl\n",
      "   Chunks: 1\n",
      "   ‚úÖ ƒê√£ import 48 chunks v·ªõi document_id: FORM-01-Ph·ª•-l·ª•c\n",
      "\n",
      "[2/31] ƒêang import: FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "   File: 04.1A._M·∫´u_K·∫ø_ho·∫°ch_ki·ªÉm_tra_ƒë·ªãnh_k·ª≥.jsonl\n",
      "   Chunks: 1\n",
      "   ‚úÖ ƒê√£ import 1 chunks v·ªõi document_id: FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "\n",
      "[3/31] ƒêang import: FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "   File: 04.1B._M·∫´u_K·∫ø_ho·∫°ch_ki·ªÉm_tra_chi_ti·∫øt.jsonl\n",
      "   Chunks: 10\n",
      "   ‚úÖ ƒê√£ import 1 chunks v·ªõi document_id: FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "\n",
      "[3/31] ƒêang import: FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "   File: 04.1B._M·∫´u_K·∫ø_ho·∫°ch_ki·ªÉm_tra_chi_ti·∫øt.jsonl\n",
      "   Chunks: 10\n",
      "   ‚úÖ ƒê√£ import 10 chunks v·ªõi document_id: FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "\n",
      "[4/31] ƒêang import: FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b\n",
      "   File: 04.2._M·∫´u_ƒê·ªÅ_c∆∞∆°ng_b√°o_c√°o_ƒë·∫•u_th·∫ßu_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "   Chunks: 10\n",
      "   ‚úÖ ƒê√£ import 10 chunks v·ªõi document_id: FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-\n",
      "\n",
      "[4/31] ƒêang import: FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b\n",
      "   File: 04.2._M·∫´u_ƒê·ªÅ_c∆∞∆°ng_b√°o_c√°o_ƒë·∫•u_th·∫ßu_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "   Chunks: 10\n",
      "   ‚úÖ ƒê√£ import 10 chunks v·ªõi document_id: FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b\n",
      "\n",
      "[5/31] ƒêang import: FORM-043-M·∫´u-B√°o-c√°o-ki\n",
      "   File: 04.3._M·∫´u_B√°o_c√°o_ki·ªÉm_tra_ƒë·ªëi_v·ªõi_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "   Chunks: 8\n",
      "   ‚úÖ ƒê√£ import 10 chunks v·ªõi document_id: FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b\n",
      "\n",
      "[5/31] ƒêang import: FORM-043-M·∫´u-B√°o-c√°o-ki\n",
      "   File: 04.3._M·∫´u_B√°o_c√°o_ki·ªÉm_tra_ƒë·ªëi_v·ªõi_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "   Chunks: 8\n",
      "   ‚úÖ ƒê√£ import 8 chunks v·ªõi document_id: FORM-043-M·∫´u-B√°o-c√°o-ki\n",
      "\n",
      "[6/31] ƒêang import: FORM-044-M·∫´u-K·∫øt-lu·∫≠n-k\n",
      "   File: 04.4._M·∫´u_K·∫øt_lu·∫≠n_ki·ªÉm_tra_ƒë·ªëi_v·ªõi_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "   Chunks: 4\n",
      "   ‚úÖ ƒê√£ import 8 chunks v·ªõi document_id: FORM-043-M·∫´u-B√°o-c√°o-ki\n",
      "\n",
      "[6/31] ƒêang import: FORM-044-M·∫´u-K·∫øt-lu·∫≠n-k\n",
      "   File: 04.4._M·∫´u_K·∫øt_lu·∫≠n_ki·ªÉm_tra_ƒë·ªëi_v·ªõi_l·ª±a_ch·ªçn_nh√†_th·∫ßu,_NƒêT.jsonl\n",
      "   Chunks: 4\n",
      "   ‚úÖ ƒê√£ import 4 chunks v·ªõi document_id: FORM-044-M·∫´u-K·∫øt-lu·∫≠n-k\n",
      "\n",
      "[7/31] ƒêang import: FORM-045-M·∫´u-B√°o-c√°o-ph\n",
      "   File: 04.5._M·∫´u_B√°o_c√°o_ph·∫£n_h·ªìi_v·ªÅ_K·∫øt_lu·∫≠n_ki·ªÉm_tra.jsonl\n",
      "   Chunks: 4\n",
      "   ‚úÖ ƒê√£ import 4 chunks v·ªõi document_id: FORM-044-M·∫´u-K·∫øt-lu·∫≠n-k\n",
      "\n",
      "[7/31] ƒêang import: FORM-045-M·∫´u-B√°o-c√°o-ph\n",
      "   File: 04.5._M·∫´u_B√°o_c√°o_ph·∫£n_h·ªìi_v·ªÅ_K·∫øt_lu·∫≠n_ki·ªÉm_tra.jsonl\n",
      "   Chunks: 4\n",
      "   ‚úÖ ƒê√£ import 4 chunks v·ªõi document_id: FORM-045-M·∫´u-B√°o-c√°o-ph\n",
      "\n",
      "[8/31] ƒêang import: FORM-1-M·∫´u-01A-02C_K·∫ø-ho\n",
      "   File: 1._M·∫´u_01A-02C_K·∫ø_ho·∫°ch_t·ªïng_th·ªÉ_LCNT.jsonl\n",
      "   Chunks: 31\n",
      "   ‚úÖ ƒê√£ import 4 chunks v·ªõi document_id: FORM-045-M·∫´u-B√°o-c√°o-ph\n",
      "\n",
      "[8/31] ƒêang import: FORM-1-M·∫´u-01A-02C_K·∫ø-ho\n",
      "   File: 1._M·∫´u_01A-02C_K·∫ø_ho·∫°ch_t·ªïng_th·ªÉ_LCNT.jsonl\n",
      "   Chunks: 31\n",
      "   ‚úÖ ƒê√£ import 31 chunks v·ªõi document_id: FORM-1-M·∫´u-01A-02C_K·∫ø-ho\n",
      "\n",
      "[9/31] ƒêang import: FORM-10-M·∫´u-s·ªë-10C_E-HSM\n",
      "   File: 10._M·∫´u_s·ªë_10C_E-HSMST_EPC_s∆°_tuy·ªÉn.jsonl\n",
      "   Chunks: 19\n",
      "   ‚úÖ ƒê√£ import 31 chunks v·ªõi document_id: FORM-1-M·∫´u-01A-02C_K·∫ø-ho\n",
      "\n",
      "[9/31] ƒêang import: FORM-10-M·∫´u-s·ªë-10C_E-HSM\n",
      "   File: 10._M·∫´u_s·ªë_10C_E-HSMST_EPC_s∆°_tuy·ªÉn.jsonl\n",
      "   Chunks: 19\n",
      "   ‚úÖ ƒê√£ import 19 chunks v·ªõi document_id: FORM-10-M·∫´u-s·ªë-10C_E-HSM\n",
      "\n",
      "[10/31] ƒêang import: FORM-12-M·∫´u-s·ªë-12C-CGTT-\n",
      "   File: 12._M·∫´u_s·ªë_12C_CGTT_HH_r√∫t_g·ªçn.jsonl\n",
      "   Chunks: 11\n",
      "   ‚úÖ ƒê√£ import 19 chunks v·ªõi document_id: FORM-10-M·∫´u-s·ªë-10C_E-HSM\n",
      "\n",
      "[10/31] ƒêang import: FORM-12-M·∫´u-s·ªë-12C-CGTT-\n",
      "   File: 12._M·∫´u_s·ªë_12C_CGTT_HH_r√∫t_g·ªçn.jsonl\n",
      "   Chunks: 11\n",
      "   ‚úÖ ƒê√£ import 11 chunks v·ªõi document_id: FORM-12-M·∫´u-s·ªë-12C-CGTT-\n",
      "\n",
      "[11/31] ƒêang import: FORM-12-M·∫´u-s·ªë-12D-CGTT-\n",
      "   File: 12._M·∫´u_s·ªë_12D_CGTT_PTV_r√∫t_g·ªçn.jsonl\n",
      "   Chunks: 12\n",
      "   ‚úÖ ƒê√£ import 11 chunks v·ªõi document_id: FORM-12-M·∫´u-s·ªë-12C-CGTT-\n",
      "\n",
      "[11/31] ƒêang import: FORM-12-M·∫´u-s·ªë-12D-CGTT-\n",
      "   File: 12._M·∫´u_s·ªë_12D_CGTT_PTV_r√∫t_g·ªçn.jsonl\n",
      "   Chunks: 12\n",
      "   ‚úÖ ƒê√£ import 12 chunks v·ªõi document_id: FORM-12-M·∫´u-s·ªë-12D-CGTT-\n",
      "\n",
      "[12/31] ƒêang import: FORM-12-M·∫´u-s·ªë-12G-CGTT-\n",
      "   File: 12._M·∫´u_s·ªë_12G_CGTT_r√∫t_g·ªçn_x·ª≠_l√Ω_t√¨nh_hu·ªëng_final.jsonl\n",
      "   Chunks: 4\n",
      "   ‚úÖ ƒê√£ import 12 chunks v·ªõi document_id: FORM-12-M·∫´u-s·ªë-12D-CGTT-\n",
      "\n",
      "[12/31] ƒêang import: FORM-12-M·∫´u-s·ªë-12G-CGTT-\n",
      "   File: 12._M·∫´u_s·ªë_12G_CGTT_r√∫t_g·ªçn_x·ª≠_l√Ω_t√¨nh_hu·ªëng_final.jsonl\n",
      "   Chunks: 4\n",
      "   ‚úÖ ƒê√£ import 4 chunks v·ªõi document_id: FORM-12-M·∫´u-s·ªë-12G-CGTT-\n",
      "\n",
      "[13/31] ƒêang import: FORM-13-M·∫´u-s·ªë-13_-Mua-s\n",
      "   File: 13._M·∫´u_s·ªë_13__Mua_s·∫Øm_tr·ª±c_tuy·∫øn.jsonl\n",
      "   Chunks: 21\n",
      "   ‚úÖ ƒê√£ import 4 chunks v·ªõi document_id: FORM-12-M·∫´u-s·ªë-12G-CGTT-\n",
      "\n",
      "[13/31] ƒêang import: FORM-13-M·∫´u-s·ªë-13_-Mua-s\n",
      "   File: 13._M·∫´u_s·ªë_13__Mua_s·∫Øm_tr·ª±c_tuy·∫øn.jsonl\n",
      "   Chunks: 21\n",
      "   ‚úÖ ƒê√£ import 21 chunks v·ªõi document_id: FORM-13-M·∫´u-s·ªë-13_-Mua-s\n",
      "\n",
      "[14/31] ƒêang import: FORM-7-M·∫´u-s·ªë-7C-E-HSMS\n",
      "   File: 7._M·∫´u_s·ªë_7C._E-HSMST_EP_s∆°_tuy·ªÉn.jsonl\n",
      "   Chunks: 15\n",
      "   ‚úÖ ƒê√£ import 21 chunks v·ªõi document_id: FORM-13-M·∫´u-s·ªë-13_-Mua-s\n",
      "\n",
      "[14/31] ƒêang import: FORM-7-M·∫´u-s·ªë-7C-E-HSMS\n",
      "   File: 7._M·∫´u_s·ªë_7C._E-HSMST_EP_s∆°_tuy·ªÉn.jsonl\n",
      "   Chunks: 15\n",
      "   ‚úÖ ƒê√£ import 15 chunks v·ªõi document_id: FORM-7-M·∫´u-s·ªë-7C-E-HSMS\n",
      "\n",
      "[15/31] ƒêang import: FORM-8-M·∫´u-s·ªë-8A-E-HSMT\n",
      "   File: 8._M·∫´u_s·ªë_8A._E-HSMT__EC_01_t√∫i.jsonl\n",
      "   Chunks: 101\n",
      "   ‚úÖ ƒê√£ import 15 chunks v·ªõi document_id: FORM-7-M·∫´u-s·ªë-7C-E-HSMS\n",
      "\n",
      "[15/31] ƒêang import: FORM-8-M·∫´u-s·ªë-8A-E-HSMT\n",
      "   File: 8._M·∫´u_s·ªë_8A._E-HSMT__EC_01_t√∫i.jsonl\n",
      "   Chunks: 101\n",
      "   ‚úÖ ƒê√£ import 101 chunks v·ªõi document_id: FORM-8-M·∫´u-s·ªë-8A-E-HSMT\n",
      "\n",
      "[16/31] ƒêang import: FORM-8-M·∫´u-s·ªë-8C-E-HSMS\n",
      "   File: 8._M·∫´u_s·ªë_8C._E-HSMST_EC_s∆°_tuy·ªÉn.jsonl\n",
      "   Chunks: 17\n",
      "   ‚úÖ ƒê√£ import 101 chunks v·ªõi document_id: FORM-8-M·∫´u-s·ªë-8A-E-HSMT\n",
      "\n",
      "[16/31] ƒêang import: FORM-8-M·∫´u-s·ªë-8C-E-HSMS\n",
      "   File: 8._M·∫´u_s·ªë_8C._E-HSMST_EC_s∆°_tuy·ªÉn.jsonl\n",
      "   Chunks: 17\n",
      "   ‚úÖ ƒê√£ import 17 chunks v·ªõi document_id: FORM-8-M·∫´u-s·ªë-8C-E-HSMS\n",
      "\n",
      "[17/31] ƒêang import: FORM-9-M·∫´u-s·ªë-9C_E-HSMST\n",
      "   File: 9._M·∫´u_s·ªë_9C_E-HSMST_PC_s∆°_tuy·ªÉn.jsonl\n",
      "   Chunks: 18\n",
      "   ‚úÖ ƒê√£ import 17 chunks v·ªõi document_id: FORM-8-M·∫´u-s·ªë-8C-E-HSMS\n",
      "\n",
      "[17/31] ƒêang import: FORM-9-M·∫´u-s·ªë-9C_E-HSMST\n",
      "   File: 9._M·∫´u_s·ªë_9C_E-HSMST_PC_s∆°_tuy·ªÉn.jsonl\n",
      "   Chunks: 18\n",
      "   ‚úÖ ƒê√£ import 18 chunks v·ªõi document_id: FORM-9-M·∫´u-s·ªë-9C_E-HSMST\n",
      "\n",
      "[18/31] ƒêang import: FORM-HSYC-HANGHOA-01B\n",
      "   File: 01B._M·∫´u_HSYC_h√†ng_h√≥a.jsonl\n",
      "   Chunks: 55\n",
      "   ‚úÖ ƒê√£ import 18 chunks v·ªõi document_id: FORM-9-M·∫´u-s·ªë-9C_E-HSMST\n",
      "\n",
      "[18/31] ƒêang import: FORM-HSYC-HANGHOA-01B\n",
      "   File: 01B._M·∫´u_HSYC_h√†ng_h√≥a.jsonl\n",
      "   Chunks: 55\n",
      "   ‚úÖ ƒê√£ import 55 chunks v·ªõi document_id: FORM-HSYC-HANGHOA-01B\n",
      "\n",
      "[19/31] ƒêang import: FORM-HSYC-TUVAN-01C\n",
      "   File: 01C._M·∫´u_HSYC_Phi_t∆∞_v·∫•n.jsonl\n",
      "   Chunks: 35\n",
      "   ‚úÖ ƒê√£ import 55 chunks v·ªõi document_id: FORM-HSYC-HANGHOA-01B\n",
      "\n",
      "[19/31] ƒêang import: FORM-HSYC-TUVAN-01C\n",
      "   File: 01C._M·∫´u_HSYC_Phi_t∆∞_v·∫•n.jsonl\n",
      "   Chunks: 35\n",
      "   ‚úÖ ƒê√£ import 35 chunks v·ªõi document_id: FORM-HSYC-TUVAN-01C\n",
      "\n",
      "[20/31] ƒêang import: FORM-HSYC-TUVAN-5\n",
      "   File: 5._M·∫´u_s·ªë_5A_E-HSMT_Phi_t∆∞_v·∫•n_1_t√∫i.jsonl\n",
      "   Chunks: 68\n",
      "   ‚úÖ ƒê√£ import 35 chunks v·ªõi document_id: FORM-HSYC-TUVAN-01C\n",
      "\n",
      "[20/31] ƒêang import: FORM-HSYC-TUVAN-5\n",
      "   File: 5._M·∫´u_s·ªë_5A_E-HSMT_Phi_t∆∞_v·∫•n_1_t√∫i.jsonl\n",
      "   Chunks: 68\n",
      "   ‚úÖ ƒê√£ import 68 chunks v·ªõi document_id: FORM-HSYC-TUVAN-5\n",
      "\n",
      "[21/31] ƒêang import: FORM-HSYC-TUVAN-6\n",
      "   File: 6._M·∫´u_s·ªë_6B_E-HSMQT_t∆∞_v·∫•n.jsonl\n",
      "   Chunks: 9\n",
      "   ‚úÖ ƒê√£ import 68 chunks v·ªõi document_id: FORM-HSYC-TUVAN-5\n",
      "\n",
      "[21/31] ƒêang import: FORM-HSYC-TUVAN-6\n",
      "   File: 6._M·∫´u_s·ªë_6B_E-HSMQT_t∆∞_v·∫•n.jsonl\n",
      "   Chunks: 9\n",
      "   ‚úÖ ƒê√£ import 9 chunks v·ªõi document_id: FORM-HSYC-TUVAN-6\n",
      "\n",
      "[22/31] ƒêang import: FORM-HSYC-XAYLAP-12\n",
      "   File: 12._M·∫´u_s·ªë_12E_CGTT_x√¢y_l·∫Øp_r√∫t_g·ªçn_final.jsonl\n",
      "   Chunks: 54\n",
      "   ‚úÖ ƒê√£ import 9 chunks v·ªõi document_id: FORM-HSYC-TUVAN-6\n",
      "\n",
      "[22/31] ƒêang import: FORM-HSYC-XAYLAP-12\n",
      "   File: 12._M·∫´u_s·ªë_12E_CGTT_x√¢y_l·∫Øp_r√∫t_g·ªçn_final.jsonl\n",
      "   Chunks: 54\n",
      "   ‚úÖ ƒê√£ import 54 chunks v·ªõi document_id: FORM-HSYC-XAYLAP-12\n",
      "\n",
      "[23/31] ƒêang import: TEMPLATE-BC-02A\n",
      "   File: 02A._M·∫´u_BCƒêG_cho_1_giai_ƒëo·∫°n_1_t√∫i.jsonl\n",
      "   Chunks: 27\n",
      "   ‚úÖ ƒê√£ import 54 chunks v·ªõi document_id: FORM-HSYC-XAYLAP-12\n",
      "\n",
      "[23/31] ƒêang import: TEMPLATE-BC-02A\n",
      "   File: 02A._M·∫´u_BCƒêG_cho_1_giai_ƒëo·∫°n_1_t√∫i.jsonl\n",
      "   Chunks: 27\n",
      "   ‚úÖ ƒê√£ import 27 chunks v·ªõi document_id: TEMPLATE-BC-02A\n",
      "\n",
      "[24/31] ƒêang import: TEMPLATE-BC-02B\n",
      "   File: 02B._M·∫´u_BCƒêG_cho_1_giai_ƒëo·∫°n_2_t√∫i_h·ªì_s∆°.jsonl\n",
      "   Chunks: 37\n",
      "   ‚úÖ ƒê√£ import 27 chunks v·ªõi document_id: TEMPLATE-BC-02A\n",
      "\n",
      "[24/31] ƒêang import: TEMPLATE-BC-02B\n",
      "   File: 02B._M·∫´u_BCƒêG_cho_1_giai_ƒëo·∫°n_2_t√∫i_h·ªì_s∆°.jsonl\n",
      "   Chunks: 37\n",
      "   ‚úÖ ƒê√£ import 37 chunks v·ªõi document_id: TEMPLATE-BC-02B\n",
      "\n",
      "[25/31] ƒêang import: TEMPLATE-BC-02C\n",
      "   File: 02C._M·∫´u_BCƒêG_cho_g√≥i_th·∫ßu_t∆∞_v·∫•n.jsonl\n",
      "   Chunks: 38\n",
      "   ‚úÖ ƒê√£ import 37 chunks v·ªõi document_id: TEMPLATE-BC-02B\n",
      "\n",
      "[25/31] ƒêang import: TEMPLATE-BC-02C\n",
      "   File: 02C._M·∫´u_BCƒêG_cho_g√≥i_th·∫ßu_t∆∞_v·∫•n.jsonl\n",
      "   Chunks: 38\n",
      "   ‚úÖ ƒê√£ import 38 chunks v·ªõi document_id: TEMPLATE-BC-02C\n",
      "\n",
      "[26/31] ƒêang import: TEMPLATE-BC-03A\n",
      "   File: 03A._M·∫´u_BCTƒê_HSMT.jsonl\n",
      "   Chunks: 8\n",
      "   ‚úÖ ƒê√£ import 38 chunks v·ªõi document_id: TEMPLATE-BC-02C\n",
      "\n",
      "[26/31] ƒêang import: TEMPLATE-BC-03A\n",
      "   File: 03A._M·∫´u_BCTƒê_HSMT.jsonl\n",
      "   Chunks: 8\n",
      "   ‚úÖ ƒê√£ import 8 chunks v·ªõi document_id: TEMPLATE-BC-03A\n",
      "\n",
      "[27/31] ƒêang import: TEMPLATE-BC-03B\n",
      "   File: 03B._M·∫´u_BCTƒê_danh_s√°ch_ƒë√°p_·ª©ng_v·ªÅ_k·ªπ_thu·∫≠t.jsonl\n",
      "   Chunks: 11\n",
      "   ‚úÖ ƒê√£ import 8 chunks v·ªõi document_id: TEMPLATE-BC-03A\n",
      "\n",
      "[27/31] ƒêang import: TEMPLATE-BC-03B\n",
      "   File: 03B._M·∫´u_BCTƒê_danh_s√°ch_ƒë√°p_·ª©ng_v·ªÅ_k·ªπ_thu·∫≠t.jsonl\n",
      "   Chunks: 11\n",
      "   ‚úÖ ƒê√£ import 11 chunks v·ªõi document_id: TEMPLATE-BC-03B\n",
      "\n",
      "[28/31] ƒêang import: TEMPLATE-BC-03C\n",
      "   File: 03C._M·∫´u_BCTƒê_KQLCNT.jsonl\n",
      "   Chunks: 11\n",
      "   ‚úÖ ƒê√£ import 11 chunks v·ªõi document_id: TEMPLATE-BC-03B\n",
      "\n",
      "[28/31] ƒêang import: TEMPLATE-BC-03C\n",
      "   File: 03C._M·∫´u_BCTƒê_KQLCNT.jsonl\n",
      "   Chunks: 11\n",
      "   ‚úÖ ƒê√£ import 11 chunks v·ªõi document_id: TEMPLATE-BC-03C\n",
      "\n",
      "[29/31] ƒêang import: TEMPLATE-BC-14B\n",
      "   File: 14B._M·∫´u_BCƒêG_PTV_HH_TBYT_quy_tr√¨nh_2_1_t√∫i_.jsonl\n",
      "   Chunks: 17\n",
      "   ‚úÖ ƒê√£ import 11 chunks v·ªõi document_id: TEMPLATE-BC-03C\n",
      "\n",
      "[29/31] ƒêang import: TEMPLATE-BC-14B\n",
      "   File: 14B._M·∫´u_BCƒêG_PTV_HH_TBYT_quy_tr√¨nh_2_1_t√∫i_.jsonl\n",
      "   Chunks: 17\n",
      "   ‚úÖ ƒê√£ import 17 chunks v·ªõi document_id: TEMPLATE-BC-14B\n",
      "\n",
      "[30/31] ƒêang import: TEMPLATE-BC-14C\n",
      "   File: 14C._M·∫´u_BCƒêG_HH_XL_PTV_hon_hop_TBYT_2_tui_.jsonl\n",
      "   Chunks: 43\n",
      "   ‚úÖ ƒê√£ import 17 chunks v·ªõi document_id: TEMPLATE-BC-14B\n",
      "\n",
      "[30/31] ƒêang import: TEMPLATE-BC-14C\n",
      "   File: 14C._M·∫´u_BCƒêG_HH_XL_PTV_hon_hop_TBYT_2_tui_.jsonl\n",
      "   Chunks: 43\n",
      "   ‚úÖ ƒê√£ import 43 chunks v·ªõi document_id: TEMPLATE-BC-14C\n",
      "\n",
      "[31/31] ƒêang import: TEMPLATE-BC-14D\n",
      "   File: 14D._Mau_BCƒêG_T∆∞_v·∫•n_.jsonl\n",
      "   Chunks: 20\n",
      "   ‚úÖ ƒê√£ import 43 chunks v·ªõi document_id: TEMPLATE-BC-14C\n",
      "\n",
      "[31/31] ƒêang import: TEMPLATE-BC-14D\n",
      "   File: 14D._Mau_BCƒêG_T∆∞_v·∫•n_.jsonl\n",
      "   Chunks: 20\n",
      "   ‚úÖ ƒê√£ import 20 chunks v·ªõi document_id: TEMPLATE-BC-14D\n",
      "\n",
      "============================================================\n",
      "‚úÖ Ho√†n Th√†nh Import!\n",
      "   Files ƒë√£ x·ª≠ l√Ω: 31\n",
      "   Chunks ƒë√£ import: 767\n",
      "============================================================\n",
      "   ‚úÖ ƒê√£ import 20 chunks v·ªõi document_id: TEMPLATE-BC-14D\n",
      "\n",
      "============================================================\n",
      "‚úÖ Ho√†n Th√†nh Import!\n",
      "   Files ƒë√£ x·ª≠ l√Ω: 31\n",
      "   Chunks ƒë√£ import: 767\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import c√°c utilities ƒë·ªÉ import\n",
    "sys.path.insert(0, '/home/sakana/Code/RAG-bidding')\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from src.config.models import settings\n",
    "\n",
    "print(\"üöÄ ƒêang Import C√°c Chunk Files B·ªã Thi·∫øu (V·ªõi Document ID M·ªõi)\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load danh s√°ch files b·ªã thi·∫øu\n",
    "missing_list_file = Path(\"data/processed/missing_chunk_files.json\")\n",
    "with open(missing_list_file, 'r') as f:\n",
    "    missing_files_info = json.load(f)\n",
    "\n",
    "# L·ªçc ch·ªâ l·∫•y files t·ªìn t·∫°i\n",
    "files_to_ingest = [f for f in missing_files_info if f['exists']]\n",
    "\n",
    "print(f\"üìÇ Files c·∫ßn import: {len(files_to_ingest)}\")\n",
    "print(f\"üìä T·ªïng chunks: {sum(f['chunk_count'] for f in files_to_ingest):,}\\n\")\n",
    "\n",
    "# Load metadata ƒë·ªÉ l·∫•y source_file\n",
    "METADATA_DIR = Path(\"data/processed/metadata\")\n",
    "metadata_files = list(METADATA_DIR.glob(\"*.json\"))\n",
    "metadata_by_chunk_file = {}\n",
    "\n",
    "for meta_file in metadata_files:\n",
    "    with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    chunk_file = metadata.get(\"output_file\")\n",
    "    if chunk_file:\n",
    "        metadata_by_chunk_file[chunk_file] = metadata\n",
    "\n",
    "print(f\"üìã Loaded metadata for {len(metadata_by_chunk_file)} files\\n\")\n",
    "\n",
    "# Kh·ªüi t·∫°o vector store\n",
    "embeddings = OpenAIEmbeddings(model=settings.embed_model)\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=settings.collection,\n",
    "    connection=settings.database_url,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ k·∫øt n·ªëi vector store: {settings.collection}\\n\")\n",
    "\n",
    "# Import t·ª´ng file\n",
    "total_ingested = 0\n",
    "total_errors = 0\n",
    "\n",
    "for i, file_info in enumerate(files_to_ingest, 1):\n",
    "    chunk_file = Path(file_info['chunk_file'])\n",
    "    new_doc_id = file_info['document_id']  # Document ID M·ªöI t·ª´ mapping\n",
    "    chunk_count = file_info['chunk_count']\n",
    "    \n",
    "    # L·∫•y source_file t·ª´ metadata\n",
    "    metadata_info = metadata_by_chunk_file.get(str(chunk_file))\n",
    "    source_file = metadata_info.get(\"source_file\") if metadata_info else \"\"\n",
    "    \n",
    "    print(f\"[{i}/{len(files_to_ingest)}] ƒêang import: {new_doc_id}\")\n",
    "    print(f\"   File: {chunk_file.name}\")\n",
    "    print(f\"   Chunks: {chunk_count}\")\n",
    "    \n",
    "    try:\n",
    "        # Load chunks t·ª´ file\n",
    "        chunks = []\n",
    "        with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "                        chunks.append(chunk)\n",
    "                    except json.JSONDecodeError:\n",
    "                        total_errors += 1\n",
    "        \n",
    "        # Chuy·ªÉn sang LangChain Documents V√Ä C·∫¨P NH·∫¨T document_id m·ªõi\n",
    "        documents = []\n",
    "        for chunk in chunks:\n",
    "            content = chunk.get(\"content\", \"\")\n",
    "            old_chunk_id = chunk.get(\"chunk_id\", \"\")\n",
    "            \n",
    "            # T·∫°o chunk_id m·ªõi t·ª´ new_doc_id\n",
    "            # Format c≈©: bidding_untitled_form_0000\n",
    "            # Format m·ªõi: FORM-01-Ph·ª•-l·ª•c_form_0000\n",
    "            if old_chunk_id:\n",
    "                parts = old_chunk_id.split(\"_\")\n",
    "                if len(parts) >= 3:\n",
    "                    # L·∫•y suffix (form_0000, section_0000, etc.)\n",
    "                    chunk_suffix = \"_\".join(parts[2:])\n",
    "                else:\n",
    "                    chunk_suffix = old_chunk_id\n",
    "                new_chunk_id = f\"{new_doc_id}_{chunk_suffix}\"\n",
    "            else:\n",
    "                new_chunk_id = f\"{new_doc_id}_chunk_{len(documents)}\"\n",
    "            \n",
    "            metadata = {\n",
    "                \"chunk_id\": new_chunk_id,  # ‚úÖ Chunk ID M·ªöI\n",
    "                \"document_id\": new_doc_id,  # ‚úÖ Document ID M·ªöI\n",
    "                \"document_type\": chunk.get(\"document_type\"),\n",
    "                \"hierarchy\": json.dumps(chunk.get(\"hierarchy\", []), ensure_ascii=False),\n",
    "                \"level\": chunk.get(\"level\"),\n",
    "                \"section_title\": chunk.get(\"section_title\", \"\"),\n",
    "                \"char_count\": chunk.get(\"char_count\", len(content)),\n",
    "                \"chunk_index\": chunk.get(\"chunk_index\", 0),\n",
    "                \"total_chunks\": chunk.get(\"total_chunks\", 1),\n",
    "                \"is_complete_unit\": chunk.get(\"is_complete_unit\", True),\n",
    "                \"has_table\": chunk.get(\"has_table\", False),\n",
    "                \"has_list\": chunk.get(\"has_list\", False),\n",
    "                \"source_file\": source_file,  # ‚úÖ Source file t·ª´ metadata\n",
    "            }\n",
    "            documents.append(Document(page_content=content, metadata=metadata))\n",
    "        \n",
    "        # Th√™m v√†o vector store (theo batch)\n",
    "        if documents:\n",
    "            batch_size = 50\n",
    "            for j in range(0, len(documents), batch_size):\n",
    "                batch = documents[j:j+batch_size]\n",
    "                vector_store.add_documents(batch)\n",
    "            \n",
    "            total_ingested += len(documents)\n",
    "            print(f\"   ‚úÖ ƒê√£ import {len(documents)} chunks v·ªõi document_id: {new_doc_id}\\n\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y chunks h·ª£p l·ªá\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå L·ªói: {e}\\n\")\n",
    "        total_errors += 1\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚úÖ Ho√†n Th√†nh Import!\")\n",
    "print(f\"   Files ƒë√£ x·ª≠ l√Ω: {len(files_to_ingest)}\")\n",
    "print(f\"   Chunks ƒë√£ import: {total_ingested:,}\")\n",
    "if total_errors > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  L·ªói: {total_errors}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db094846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ki·ªÉm Tra K·∫øt Qu·∫£ Import\n",
      "\n",
      "============================================================\n",
      "üìä Sau Khi Import:\n",
      "   S·ªë document_ids: 57\n",
      "   T·ªïng chunks: 6,242\n",
      "\n",
      "üîç Ki·ªÉm tra c√°c documents tr∆∞·ªõc ƒë√¢y b·ªã thi·∫øu:\n",
      "\n",
      "   ‚úÖ FORM-01-Ph·ª•-l·ª•c                          :   48 chunks\n",
      "   ‚úÖ FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-                  :    1 chunks\n",
      "   ‚úÖ FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-                  :   10 chunks\n",
      "   ‚úÖ FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b                  :   10 chunks\n",
      "   ‚úÖ FORM-043-M·∫´u-B√°o-c√°o-ki                  :    8 chunks\n",
      "\n",
      "============================================================\n",
      "‚úÖ TH√ÄNH C√îNG: ƒê√£ c√≥ ƒë·∫ßy ƒë·ªß 57/55 documents!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import asyncpg\n",
    "import os\n",
    "\n",
    "print(\"üîç Ki·ªÉm Tra K·∫øt Qu·∫£ Import\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# K·∫øt n·ªëi database\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"rag_bidding_v2\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"sakana\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"sakana123\")\n",
    "\n",
    "async def verify_ingestion():\n",
    "    conn = await asyncpg.connect(\n",
    "        host=DB_HOST, port=DB_PORT, database=DB_NAME,\n",
    "        user=DB_USER, password=DB_PASSWORD\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # ƒê·∫øm distinct document_ids\n",
    "        stats = await conn.fetchrow(\"\"\"\n",
    "            SELECT COUNT(DISTINCT cmetadata->>'document_id') as doc_count,\n",
    "                   COUNT(*) as total_chunks\n",
    "            FROM langchain_pg_embedding\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"üìä Sau Khi Import:\")\n",
    "        print(f\"   S·ªë document_ids: {stats['doc_count']}\")\n",
    "        print(f\"   T·ªïng chunks: {stats['total_chunks']:,}\")\n",
    "        print()\n",
    "        \n",
    "        # Ki·ªÉm tra c√°c documents tr∆∞·ªõc ƒë√¢y b·ªã thi·∫øu\n",
    "        missing_doc_ids = [\n",
    "            'FORM-01-Ph·ª•-l·ª•c', 'FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-', 'FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-',\n",
    "            'FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b', 'FORM-043-M·∫´u-B√°o-c√°o-ki'\n",
    "        ]\n",
    "        \n",
    "        print(f\"üîç Ki·ªÉm tra c√°c documents tr∆∞·ªõc ƒë√¢y b·ªã thi·∫øu:\\n\")\n",
    "        for doc_id in missing_doc_ids[:5]:\n",
    "            result = await conn.fetchrow(\"\"\"\n",
    "                SELECT COUNT(*) as count\n",
    "                FROM langchain_pg_embedding\n",
    "                WHERE cmetadata->>'document_id' = $1\n",
    "            \"\"\", doc_id)\n",
    "            \n",
    "            status = \"‚úÖ\" if result['count'] > 0 else \"‚ùå\"\n",
    "            print(f\"   {status} {doc_id:40} : {result['count']:4} chunks\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        \n",
    "        # So s√°nh Mong ƒë·ª£i vs Th·ª±c t·∫ø\n",
    "        expected_docs = 55\n",
    "        actual_docs = stats['doc_count']\n",
    "        \n",
    "        if actual_docs >= expected_docs:\n",
    "            print(f\"‚úÖ TH√ÄNH C√îNG: ƒê√£ c√≥ ƒë·∫ßy ƒë·ªß {actual_docs}/{expected_docs} documents!\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  CH∆ØA ƒê·ª¶: {actual_docs}/{expected_docs} documents\")\n",
    "            print(f\"   C√≤n thi·∫øu: {expected_docs - actual_docs} documents\")\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "await verify_ingestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04857fd1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã T·ªïng K·∫øt T√¨nh Tr·∫°ng Migration\n",
    "\n",
    "### ‚úÖ Nh·ªØng G√¨ Ch√∫ng Ta Ph√°t Hi·ªán:\n",
    "\n",
    "**V·∫•n ƒê·ªÅ Ban ƒê·∫ßu:**\n",
    "- Mong ƒë·ª£i: 55 documents ƒë∆∞·ª£c migrate\n",
    "- Th·ª±c t·∫ø: Ch·ªâ t√¨m th·∫•y 25 documents\n",
    "- Kho·∫£ng c√°ch: 31 documents b·ªã thi·∫øu\n",
    "\n",
    "**Ph√¢n T√≠ch Nguy√™n Nh√¢n:**\n",
    "1. ‚úÖ **Script Migration Step 4**: Th·ª±c ra ƒë√£ HO·∫†T ƒê·ªòNG ƒê√öNG!\n",
    "   - C·∫≠p nh·∫≠t ƒë∆∞·ª£c c√°c chunks c√≥ trong database\n",
    "   - Chunks ƒë√£ c√≥ format `document_id` v√† `chunk_id` ƒë√∫ng r·ªìi\n",
    "   \n",
    "2. ‚ùå **V·∫•n ƒê·ªÅ Th·ª±c S·ª±**: **L·ªó H·ªïng Import Data**\n",
    "   - 31 chunk files t·ªìn t·∫°i tr√™n disk nh∆∞ng **ch∆∞a bao gi·ªù ƒë∆∞·ª£c import** v√†o vector database\n",
    "   - ƒêi·ªÅu n√†y x·∫£y ra trong giai ƒëo·∫°n x·ª≠ l√Ω data ban ƒë·∫ßu (tr∆∞·ªõc khi migration)\n",
    "   - Migration kh√¥ng th·ªÉ c·∫≠p nh·∫≠t chunks kh√¥ng t·ªìn t·∫°i trong database\n",
    "\n",
    "**B·∫±ng Ch·ª©ng:**\n",
    "```\n",
    "Chunks trong database ƒë√£ c√≥ FORMAT M·ªöI:\n",
    "- LUA-57-2024-QH15_dieu_0143\n",
    "- FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-_section_0000\n",
    "- FORM-HSYC-TUVAN-01D_form_0004\n",
    "\n",
    "Documents b·ªã thi·∫øu (31):\n",
    "- FORM-01-Ph·ª•-l·ª•c ‚Üí chunk file t·ªìn t·∫°i, 0 chunks trong DB\n",
    "- FORM-041A-M·∫´u-K·∫ø-ho·∫°ch- ‚Üí chunk file t·ªìn t·∫°i, 0 chunks trong DB\n",
    "- ... v√† c√≤n 29 documents n·ªØa\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ K·∫ø Ho·∫°ch H√†nh ƒê·ªông:\n",
    "\n",
    "**Giai ƒêo·∫°n 1: Ho√†n Th√†nh Import Data** (Hi·ªán t·∫°i)\n",
    "1. ‚úÖ X√°c ƒë·ªãnh 31 chunk files b·ªã thi·∫øu\n",
    "2. üîÑ Import c√°c files thi·∫øu v√†o vector database (ch·∫°y cell ph√≠a tr√™n)\n",
    "3. ‚úÖ Verify: Ph·∫£i ƒë·∫°t 55 documents\n",
    "\n",
    "**Giai ƒêo·∫°n 2: Ho√†n Th√†nh C√°c B∆∞·ªõc Migration**\n",
    "1. Ch·∫°y Step 6: C·∫≠p nh·∫≠t `documents.total_chunks`\n",
    "2. Ch·∫°y Step 7: T·∫°o b√°o c√°o t·ªïng k·∫øt migration\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ H√†nh ƒê·ªông Ti·∫øp Theo:\n",
    "\n",
    "**Ch·∫°y c√°c cells ph√≠a tr√™n theo th·ª© t·ª±:**\n",
    "1. **X√°c ƒê·ªãnh Files B·ªã Thi·∫øu** ‚Üí Hi·ªÉn th·ªã 31 files c·∫ßn import\n",
    "2. **Import Files B·ªã Thi·∫øu** ‚Üí Th√™m v√†o vector database (~10-15 ph√∫t, c·∫ßn OpenAI API)\n",
    "3. **Ki·ªÉm Tra K·∫øt Qu·∫£ Import** ‚Üí X√°c nh·∫≠n c√≥ ƒë·ªß 55 documents ‚úÖ\n",
    "4. **Ti·∫øp T·ª•c Step 6** ‚Üí C·∫≠p nh·∫≠t s·ªë l∆∞·ª£ng chunks trong b·∫£ng documents\n",
    "5. **Step 7** ‚Üí T·∫°o b√°o c√°o migration cu·ªëi c√πng\n",
    "\n",
    "Script migration ho·∫°t ƒë·ªông ƒë√∫ng - ch√∫ng ta ch·ªâ c·∫ßn ho√†n th√†nh vi·ªác import data tr∆∞·ªõc!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90faaa05",
   "metadata": {},
   "source": [
    "## Step 6: Update documents.total_chunks\n",
    "\n",
    "**Th·ªùi gian:** 30 ph√∫t  \n",
    "**M·ª•c ƒë√≠ch:** Sync chunk counts t·ª´ database v√†o documents table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2447a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ C·∫≠p Nh·∫≠t documents.total_chunks...\n",
      "\n",
      "üìÇ T√¨m th·∫•y 59 documents trong b·∫£ng documents\n",
      "\n",
      "  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t 10/59...\n",
      "  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t 20/59...\n",
      "  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t 30/59...\n",
      "  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t 20/59...\n",
      "  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t 30/59...\n",
      "  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t 40/59...\n",
      "  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t 50/59...\n",
      "  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t 40/59...\n",
      "  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t 50/59...\n",
      "\n",
      "‚úÖ ƒê√£ c·∫≠p nh·∫≠t chunk counts cho 59 documents\n",
      "\n",
      "üìä Top 10 documents theo s·ªë l∆∞·ª£ng chunks:\n",
      "   FORM-HSYC-HANGHOA-4                      : 1293 chunks\n",
      "   LUA-H·ª¢P-NH·∫§T-126-2025-v·ªÅ                 :  617 chunks\n",
      "   ND-214-4.8.-CP                           :  595 chunks\n",
      "   TEMPLATE-BC-14A                          :  510 chunks\n",
      "   LUA-57-2024-QH15                         :  271 chunks\n",
      "   FORM-7-M·∫´u-s·ªë-7B-E-HSMT                  :  264 chunks\n",
      "   LUA-Luat-dau-thau-2023                   :  188 chunks\n",
      "   FORM-8-M·∫´u-s·ªë-8B-E-HSMT                  :  166 chunks\n",
      "   FORM-15-Phu-luc                          :  153 chunks\n",
      "   FORM-HSYC-TUVAN-01D                      :  134 chunks\n",
      "\n",
      "‚ö†Ô∏è  4 documents kh√¥ng c√≥ chunks trong vector DB\n",
      "\n",
      "‚úÖ ƒê√£ c·∫≠p nh·∫≠t chunk counts cho 59 documents\n",
      "\n",
      "üìä Top 10 documents theo s·ªë l∆∞·ª£ng chunks:\n",
      "   FORM-HSYC-HANGHOA-4                      : 1293 chunks\n",
      "   LUA-H·ª¢P-NH·∫§T-126-2025-v·ªÅ                 :  617 chunks\n",
      "   ND-214-4.8.-CP                           :  595 chunks\n",
      "   TEMPLATE-BC-14A                          :  510 chunks\n",
      "   LUA-57-2024-QH15                         :  271 chunks\n",
      "   FORM-7-M·∫´u-s·ªë-7B-E-HSMT                  :  264 chunks\n",
      "   LUA-Luat-dau-thau-2023                   :  188 chunks\n",
      "   FORM-8-M·∫´u-s·ªë-8B-E-HSMT                  :  166 chunks\n",
      "   FORM-15-Phu-luc                          :  153 chunks\n",
      "   FORM-HSYC-TUVAN-01D                      :  134 chunks\n",
      "\n",
      "‚ö†Ô∏è  4 documents kh√¥ng c√≥ chunks trong vector DB\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "\n",
    "print(\"üîÑ C·∫≠p Nh·∫≠t documents.total_chunks...\\n\")\n",
    "\n",
    "# Get DATABASE_URL if not already defined\n",
    "try:\n",
    "    DATABASE_URL\n",
    "except NameError:\n",
    "    DATABASE_URL = os.getenv(\n",
    "        \"DATABASE_URL_ASYNC\",\n",
    "        \"postgresql+asyncpg://sakana:sakana123@localhost/rag_bidding_v2\"\n",
    "    )\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "async def update_chunk_counts():\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        # L·∫•y t·∫•t c·∫£ documents\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT document_id FROM documents\n",
    "        \"\"\"))\n",
    "        documents = [row[0] for row in result.fetchall()]\n",
    "        \n",
    "        print(f\"üìÇ T√¨m th·∫•y {len(documents)} documents trong b·∫£ng documents\\n\")\n",
    "        \n",
    "        updated = 0\n",
    "        for doc_id in documents:\n",
    "            # ƒê·∫øm chunks cho document n√†y\n",
    "            result = await session.execute(\n",
    "                text(\"\"\"\n",
    "                    SELECT COUNT(*)\n",
    "                    FROM langchain_pg_embedding\n",
    "                    WHERE cmetadata->>'document_id' = :doc_id\n",
    "                \"\"\"),\n",
    "                {\"doc_id\": doc_id}\n",
    "            )\n",
    "            chunk_count = result.scalar()\n",
    "            \n",
    "            # Update documents table\n",
    "            await session.execute(\n",
    "                text(\"\"\"\n",
    "                    UPDATE documents\n",
    "                    SET total_chunks = :count,\n",
    "                        updated_at = NOW()\n",
    "                    WHERE document_id = :doc_id\n",
    "                \"\"\"),\n",
    "                {\"count\": chunk_count, \"doc_id\": doc_id}\n",
    "            )\n",
    "            \n",
    "            updated += 1\n",
    "            if updated % 10 == 0:\n",
    "                print(f\"  ‚úÖ ƒê√£ c·∫≠p nh·∫≠t {updated}/{len(documents)}...\")\n",
    "        \n",
    "        await session.commit()\n",
    "        return updated\n",
    "\n",
    "updated = await update_chunk_counts()\n",
    "print(f\"\\n‚úÖ ƒê√£ c·∫≠p nh·∫≠t chunk counts cho {updated} documents\")\n",
    "\n",
    "# Verify\n",
    "async with AsyncSessionLocal() as session:\n",
    "    result = await session.execute(text(\"\"\"\n",
    "        SELECT document_id, total_chunks\n",
    "        FROM documents\n",
    "        WHERE total_chunks > 0\n",
    "        ORDER BY total_chunks DESC\n",
    "        LIMIT 10\n",
    "    \"\"\"))\n",
    "    \n",
    "    print(f\"\\nüìä Top 10 documents theo s·ªë l∆∞·ª£ng chunks:\")\n",
    "    for row in result.fetchall():\n",
    "        print(f\"   {row[0]:40} : {row[1]:4} chunks\")\n",
    "    \n",
    "    # Check documents without chunks\n",
    "    result = await session.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) FROM documents WHERE total_chunks = 0\n",
    "    \"\"\"))\n",
    "    zero_count = result.scalar()\n",
    "    \n",
    "    if zero_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  {zero_count} documents kh√¥ng c√≥ chunks trong vector DB\")\n",
    "\n",
    "await engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585f078",
   "metadata": {},
   "source": [
    "## Step 7: Summary Report\n",
    "\n",
    "**Th·ªùi gian:** 30 ph√∫t  \n",
    "**T·∫°o report v·ªÅ migration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e59aca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä B√ÅO C√ÅO T·ªîNG K·∫æT MIGRATION\n",
      "============================================================\n",
      "Ng√†y: 2025-11-20 00:26:46\n",
      "\n",
      "üìÅ B·∫£ng Documents:\n",
      "   T·ªïng documents: 59\n",
      "   Documents active: 59\n",
      "   T·ªïng chunks (d·ª± ki·∫øn): 5472\n",
      "\n",
      "üóÑÔ∏è  Vector Database:\n",
      "   Document_ids duy nh·∫•t: 57\n",
      "   T·ªïng chunks: 6242\n",
      "   C√≥ source_file: 6239 (100.0%)\n",
      "\n",
      "üìÇ Theo Danh M·ª•c:\n",
      "   H·ªì s∆° m·ªùi th·∫ßu            : 37 docs, 2873 chunks\n",
      "   M·∫´u b√°o c√°o               : 10 docs,  722 chunks\n",
      "   C√¢u h·ªèi thi               :  4 docs,    0 chunks\n",
      "   Lu·∫≠t ch√≠nh                :  4 docs, 1154 chunks\n",
      "   Th√¥ng t∆∞                  :  2 docs,  123 chunks\n",
      "   Ngh·ªã ƒë·ªãnh                 :  1 docs,  595 chunks\n",
      "   Quy·∫øt ƒë·ªãnh                :  1 docs,    5 chunks\n",
      "\n",
      "‚ö†Ô∏è  Ki·ªÉm Tra Ch·∫•t L∆∞·ª£ng:\n",
      "   ‚ö†Ô∏è Documents c√≥ 0 chunks: 4\n",
      "   ‚ö†Ô∏è Chunks format c≈© c√≤n l·∫°i: 770\n",
      "   ‚ö†Ô∏è Chunks thi·∫øu source_file: 770\n",
      "\n",
      "üìä So S√°nh:\n",
      "   Documents trong table: 59\n",
      "   Documents trong vector DB: 57\n",
      "   ‚ÑπÔ∏è  2 documents trong table nh∆∞ng kh√¥ng c√≥ chunks trong vector DB\n",
      "\n",
      "============================================================\n",
      "‚úÖ MIGRATION HO√ÄN T·∫§T\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üìä B√ÅO C√ÅO T·ªîNG K·∫æT MIGRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Ng√†y: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Get DATABASE_URL if not already defined\n",
    "try:\n",
    "    DATABASE_URL\n",
    "except NameError:\n",
    "    DATABASE_URL = os.getenv(\n",
    "        \"DATABASE_URL_ASYNC\",\n",
    "        \"postgresql+asyncpg://sakana:sakana123@localhost/rag_bidding_v2\"\n",
    "    )\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "async def generate_report():\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        # Documents table stats\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_docs,\n",
    "                COUNT(*) FILTER (WHERE status = 'active') as active_docs,\n",
    "                SUM(total_chunks) as total_chunks\n",
    "            FROM documents\n",
    "        \"\"\"))\n",
    "        row = result.fetchone()\n",
    "        \n",
    "        print(\"üìÅ B·∫£ng Documents:\")\n",
    "        print(f\"   T·ªïng documents: {row[0]}\")\n",
    "        print(f\"   Documents active: {row[1]}\")\n",
    "        print(f\"   T·ªïng chunks (d·ª± ki·∫øn): {row[2]}\\n\")\n",
    "        \n",
    "        # Chunks in vector database\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT cmetadata->>'document_id') as unique_docs,\n",
    "                COUNT(*) as total_chunks,\n",
    "                COUNT(*) FILTER (WHERE cmetadata->>'source_file' IS NOT NULL) as with_source\n",
    "            FROM langchain_pg_embedding\n",
    "        \"\"\"))\n",
    "        row = result.fetchone()\n",
    "        \n",
    "        print(\"üóÑÔ∏è  Vector Database:\")\n",
    "        print(f\"   Document_ids duy nh·∫•t: {row[0]}\")\n",
    "        print(f\"   T·ªïng chunks: {row[1]}\")\n",
    "        print(f\"   C√≥ source_file: {row[2]} ({row[2]/row[1]*100:.1f}%)\\n\")\n",
    "        \n",
    "        # By category\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT category, COUNT(*) as docs, SUM(total_chunks) as chunks\n",
    "            FROM documents\n",
    "            GROUP BY category\n",
    "            ORDER BY docs DESC\n",
    "        \"\"\"))\n",
    "        \n",
    "        print(\"üìÇ Theo Danh M·ª•c:\")\n",
    "        for row in result.fetchall():\n",
    "            chunks_display = row[2] if row[2] is not None else 0\n",
    "            print(f\"   {row[0]:25} : {row[1]:2} docs, {chunks_display:4} chunks\")\n",
    "        \n",
    "        # Check for issues\n",
    "        print(f\"\\n‚ö†Ô∏è  Ki·ªÉm Tra Ch·∫•t L∆∞·ª£ng:\")\n",
    "        \n",
    "        # Documents without chunks\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT COUNT(*) FROM documents WHERE total_chunks = 0\n",
    "        \"\"\"))\n",
    "        zero_chunks = result.scalar()\n",
    "        status = \"‚úÖ\" if zero_chunks == 0 else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} Documents c√≥ 0 chunks: {zero_chunks}\")\n",
    "        \n",
    "        # Old format chunks\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM langchain_pg_embedding\n",
    "            WHERE cmetadata->>'document_id' LIKE '%untitled%'\n",
    "               OR cmetadata->>'chunk_id' LIKE '%untitled%'\n",
    "        \"\"\"))\n",
    "        old_format = result.scalar()\n",
    "        status = \"‚úÖ\" if old_format == 0 else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} Chunks format c≈© c√≤n l·∫°i: {old_format}\")\n",
    "        \n",
    "        # Missing source_file\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM langchain_pg_embedding\n",
    "            WHERE cmetadata->>'source_file' IS NULL\n",
    "               OR cmetadata->>'source_file' = ''\n",
    "        \"\"\"))\n",
    "        no_source = result.scalar()\n",
    "        status = \"‚úÖ\" if no_source == 0 else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} Chunks thi·∫øu source_file: {no_source}\")\n",
    "        \n",
    "        # Documents mismatch\n",
    "        docs_in_table = await session.execute(text(\"SELECT COUNT(*) FROM documents\"))\n",
    "        docs_in_vector = await session.execute(text(\n",
    "            \"SELECT COUNT(DISTINCT cmetadata->>'document_id') FROM langchain_pg_embedding\"\n",
    "        ))\n",
    "        \n",
    "        table_count = docs_in_table.scalar()\n",
    "        vector_count = docs_in_vector.scalar()\n",
    "        \n",
    "        print(f\"\\nüìä So S√°nh:\")\n",
    "        print(f\"   Documents trong table: {table_count}\")\n",
    "        print(f\"   Documents trong vector DB: {vector_count}\")\n",
    "        \n",
    "        if table_count > vector_count:\n",
    "            diff = table_count - vector_count\n",
    "            print(f\"   ‚ÑπÔ∏è  {diff} documents trong table nh∆∞ng kh√¥ng c√≥ chunks trong vector DB\")\n",
    "\n",
    "await generate_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ MIGRATION HO√ÄN T·∫§T\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "await engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be281a48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "Sau khi migration ho√†n t·∫•t, c·∫ßn update:\n",
    "\n",
    "### 1. API Endpoints\n",
    "- [ ] Update catalog endpoint ƒë·ªÉ query t·ª´ documents table\n",
    "- [ ] Update toggle endpoint ƒë·ªÉ support document_id\n",
    "- [ ] Add filtering by category, document_type\n",
    "\n",
    "### 2. Retrieval Pipeline\n",
    "- [ ] Update retriever ƒë·ªÉ filter by document status\n",
    "- [ ] Add document_id filter support\n",
    "- [ ] Test retrieval v·ªõi new document_ids\n",
    "\n",
    "### 3. Testing\n",
    "- [ ] Test catalog API: Should return 70 documents\n",
    "- [ ] Test toggle: Deactivate specific document\n",
    "- [ ] Test retrieval: Should exclude inactive documents\n",
    "- [ ] Performance testing\n",
    "\n",
    "### 4. Documentation\n",
    "- [ ] Update API documentation\n",
    "- [ ] Document new document_id format\n",
    "- [ ] Create migration runbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ea257",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóëÔ∏è Optional: Cleanup Old Collections\n",
    "\n",
    "**Th·ªùi gian:** 5 ph√∫t  \n",
    "**M·ª•c ƒë√≠ch:** X√≥a langchain_pg_collection c≈© sau khi verify th√†nh c√¥ng\n",
    "\n",
    "‚ö†Ô∏è **CH·ªà CH·∫†Y SAU KHI:**\n",
    "- ‚úÖ Step 5 verify: 100% chunks c√≥ document_id m·ªõi\n",
    "- ‚úÖ Step 6 complete: documents.total_chunks ƒë√£ sync\n",
    "- ‚úÖ Step 7 report: Kh√¥ng c√≥ l·ªói n√†o\n",
    "\n",
    "**L√Ω do c·∫ßn cleanup:**\n",
    "- `langchain_pg_collection` c√≥ 5 collections c≈© (kh√¥ng d√πng n·ªØa)\n",
    "- `langchain_pg_embedding.collection_id` v·∫´n tr·ªè v·ªÅ collections c≈©\n",
    "- Sau migration, ch√∫ng ta ch·ªâ d√πng `cmetadata->>'document_id'` m·ªõi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üóëÔ∏è  CLEANUP OLD COLLECTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get DATABASE_URL if not already defined\n",
    "try:\n",
    "    DATABASE_URL\n",
    "except NameError:\n",
    "    DATABASE_URL = os.getenv(\n",
    "        \"DATABASE_URL_ASYNC\",\n",
    "        \"postgresql+asyncpg://sakana:sakana123@localhost/rag_bidding_v2\"\n",
    "    )\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "async def cleanup_collections():\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        # 1. Check current collections\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT uuid, name, cmetadata \n",
    "            FROM langchain_pg_collection\n",
    "            ORDER BY name\n",
    "        \"\"\"))\n",
    "        collections = result.fetchall()\n",
    "        \n",
    "        print(f\"\\nüìã Current collections in langchain_pg_collection:\")\n",
    "        for col in collections:\n",
    "            print(f\"   ‚Ä¢ {col[1]} (UUID: {col[0]})\")\n",
    "        \n",
    "        if not collections:\n",
    "            print(\"   ‚úÖ No collections found - already cleaned!\")\n",
    "            return\n",
    "        \n",
    "        # 2. Check if any chunks still using collection_id\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT COUNT(*) \n",
    "            FROM langchain_pg_embedding \n",
    "            WHERE collection_id IS NOT NULL\n",
    "        \"\"\"))\n",
    "        chunks_with_collection = result.scalar()\n",
    "        \n",
    "        print(f\"\\nüìä Chunks still linked to collections: {chunks_with_collection}\")\n",
    "        \n",
    "        # 3. Option 1: Unlink chunks from collections (RECOMMENDED)\n",
    "        print(f\"\\nüîß Option 1: Unlink chunks (set collection_id = NULL)\")\n",
    "        print(\"   This keeps collections but removes the link.\")\n",
    "        print(\"   Safer option - can still rollback if needed.\\n\")\n",
    "        \n",
    "        user_input = input(\"   Execute Option 1? (yes/no): \").strip().lower()\n",
    "        \n",
    "        if user_input == 'yes':\n",
    "            await session.execute(text(\"\"\"\n",
    "                UPDATE langchain_pg_embedding\n",
    "                SET collection_id = NULL\n",
    "            \"\"\"))\n",
    "            await session.commit()\n",
    "            print(\"   ‚úÖ All chunks unlinked from collections\")\n",
    "        \n",
    "        # 4. Option 2: Delete collections (PERMANENT)\n",
    "        print(f\"\\nüóëÔ∏è  Option 2: DELETE collections permanently\")\n",
    "        print(\"   ‚ö†Ô∏è  WARNING: This is IRREVERSIBLE!\")\n",
    "        print(\"   ‚ö†Ô∏è  Only do this after 100% verification!\\n\")\n",
    "        \n",
    "        user_input = input(\"   Execute Option 2? (yes/no): \").strip().lower()\n",
    "        \n",
    "        if user_input == 'yes':\n",
    "            # First, unlink all chunks\n",
    "            await session.execute(text(\"\"\"\n",
    "                UPDATE langchain_pg_embedding\n",
    "                SET collection_id = NULL\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Then delete collections\n",
    "            await session.execute(text(\"\"\"\n",
    "                DELETE FROM langchain_pg_collection\n",
    "            \"\"\"))\n",
    "            \n",
    "            await session.commit()\n",
    "            print(\"   ‚úÖ Collections deleted permanently\")\n",
    "            print(\"   ‚úÖ All chunks unlinked\")\n",
    "        \n",
    "        # 5. Verify cleanup\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT COUNT(*) FROM langchain_pg_collection\n",
    "        \"\"\"))\n",
    "        remaining_collections = result.scalar()\n",
    "        \n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT COUNT(*) \n",
    "            FROM langchain_pg_embedding \n",
    "            WHERE collection_id IS NOT NULL\n",
    "        \"\"\"))\n",
    "        remaining_links = result.scalar()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä After cleanup:\")\n",
    "        print(f\"   Collections remaining: {remaining_collections}\")\n",
    "        print(f\"   Chunks with collection_id: {remaining_links}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "await cleanup_collections()\n",
    "await engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
