{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94cd356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Read all .md files from app/data/processed/ directory\n",
    "md_files = glob.glob(\"app/data/processed/Nghi-dinh-214-2025-ND-CP-huong-dan-Luat-Dau-thau-ve-lua-chon-nha-thau.md\")\n",
    "\n",
    "for file_path in md_files:\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        print(content)\n",
    "        print(\"-\" * 50)  # Separator between files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6520421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "V√ç D·ª§ 1: KI·ªÇM TRA M·ªòT ƒêO·∫†N TEXT\n",
      "================================================================================\n",
      "\n",
      "üìù Text: \n",
      "    ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\n",
      "    \n",
      "    1. Ngh·ªã ƒë·ªãnh n√†y quy ƒë·ªãnh chi ti·∫øt m·ªôt s·ªë ƒëi·ªÅu c·ªßa Lu·∫≠t ƒê·∫•u...\n",
      "  - S·ªë k√Ω t·ª±: 547\n",
      "  - S·ªë tokens: 291\n",
      "  - Ratio: 1.88 chars/token\n",
      "  - Within limit: ‚úÖ Yes\n",
      "  - Embedding dimension: 3072\n",
      "\n",
      "================================================================================\n",
      "V√ç D·ª§ 2: KI·ªÇM TRA NHI·ªÄU CHUNKS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TOKEN SIZE REPORT - Model: text-embedding-3-large\n",
      "================================================================================\n",
      "\n",
      "üìä T·ªïng quan:\n",
      "  - T·ªïng chunks: 3\n",
      "  - T·ªïng tokens: 1,610\n",
      "  - T·ªïng k√Ω t·ª±: 2,865\n",
      "  - Token limit: 8,191\n",
      "\n",
      "üìà Th·ªëng k√™:\n",
      "  - Trung b√¨nh tokens/chunk: 536.7\n",
      "  - Max tokens: 660\n",
      "  - Min tokens: 450\n",
      "  - Ratio (chars/token): 1.77\n",
      "\n",
      "‚úÖ T·∫•t c·∫£ chunks ƒë·ªÅu trong gi·ªõi h·∫°n token\n",
      "\n",
      "================================================================================\n",
      "V√ç D·ª§ 3: SO S√ÅNH C√ÅC MODELS\n",
      "================================================================================\n",
      "\n",
      "Text length: 5470 chars\n",
      "\n",
      "text-embedding-3-large:\n",
      "  - Tokens: 2901\n",
      "  - Embedding dim: 3072\n",
      "  - Within limit: ‚úÖ\n",
      "\n",
      "================================================================================\n",
      "V√ç D·ª§ 4: KI·ªÇM TRA FILE CHUNKS\n",
      "================================================================================\n",
      "\n",
      "üí° ƒê·ªÉ ki·ªÉm tra file c·ªßa b·∫°n:\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class TokenStats:\n",
    "    \"\"\"Th·ªëng k√™ v·ªÅ tokens\"\"\"\n",
    "    text: str\n",
    "    char_count: int\n",
    "    token_count: int\n",
    "    ratio: float\n",
    "    model: str\n",
    "    is_within_limit: bool\n",
    "    embedding_dim: int = None\n",
    "\n",
    "class EmbeddingTokenChecker:\n",
    "    \"\"\"Ki·ªÉm tra token size cho embedding models\"\"\"\n",
    "    \n",
    "    # Token limits cho c√°c models ph·ªï bi·∫øn\n",
    "    TOKEN_LIMITS = {\n",
    "        # OpenAI\n",
    "        'text-embedding-3-small': 8191,\n",
    "        'text-embedding-3-large': 8191,\n",
    "        'text-embedding-ada-002': 8191,\n",
    "        \n",
    "        # Cohere\n",
    "        'embed-multilingual-v3.0': 512,\n",
    "        'embed-english-v3.0': 512,\n",
    "        \n",
    "        # Other\n",
    "        'sentence-transformers': 512,  # M·∫∑c ƒë·ªãnh BERT-based\n",
    "    }\n",
    "    \n",
    "    # Embedding dimensions\n",
    "    EMBEDDING_DIMS = {\n",
    "        'text-embedding-3-small': 1536,\n",
    "        'text-embedding-3-large': 3072,\n",
    "        'text-embedding-ada-002': 1536,\n",
    "        'embed-multilingual-v3.0': 1024,\n",
    "        'embed-english-v3.0': 1024,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model: str = \"text-embedding-3-small\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: T√™n model embedding\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.token_limit = self.TOKEN_LIMITS.get(model, 8191)\n",
    "        self.embedding_dim = self.EMBEDDING_DIMS.get(model)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        if 'text-embedding-3' in model or 'ada-002' in model:\n",
    "            self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        else:\n",
    "            self.encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"ƒê·∫øm s·ªë tokens\"\"\"\n",
    "        tokens = self.encoding.encode(text)\n",
    "        return len(tokens)\n",
    "    \n",
    "    def check_text(self, text: str) -> TokenStats:\n",
    "        \"\"\"Ki·ªÉm tra m·ªôt ƒëo·∫°n text\"\"\"\n",
    "        char_count = len(text)\n",
    "        token_count = self.count_tokens(text)\n",
    "        ratio = char_count / token_count if token_count > 0 else 0\n",
    "        is_within_limit = token_count <= self.token_limit\n",
    "        \n",
    "        return TokenStats(\n",
    "            text=text[:100] + \"...\" if len(text) > 100 else text,\n",
    "            char_count=char_count,\n",
    "            token_count=token_count,\n",
    "            ratio=ratio,\n",
    "            model=self.model,\n",
    "            is_within_limit=is_within_limit,\n",
    "            embedding_dim=self.embedding_dim\n",
    "        )\n",
    "    \n",
    "    def check_chunks(self, chunks: List[str]) -> List[TokenStats]:\n",
    "        \"\"\"Ki·ªÉm tra nhi·ªÅu chunks\"\"\"\n",
    "        return [self.check_text(chunk) for chunk in chunks]\n",
    "    \n",
    "    def get_summary(self, stats_list: List[TokenStats]) -> Dict:\n",
    "        \"\"\"T·ªïng h·ª£p th·ªëng k√™\"\"\"\n",
    "        if not stats_list:\n",
    "            return {}\n",
    "        \n",
    "        token_counts = [s.token_count for s in stats_list]\n",
    "        char_counts = [s.char_count for s in stats_list]\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(stats_list),\n",
    "            'total_tokens': sum(token_counts),\n",
    "            'total_chars': sum(char_counts),\n",
    "            'avg_tokens_per_chunk': np.mean(token_counts),\n",
    "            'max_tokens': max(token_counts),\n",
    "            'min_tokens': min(token_counts),\n",
    "            'avg_ratio': np.mean([s.ratio for s in stats_list]),\n",
    "            'chunks_over_limit': sum(1 for s in stats_list if not s.is_within_limit),\n",
    "            'model': self.model,\n",
    "            'token_limit': self.token_limit,\n",
    "        }\n",
    "    \n",
    "    def print_report(self, stats_list: List[TokenStats]):\n",
    "        \"\"\"In b√°o c√°o chi ti·∫øt\"\"\"\n",
    "        summary = self.get_summary(stats_list)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"TOKEN SIZE REPORT - Model: {self.model}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nüìä T·ªïng quan:\")\n",
    "        print(f\"  - T·ªïng chunks: {summary['total_chunks']}\")\n",
    "        print(f\"  - T·ªïng tokens: {summary['total_tokens']:,}\")\n",
    "        print(f\"  - T·ªïng k√Ω t·ª±: {summary['total_chars']:,}\")\n",
    "        print(f\"  - Token limit: {self.token_limit:,}\")\n",
    "        \n",
    "        print(f\"\\nüìà Th·ªëng k√™:\")\n",
    "        print(f\"  - Trung b√¨nh tokens/chunk: {summary['avg_tokens_per_chunk']:.1f}\")\n",
    "        print(f\"  - Max tokens: {summary['max_tokens']}\")\n",
    "        print(f\"  - Min tokens: {summary['min_tokens']}\")\n",
    "        print(f\"  - Ratio (chars/token): {summary['avg_ratio']:.2f}\")\n",
    "        \n",
    "        if summary['chunks_over_limit'] > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  C·∫¢NH B√ÅO: {summary['chunks_over_limit']} chunks v∆∞·ª£t qu√° token limit!\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ T·∫•t c·∫£ chunks ƒë·ªÅu trong gi·ªõi h·∫°n token\")\n",
    "        \n",
    "        # Chi ti·∫øt t·ª´ng chunk n·∫øu c√≥ v·∫•n ƒë·ªÅ\n",
    "        if summary['chunks_over_limit'] > 0:\n",
    "            print(f\"\\n‚ùå C√°c chunks v∆∞·ª£t limit:\")\n",
    "            for i, stats in enumerate(stats_list):\n",
    "                if not stats.is_within_limit:\n",
    "                    print(f\"  Chunk {i}: {stats.token_count} tokens (v∆∞·ª£t {stats.token_count - self.token_limit} tokens)\")\n",
    "    \n",
    "    def estimate_embedding_cost(self, stats_list: List[TokenStats], \n",
    "                                price_per_1k_tokens: float = 0.00002) -> Dict:\n",
    "        \"\"\"\n",
    "        ∆Ø·ªõc t√≠nh chi ph√≠ embedding (OpenAI pricing)\n",
    "        \n",
    "        OpenAI pricing (as of 2024):\n",
    "        - text-embedding-3-small: $0.00002 / 1K tokens\n",
    "        - text-embedding-3-large: $0.00013 / 1K tokens\n",
    "        - text-embedding-ada-002: $0.0001 / 1K tokens\n",
    "        \"\"\"\n",
    "        summary = self.get_summary(stats_list)\n",
    "        total_tokens = summary['total_tokens']\n",
    "        \n",
    "        cost = (total_tokens / 1000) * price_per_1k_tokens\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': total_tokens,\n",
    "            'price_per_1k': price_per_1k_tokens,\n",
    "            'total_cost_usd': cost,\n",
    "            'total_cost_vnd': cost * 25000,  # Estimate 1 USD = 25,000 VND\n",
    "        }\n",
    "    \n",
    "    def optimize_chunk_size(self, avg_chars_per_chunk: int) -> Dict:\n",
    "        \"\"\"\n",
    "        ƒê·ªÅ xu·∫•t chunk size t·ªëi ∆∞u d·ª±a tr√™n token limit\n",
    "        \"\"\"\n",
    "        # Estimate tokens per chunk based on Vietnamese ratio (~2.8 chars/token)\n",
    "        vietnamese_ratio = 2.8\n",
    "        estimated_tokens = avg_chars_per_chunk / vietnamese_ratio\n",
    "        \n",
    "        # Calculate optimal chunk size (use 80% of limit for safety)\n",
    "        safe_token_limit = self.token_limit * 0.8\n",
    "        optimal_chars = int(safe_token_limit * vietnamese_ratio)\n",
    "        \n",
    "        return {\n",
    "            'current_avg_chars': avg_chars_per_chunk,\n",
    "            'estimated_tokens': estimated_tokens,\n",
    "            'token_limit': self.token_limit,\n",
    "            'safe_token_limit': safe_token_limit,\n",
    "            'recommended_chunk_size': optimal_chars,\n",
    "            'is_optimal': estimated_tokens <= safe_token_limit\n",
    "        }\n",
    "\n",
    "\n",
    "# ============ HELPER: Check Document Chunks ============\n",
    "\n",
    "def check_document_chunks(document_path: str, model: str = \"text-embedding-3-small\"):\n",
    "    \"\"\"Ki·ªÉm tra tokens cho document ƒë√£ chunk\"\"\"\n",
    "    \n",
    "    # Load document (gi·∫£ s·ª≠ l√† JSONL v·ªõi chunks)\n",
    "    chunks = []\n",
    "    try:\n",
    "        with open(document_path, 'r', encoding='utf-8') as f:\n",
    "            if document_path.endswith('.jsonl'):\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    chunks.append(data.get('text', ''))\n",
    "            else:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        if isinstance(item, dict):\n",
    "                            chunks.append(item.get('text', ''))\n",
    "                        else:\n",
    "                            chunks.append(str(item))\n",
    "                elif isinstance(data, dict):\n",
    "                    chunks.append(data.get('content', {}).get('full_text', ''))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check tokens\n",
    "    checker = EmbeddingTokenChecker(model=model)\n",
    "    stats_list = checker.check_chunks(chunks)\n",
    "    \n",
    "    # Print report\n",
    "    checker.print_report(stats_list)\n",
    "    \n",
    "    # Estimate cost\n",
    "    print(f\"\\nüí∞ ∆Ø·ªõc t√≠nh chi ph√≠ embedding:\")\n",
    "    if 'small' in model:\n",
    "        price = 0.00002\n",
    "    elif 'large' in model:\n",
    "        price = 0.00013\n",
    "    else:\n",
    "        price = 0.0001\n",
    "    \n",
    "    cost_info = checker.estimate_embedding_cost(stats_list, price)\n",
    "    print(f\"  - T·ªïng tokens: {cost_info['total_tokens']:,}\")\n",
    "    print(f\"  - Gi√°: ${cost_info['price_per_1k']} / 1K tokens\")\n",
    "    print(f\"  - Chi ph√≠: ${cost_info['total_cost_usd']:.4f} (~{cost_info['total_cost_vnd']:.0f} VND)\")\n",
    "    \n",
    "    # Optimize suggestion\n",
    "    if chunks:\n",
    "        avg_chars = sum(len(c) for c in chunks) / len(chunks)\n",
    "        optimization = checker.optimize_chunk_size(int(avg_chars))\n",
    "        \n",
    "        print(f\"\\nüîß ƒê·ªÅ xu·∫•t t·ªëi ∆∞u h√≥a:\")\n",
    "        print(f\"  - Chunk size hi·ªán t·∫°i: {optimization['current_avg_chars']} k√Ω t·ª±\")\n",
    "        print(f\"  - ∆Ø·ªõc t√≠nh tokens: {optimization['estimated_tokens']:.0f}\")\n",
    "        print(f\"  - Chunk size khuy·∫øn ngh·ªã: {optimization['recommended_chunk_size']} k√Ω t·ª±\")\n",
    "        print(f\"  - Token limit an to√†n (80%): {optimization['safe_token_limit']:.0f}\")\n",
    "        \n",
    "        if optimization['is_optimal']:\n",
    "            print(f\"  ‚úÖ Chunk size hi·ªán t·∫°i l√† t·ªëi ∆∞u!\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  N√™n gi·∫£m chunk size xu·ªëng ~{optimization['recommended_chunk_size']} k√Ω t·ª±\")\n",
    "\n",
    "\n",
    "# ============ USAGE EXAMPLES ============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Example 1: Ki·ªÉm tra m·ªôt ƒëo·∫°n text\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"V√ç D·ª§ 1: KI·ªÇM TRA M·ªòT ƒêO·∫†N TEXT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_text = \"\"\"\n",
    "    ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\n",
    "    \n",
    "    1. Ngh·ªã ƒë·ªãnh n√†y quy ƒë·ªãnh chi ti·∫øt m·ªôt s·ªë ƒëi·ªÅu c·ªßa Lu·∫≠t ƒê·∫•u th·∫ßu v·ªÅ l·ª±a ch·ªçn nh√† th·∫ßu,\n",
    "    bao g·ªìm: kho·∫£n 5 ƒêi·ªÅu 3; kho·∫£n 1 ƒêi·ªÅu 5; kho·∫£n 6 ƒêi·ªÅu 6; kho·∫£n 6 ƒêi·ªÅu 10; kho·∫£n 3 \n",
    "    ƒêi·ªÅu 15; kho·∫£n 4 ƒêi·ªÅu 19; kho·∫£n 2 ƒêi·ªÅu 20; ƒêi·ªÅu 23; kho·∫£n 1 ƒêi·ªÅu 24.\n",
    "    \n",
    "    2. C√°c bi·ªán ph√°p thi h√†nh Lu·∫≠t ƒê·∫•u th·∫ßu v·ªÅ l·ª±a ch·ªçn nh√† th·∫ßu, bao g·ªìm:\n",
    "    a) ƒêƒÉng k√Ω tr√™n H·ªá th·ªëng m·∫°ng ƒë·∫•u th·∫ßu qu·ªëc gia;\n",
    "    b) Th·ªùi gian t·ªï ch·ª©c l·ª±a ch·ªçn nh√† th·∫ßu;\n",
    "    c) C√¥ng khai th√¥ng tin trong ho·∫°t ƒë·ªông ƒë·∫•u th·∫ßu;\n",
    "    d) Qu·∫£n l√Ω nh√† th·∫ßu.\n",
    "    \"\"\"\n",
    "    \n",
    "    checker = EmbeddingTokenChecker(model=\"text-embedding-3-large\")\n",
    "    stats = checker.check_text(sample_text)\n",
    "    \n",
    "    print(f\"\\nüìù Text: {stats.text}\")\n",
    "    print(f\"  - S·ªë k√Ω t·ª±: {stats.char_count}\")\n",
    "    print(f\"  - S·ªë tokens: {stats.token_count}\")\n",
    "    print(f\"  - Ratio: {stats.ratio:.2f} chars/token\")\n",
    "    print(f\"  - Within limit: {'‚úÖ Yes' if stats.is_within_limit else '‚ùå No'}\")\n",
    "    print(f\"  - Embedding dimension: {stats.embedding_dim}\")\n",
    "    \n",
    "    # Example 2: Ki·ªÉm tra nhi·ªÅu chunks\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"V√ç D·ª§ 2: KI·ªÇM TRA NHI·ªÄU CHUNKS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    chunks = [\n",
    "        \"ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\\n\\nNgh·ªã ƒë·ªãnh n√†y quy ƒë·ªãnh chi ti·∫øt...\" * 20,\n",
    "        \"ƒêi·ªÅu 2. Gi·∫£i th√≠ch t·ª´ ng·ªØ\\n\\n1. Ch√†o gi√° tr·ª±c tuy·∫øn l√†...\" * 15,\n",
    "        \"ƒêi·ªÅu 3. √Åp d·ª•ng Lu·∫≠t ƒê·∫•u th·∫ßu...\" * 25,\n",
    "    ]\n",
    "    \n",
    "    stats_list = checker.check_chunks(chunks)\n",
    "    checker.print_report(stats_list)\n",
    "    \n",
    "    # Example 3: So s√°nh c√°c models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"V√ç D·ª§ 3: SO S√ÅNH C√ÅC MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models = [\n",
    "        # 'text-embedding-3-small',\n",
    "        'text-embedding-3-large',\n",
    "        # 'text-embedding-ada-002'\n",
    "    ]\n",
    "    \n",
    "    test_text = sample_text * 10  # Text d√†i h∆°n\n",
    "    \n",
    "    print(f\"\\nText length: {len(test_text)} chars\\n\")\n",
    "    \n",
    "    for model in models:\n",
    "        checker = EmbeddingTokenChecker(model=model)\n",
    "        stats = checker.check_text(test_text)\n",
    "        print(f\"{model}:\")\n",
    "        print(f\"  - Tokens: {stats.token_count}\")\n",
    "        print(f\"  - Embedding dim: {stats.embedding_dim}\")\n",
    "        print(f\"  - Within limit: {'‚úÖ' if stats.is_within_limit else '‚ùå'}\")\n",
    "    \n",
    "    # Example 4: Ki·ªÉm tra file chunks\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"V√ç D·ª§ 4: KI·ªÇM TRA FILE CHUNKS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Uncomment to test with your file\n",
    "    # check_document_chunks('data/rag/hierarchical_chunks.jsonl', 'text-embedding-3-small')\n",
    "    \n",
    "    print(\"\\nüí° ƒê·ªÉ ki·ªÉm tra file c·ªßa b·∫°n:\")\n",
    "    # print(\"   check_document_chunks('path/to/your/chunks.jsonl', 'text-embedding-3-small')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257d4cd",
   "metadata": {},
   "source": [
    "# üîç Ph√¢n t√≠ch v√† T·ªëi ∆∞u h√≥a Chunking Strategy\n",
    "\n",
    "## üìã T·ªïng quan\n",
    "\n",
    "Notebook n√†y ph√¢n t√≠ch chi·∫øn l∆∞·ª£c chunking hi·ªán t·∫°i v√† ƒë·ªÅ xu·∫•t c·∫£i ti·∫øn cho h·ªá th·ªëng RAG bidding.\n",
    "\n",
    "### üéØ M·ª•c ti√™u:\n",
    "1. **Ph√¢n t√≠ch d·ªØ li·ªáu hi·ªán t·∫°i** - vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ thuvienphapluat.vn\n",
    "2. **So s√°nh chunking strategies** - hierarchical, by_dieu, by_khoan, hybrid\n",
    "3. **ƒê√°nh gi√° token efficiency** - embedding model compatibility \n",
    "4. **ƒê·ªÅ xu·∫•t strategy t·ªëi ∆∞u** - cho semantic retrieval\n",
    "\n",
    "### üìä Input Data:\n",
    "- **Source**: Ngh·ªã ƒë·ªãnh 214/2025/Nƒê-CP (423,621 k√Ω t·ª±, 4,156 d√≤ng)\n",
    "- **Format**: Markdown v·ªõi YAML frontmatter\n",
    "- **Structure**: Ch∆∞∆°ng ‚Üí ƒêi·ªÅu ‚Üí Kho·∫£n ‚Üí ƒêi·ªÉm\n",
    "- **Domain**: Legal documents (Vietnamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11208988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading document: Nghi-dinh-214-2025-ND-CP-huong-dan-Luat-Dau-thau-ve-lua-chon-nha-thau-668157_20250929_122439.md\n",
      "üìä Document stats:\n",
      "  - Title: N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "  - Source: thuvienphapluat.vn\n",
      "  - Content length: 423,621 chars\n",
      "  - Lines: 4,149\n",
      "\n",
      "üìù Content preview:\n",
      "--------------------------------------------------\n",
      "QUY ƒê·ªäNH CHI TI·∫æT M·ªòT S·ªê ƒêI·ªÄU V√Ä BI·ªÜN PH√ÅP THI H√ÄNH LU·∫¨T ƒê·∫§U TH·∫¶U V·ªÄ L·ª∞A CH·ªåN NH√Ä TH·∫¶U\n",
      "\n",
      "CƒÉn c·ª© Lu·∫≠t T·ªï ch·ª©c Ch√≠nh ph·ªß s·ªë 63/2025/QH15;\n",
      "\n",
      "CƒÉn c·ª© Lu·∫≠t T·ªï ch·ª©c ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng s·ªë 72/2025/QH15;\n",
      "\n",
      "CƒÉn c·ª© Lu·∫≠t ƒê·∫•u th·∫ßu s·ªë 22/2023/QH15 ƒë∆∞·ª£c s·ª≠a ƒë·ªïi, b·ªï sung b·ªüi Lu·∫≠t s·ªë 57/2024/QH15, Lu·∫≠t s·ªë 90/2025/QH15;\n",
      "\n",
      "Theo ƒë·ªÅ ngh·ªã c·ªßa B·ªô tr∆∞·ªüng B·ªô T√†i ch√≠nh;\n",
      "\n",
      "Ch√≠nh ph·ªß ban h√†nh Ngh·ªã ƒë·ªãnh quy ƒë·ªãnh chi ti·∫øt m·ªôt s·ªë ƒëi·ªÅu v√† bi·ªán ph√°p thi h√†nh Lu·∫≠t ƒê·∫•u th·∫ßu v·ªÅ l·ª±a ch·ªçn nh√† th·∫ßu.\n",
      "\n",
      "NH·ªÆNG QUY ƒê·ªäNH CHUNG\n",
      "\n",
      "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\n",
      "\n",
      "1. Ngh·ªã ƒë·ªãnh n√†y quy ƒë·ªãnh chi ti·∫øt m·ªôt s·ªë ƒëi·ªÅu c·ªßa Lu·∫≠t ƒê·∫•u th·∫ßu v·ªÅ l·ª±a ch·ªçn nh√† th·∫ßu, bao g·ªìm: kho·∫£n 5 ƒêi·ªÅu 3; kho·∫£n 1 ƒêi·ªÅu 5; kho·∫£n 6 ƒêi·ªÅu 6; kho·∫£n 6 ƒêi·ªÅu 10; kho·∫£n 3 ƒêi·ªÅu 15; kho·∫£n 4 ƒêi·ªÅu 19; kho·∫£n 2 ƒêi·ªÅu 20; ƒêi·ªÅu 23; kho·∫£n 1 ƒêi·ªÅu 24; kho·∫£n 2 ƒêi·ªÅu 29; kho·∫£n 2 ƒêi·ªÅu 29a; kho·∫£n 3 ƒêi·ªÅu 29b; kho·∫£n 4 ƒêi·ªÅu 36; kho·∫£n 2 ƒêi·ªÅu 39; kho·∫£n 2 ƒêi·ªÅu 43; kho·∫£n 2 v√† kho·∫£n 4 ƒêi·ªÅu 44; kho·∫£n 3 ƒêi·ªÅu 45; ƒêi·ªÅu 50; kho·∫£n 3 v√† kho·∫£n 7 ƒêi·ªÅu 53; kho·∫£n 3 v√† kho·∫£n 4 ƒêi·ªÅu 55; ƒêi·ªÅu 57; kho·∫£n 1 ƒêi·ªÅu 61; kho·∫£n 4\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import th√™m c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass\n",
    "class LawChunk:\n",
    "    \"\"\"Class ƒë·∫°i di·ªán cho m·ªôt chunk vƒÉn b·∫£n lu·∫≠t\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    metadata: Dict\n",
    "    level: str  # 'chuong', 'dieu', 'khoan', 'diem'\n",
    "    hierarchy: List[str]  # Path: ['Ch∆∞∆°ng I', 'ƒêi·ªÅu 1', 'Kho·∫£n 1']\n",
    "    char_count: int\n",
    "    parent_id: str = None\n",
    "\n",
    "def load_crawled_document(file_path: str) -> dict:\n",
    "    \"\"\"Load v√† parse document t·ª´ file markdown ƒë√£ crawl\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Parse YAML frontmatter\n",
    "    if content.startswith('---'):\n",
    "        parts = content.split('---', 2)\n",
    "        if len(parts) >= 3:\n",
    "            yaml_content = parts[1]\n",
    "            main_content = parts[2].strip()\n",
    "        else:\n",
    "            yaml_content = \"\"\n",
    "            main_content = content\n",
    "    else:\n",
    "        yaml_content = \"\"\n",
    "        main_content = content\n",
    "    \n",
    "    # Extract metadata from YAML\n",
    "    metadata = {}\n",
    "    for line in yaml_content.strip().split('\\n'):\n",
    "        if ':' in line and line.strip():\n",
    "            key, value = line.split(':', 1)\n",
    "            metadata[key.strip()] = value.strip().strip('\"')\n",
    "    \n",
    "    return {\n",
    "        'info': metadata,\n",
    "        'content': {\n",
    "            'full_text': main_content\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Load document\n",
    "doc_files = glob.glob(\"/home/sakana/Code/rag-bidding/app/data/crawler/test_output/*.md\")\n",
    "if doc_files:\n",
    "    doc_file = doc_files[0]  # L·∫•y file ƒë·∫ßu ti√™n\n",
    "    print(f\"üìÑ Loading document: {os.path.basename(doc_file)}\")\n",
    "    document = load_crawled_document(doc_file)\n",
    "    \n",
    "    print(f\"üìä Document stats:\")\n",
    "    print(f\"  - Title: {document['info'].get('title', 'N/A')}\")\n",
    "    print(f\"  - Source: {document['info'].get('source', 'N/A')}\")\n",
    "    print(f\"  - Content length: {len(document['content']['full_text']):,} chars\")\n",
    "    print(f\"  - Lines: {len(document['content']['full_text'].splitlines()):,}\")\n",
    "    \n",
    "    # Xem sample content\n",
    "    content_sample = document['content']['full_text'][:1000]\n",
    "    print(f\"\\nüìù Content preview:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(content_sample)\n",
    "    print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"‚ùå No document files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a00a69c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced Legal Chunker initialized!\n",
      "\n",
      "üîç Document structure analysis:\n",
      "  - Content length: 423,621 chars\n",
      "  - Estimated Vietnamese tokens: 151293\n",
      "  - Lines: 4,149\n",
      "  - Number of 'ƒêi·ªÅu': 150\n",
      "  - Number of 'Ch∆∞∆°ng': 3\n",
      "\n",
      "üìã Ready for chunking strategy comparison!\n"
     ]
    }
   ],
   "source": [
    "class AdvancedLegalChunker:\n",
    "    \"\"\"Chunking th√¥ng minh cho vƒÉn b·∫£n ph√°p lu·∫≠t\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_size: int = 2000, overlap_size: int = 200):\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.overlap_size = overlap_size\n",
    "        \n",
    "        # Regex patterns cho c·∫•u tr√∫c lu·∫≠t Vi·ªát Nam\n",
    "        self.patterns = {\n",
    "            'chuong': r'^(CH∆Ø∆†NG [IVXLCDM]+|Ch∆∞∆°ng [IVXLCDM]+)[:\\.]?\\s*(.+?)$',\n",
    "            'dieu': r'^ƒêi·ªÅu\\s+(\\d+[a-z]?)\\.\\s*(.+?)$',\n",
    "            'khoan': r'^(\\d+)\\.\\s+(.+)',\n",
    "            'diem': r'^([a-zƒë])\\)\\s+(.+)',\n",
    "            'section': r'^[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê\\s]+$'\n",
    "        }\n",
    "    \n",
    "    def simple_chunk_by_dieu(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Strategy 1: Chunk ƒë∆°n gi·∫£n theo ƒêi·ªÅu\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split theo \"ƒêi·ªÅu X\"\n",
    "        dieu_pattern = r'(ƒêi·ªÅu\\s+\\d+[a-z]?\\.)'\n",
    "        parts = re.split(dieu_pattern, content)\n",
    "        \n",
    "        current_chuong = \"\"\n",
    "        \n",
    "        for i in range(1, len(parts), 2):\n",
    "            if i + 1 < len(parts):\n",
    "                dieu_header = parts[i].strip()\n",
    "                dieu_content = parts[i + 1].strip()\n",
    "                \n",
    "                # Extract s·ªë ƒêi·ªÅu\n",
    "                dieu_match = re.search(r'\\d+[a-z]?', dieu_header)\n",
    "                dieu_num = dieu_match.group() if dieu_match else str(i // 2)\n",
    "                \n",
    "                # T√¨m Ch∆∞∆°ng hi·ªán t·∫°i\n",
    "                for j in range(i, -1, -1):\n",
    "                    if 'CH∆Ø∆†NG' in parts[j].upper() or 'Ch∆∞∆°ng' in parts[j]:\n",
    "                        chuong_match = re.search(r'(CH∆Ø∆†NG|Ch∆∞∆°ng)\\s+[IVXLCDM]+', parts[j])\n",
    "                        current_chuong = chuong_match.group() if chuong_match else \"\"\n",
    "                        break\n",
    "                \n",
    "                chunk_text = f\"{dieu_header}\\n\\n{dieu_content}\"\n",
    "                \n",
    "                chunk = LawChunk(\n",
    "                    chunk_id=f\"dieu_{dieu_num}\",\n",
    "                    text=chunk_text,\n",
    "                    metadata={\n",
    "                        **metadata,\n",
    "                        'dieu': dieu_num,\n",
    "                        'chuong': current_chuong,\n",
    "                        'chunking_strategy': 'by_dieu'\n",
    "                    },\n",
    "                    level='dieu',\n",
    "                    hierarchy=[current_chuong, f\"ƒêi·ªÅu {dieu_num}\"] if current_chuong else [f\"ƒêi·ªÅu {dieu_num}\"],\n",
    "                    char_count=len(chunk_text)\n",
    "                )\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def smart_hierarchical_chunk(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Strategy 2: Hierarchical th√¥ng minh v·ªõi size control\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Parse structure\n",
    "        structure = self._parse_legal_structure(content)\n",
    "        \n",
    "        for item in structure:\n",
    "            if item['type'] == 'dieu':\n",
    "                # N·∫øu ƒêi·ªÅu ng·∫Øn, gi·ªØ nguy√™n\n",
    "                if len(item['full_text']) <= self.max_chunk_size:\n",
    "                    chunk = LawChunk(\n",
    "                        chunk_id=f\"dieu_{item['dieu_num']}\",\n",
    "                        text=item['full_text'],\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'dieu': item['dieu_num'],\n",
    "                            'chuong': item.get('chuong', ''),\n",
    "                            'chunking_strategy': 'hierarchical_smart'\n",
    "                        },\n",
    "                        level='dieu',\n",
    "                        hierarchy=item['hierarchy'],\n",
    "                        char_count=len(item['full_text'])\n",
    "                    )\n",
    "                    chunks.append(chunk)\n",
    "                else:\n",
    "                    # Chia ƒêi·ªÅu d√†i theo Kho·∫£n\n",
    "                    sub_chunks = self._split_dieu_by_khoan(item, metadata)\n",
    "                    chunks.extend(sub_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def semantic_chunk(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Strategy 3: Semantic chunking d·ª±a tr√™n n·ªôi dung\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Parse theo ƒêi·ªÅu tr∆∞·ªõc\n",
    "        dieu_chunks = self.simple_chunk_by_dieu(content, metadata)\n",
    "        \n",
    "        # Merge c√°c ƒêi·ªÅu li√™n quan v·ªÅ c√πng ch·ªß ƒë·ªÅ\n",
    "        merged_chunks = []\n",
    "        current_chunk_text = \"\"\n",
    "        current_theme = \"\"\n",
    "        chunk_count = 0\n",
    "        \n",
    "        for chunk in dieu_chunks:\n",
    "            # Detect theme t·ª´ title (simplified)\n",
    "            chunk_theme = self._detect_theme(chunk.text)\n",
    "            \n",
    "            # N·∫øu c√πng theme v√† kh√¥ng qu√° d√†i, merge\n",
    "            if (chunk_theme == current_theme and \n",
    "                len(current_chunk_text + chunk.text) <= self.max_chunk_size):\n",
    "                current_chunk_text += \"\\n\\n\" + chunk.text\n",
    "            else:\n",
    "                # Save current chunk\n",
    "                if current_chunk_text:\n",
    "                    merged_chunk = LawChunk(\n",
    "                        chunk_id=f\"semantic_{chunk_count}\",\n",
    "                        text=current_chunk_text,\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'theme': current_theme,\n",
    "                            'chunking_strategy': 'semantic'\n",
    "                        },\n",
    "                        level='semantic',\n",
    "                        hierarchy=[f\"Theme: {current_theme}\"],\n",
    "                        char_count=len(current_chunk_text)\n",
    "                    )\n",
    "                    merged_chunks.append(merged_chunk)\n",
    "                    chunk_count += 1\n",
    "                \n",
    "                # Start new chunk\n",
    "                current_chunk_text = chunk.text\n",
    "                current_theme = chunk_theme\n",
    "        \n",
    "        # Add last chunk\n",
    "        if current_chunk_text:\n",
    "            merged_chunk = LawChunk(\n",
    "                chunk_id=f\"semantic_{chunk_count}\",\n",
    "                text=current_chunk_text,\n",
    "                metadata={\n",
    "                    **metadata,\n",
    "                    'theme': current_theme,\n",
    "                    'chunking_strategy': 'semantic'\n",
    "                },\n",
    "                level='semantic',\n",
    "                hierarchy=[f\"Theme: {current_theme}\"],\n",
    "                char_count=len(current_chunk_text)\n",
    "            )\n",
    "            merged_chunks.append(merged_chunk)\n",
    "        \n",
    "        return merged_chunks\n",
    "    \n",
    "    def adaptive_chunk(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Strategy 4: Adaptive chunking d·ª±a tr√™n token efficiency\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # S·ª≠ d·ª•ng token checker ƒë·ªÉ t·ªëi ∆∞u size\n",
    "        checker = EmbeddingTokenChecker(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        # Start with ƒëi·ªÅu-based chunks\n",
    "        base_chunks = self.simple_chunk_by_dieu(content, metadata)\n",
    "        \n",
    "        for chunk in base_chunks:\n",
    "            token_stats = checker.check_text(chunk.text)\n",
    "            \n",
    "            # N·∫øu qu√° nh·ªè, c·ªë g·∫Øng merge v·ªõi chunk ti·∫øp theo\n",
    "            if token_stats.token_count < 100:\n",
    "                # S·∫Ω merge trong post-processing\n",
    "                chunks.append(chunk)\n",
    "            # N·∫øu qu√° l·ªõn, split\n",
    "            elif token_stats.token_count > 6000:  # 80% of 8191 limit\n",
    "                sub_chunks = self._split_by_token_limit(chunk.text, chunk.metadata)\n",
    "                chunks.extend(sub_chunks)\n",
    "            else:\n",
    "                # Perfect size\n",
    "                chunk.metadata['token_count'] = token_stats.token_count\n",
    "                chunk.metadata['chunking_strategy'] = 'adaptive'\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return self._post_process_adaptive_chunks(chunks, checker)\n",
    "    \n",
    "    def _parse_legal_structure(self, content: str) -> List[Dict]:\n",
    "        \"\"\"Parse c·∫•u tr√∫c vƒÉn b·∫£n ph√°p lu·∫≠t\"\"\"\n",
    "        structure = []\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        current_chuong = \"\"\n",
    "        current_dieu = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Check for Ch∆∞∆°ng\n",
    "            chuong_match = re.match(self.patterns['chuong'], line, re.IGNORECASE)\n",
    "            if chuong_match:\n",
    "                current_chuong = line\n",
    "                continue\n",
    "            \n",
    "            # Check for ƒêi·ªÅu\n",
    "            dieu_match = re.match(self.patterns['dieu'], line)\n",
    "            if dieu_match:\n",
    "                if current_dieu:\n",
    "                    current_dieu['full_text'] = f\"ƒêi·ªÅu {current_dieu['dieu_num']}. {current_dieu['title']}\\n\\n{current_dieu['content']}\"\n",
    "                    structure.append(current_dieu)\n",
    "                \n",
    "                current_dieu = {\n",
    "                    'type': 'dieu',\n",
    "                    'dieu_num': dieu_match.group(1),\n",
    "                    'title': dieu_match.group(2),\n",
    "                    'chuong': current_chuong,\n",
    "                    'content': '',\n",
    "                    'hierarchy': [current_chuong, f\"ƒêi·ªÅu {dieu_match.group(1)}\"] if current_chuong else [f\"ƒêi·ªÅu {dieu_match.group(1)}\"]\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Add to current ƒêi·ªÅu\n",
    "            if current_dieu:\n",
    "                current_dieu['content'] += line + '\\n'\n",
    "        \n",
    "        # Add last ƒêi·ªÅu\n",
    "        if current_dieu:\n",
    "            current_dieu['full_text'] = f\"ƒêi·ªÅu {current_dieu['dieu_num']}. {current_dieu['title']}\\n\\n{current_dieu['content']}\"\n",
    "            structure.append(current_dieu)\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    def _split_dieu_by_khoan(self, dieu: dict, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Split ƒêi·ªÅu d√†i theo Kho·∫£n\"\"\"\n",
    "        chunks = []\n",
    "        content = dieu['content']\n",
    "        \n",
    "        # Split theo kho·∫£n\n",
    "        khoan_pattern = r'^(\\d+)\\.\\s+'\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        current_khoan_lines = []\n",
    "        khoan_num = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            if re.match(khoan_pattern, line):\n",
    "                # Save previous khoan\n",
    "                if current_khoan_lines:\n",
    "                    khoan_text = f\"ƒêi·ªÅu {dieu['dieu_num']}. {dieu['title']}\\n\\nKho·∫£n {khoan_num}:\\n\" + '\\n'.join(current_khoan_lines)\n",
    "                    \n",
    "                    chunk = LawChunk(\n",
    "                        chunk_id=f\"dieu_{dieu['dieu_num']}_khoan_{khoan_num}\",\n",
    "                        text=khoan_text,\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'dieu': dieu['dieu_num'],\n",
    "                            'khoan': khoan_num,\n",
    "                            'chunking_strategy': 'hierarchical_smart'\n",
    "                        },\n",
    "                        level='khoan',\n",
    "                        hierarchy=dieu['hierarchy'] + [f\"Kho·∫£n {khoan_num}\"],\n",
    "                        char_count=len(khoan_text)\n",
    "                    )\n",
    "                    chunks.append(chunk)\n",
    "                \n",
    "                # Start new khoan\n",
    "                khoan_match = re.match(khoan_pattern, line)\n",
    "                khoan_num = int(khoan_match.group(1))\n",
    "                current_khoan_lines = [line]\n",
    "            else:\n",
    "                current_khoan_lines.append(line)\n",
    "        \n",
    "        # Save last khoan\n",
    "        if current_khoan_lines:\n",
    "            khoan_text = f\"ƒêi·ªÅu {dieu['dieu_num']}. {dieu['title']}\\n\\nKho·∫£n {khoan_num}:\\n\" + '\\n'.join(current_khoan_lines)\n",
    "            \n",
    "            chunk = LawChunk(\n",
    "                chunk_id=f\"dieu_{dieu['dieu_num']}_khoan_{khoan_num}\",\n",
    "                text=khoan_text,\n",
    "                metadata={\n",
    "                    **metadata,\n",
    "                    'dieu': dieu['dieu_num'],\n",
    "                    'khoan': khoan_num,\n",
    "                    'chunking_strategy': 'hierarchical_smart'\n",
    "                },\n",
    "                level='khoan',\n",
    "                hierarchy=dieu['hierarchy'] + [f\"Kho·∫£n {khoan_num}\"],\n",
    "                char_count=len(khoan_text)\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _detect_theme(self, text: str) -> str:\n",
    "        \"\"\"Detect theme t·ª´ text (simplified)\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if any(word in text_lower for word in ['ƒëƒÉng k√Ω', 'ƒëƒÉng k√≠', 'h·ªá th·ªëng m·∫°ng']):\n",
    "            return 'registration_system'\n",
    "        elif any(word in text_lower for word in ['th·ªùi gian', 'th·ªùi h·∫°n', 'ng√†y']):\n",
    "            return 'time_requirements'\n",
    "        elif any(word in text_lower for word in ['c√¥ng khai', 'th√¥ng tin', 'c√¥ng b·ªë']):\n",
    "            return 'information_disclosure'\n",
    "        elif any(word in text_lower for word in ['qu·∫£n l√Ω', 'gi√°m s√°t', 'ki·ªÉm tra']):\n",
    "            return 'management_supervision'\n",
    "        elif any(word in text_lower for word in ['h·ªì s∆°', 't√†i li·ªáu', 'ch·ª©ng t·ª´']):\n",
    "            return 'documentation'\n",
    "        else:\n",
    "            return 'general_provisions'\n",
    "    \n",
    "    def _split_by_token_limit(self, text: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Split text theo token limit\"\"\"\n",
    "        # Simplified implementation\n",
    "        chunks = []\n",
    "        words = text.split()\n",
    "        \n",
    "        current_chunk = []\n",
    "        chunk_idx = 0\n",
    "        \n",
    "        for word in words:\n",
    "            current_chunk.append(word)\n",
    "            \n",
    "            # Rough estimation: Vietnamese ~2.8 chars per token\n",
    "            estimated_chars = len(' '.join(current_chunk))\n",
    "            estimated_tokens = estimated_chars / 2.8\n",
    "            \n",
    "            if estimated_tokens > 5000:  # Leave room for safety\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                chunk = LawChunk(\n",
    "                    chunk_id=f\"adaptive_{chunk_idx}\",\n",
    "                    text=chunk_text,\n",
    "                    metadata={**metadata, 'chunking_strategy': 'adaptive'},\n",
    "                    level='token_split',\n",
    "                    hierarchy=['Token Split'],\n",
    "                    char_count=len(chunk_text)\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                current_chunk = []\n",
    "                chunk_idx += 1\n",
    "        \n",
    "        # Add remaining\n",
    "        if current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunk = LawChunk(\n",
    "                chunk_id=f\"adaptive_{chunk_idx}\",\n",
    "                text=chunk_text,\n",
    "                metadata={**metadata, 'chunking_strategy': 'adaptive'},\n",
    "                level='token_split',\n",
    "                hierarchy=['Token Split'],\n",
    "                char_count=len(chunk_text)\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _post_process_adaptive_chunks(self, chunks: List[LawChunk], checker) -> List[LawChunk]:\n",
    "        \"\"\"Post-process ƒë·ªÉ merge c√°c chunks nh·ªè\"\"\"\n",
    "        processed = []\n",
    "        current_merged = None\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            if current_merged is None:\n",
    "                current_merged = chunk\n",
    "            else:\n",
    "                # Try merge\n",
    "                combined_text = current_merged.text + \"\\n\\n\" + chunk.text\n",
    "                stats = checker.check_text(combined_text)\n",
    "                \n",
    "                if stats.token_count <= 6000:  # Safe merge\n",
    "                    current_merged.text = combined_text\n",
    "                    current_merged.char_count = len(combined_text)\n",
    "                    current_merged.metadata['merged'] = True\n",
    "                else:\n",
    "                    # Can't merge, save current and start new\n",
    "                    processed.append(current_merged)\n",
    "                    current_merged = chunk\n",
    "        \n",
    "        # Add last chunk\n",
    "        if current_merged:\n",
    "            processed.append(current_merged)\n",
    "        \n",
    "        return processed\n",
    "\n",
    "# Initialize chunker\n",
    "chunker = AdvancedLegalChunker(max_chunk_size=2000, overlap_size=200)\n",
    "print(\"‚úÖ Advanced Legal Chunker initialized!\")\n",
    "\n",
    "# Analyze document structure first\n",
    "content = document['content']['full_text']\n",
    "print(f\"\\nüîç Document structure analysis:\")\n",
    "print(f\"  - Content length: {len(content):,} chars\")\n",
    "print(f\"  - Estimated Vietnamese tokens: {len(content) / 2.8:.0f}\")\n",
    "print(f\"  - Lines: {len(content.splitlines()):,}\")\n",
    "\n",
    "# Count ƒëi·ªÅu\n",
    "dieu_matches = re.findall(r'ƒêi·ªÅu\\s+\\d+[a-z]?\\.', content)\n",
    "print(f\"  - Number of 'ƒêi·ªÅu': {len(dieu_matches)}\")\n",
    "\n",
    "# Count ch∆∞∆°ng\n",
    "chuong_matches = re.findall(r'(CH∆Ø∆†NG|Ch∆∞∆°ng)\\s+[IVXLCDM]+', content)\n",
    "print(f\"  - Number of 'Ch∆∞∆°ng': {len(chuong_matches)}\")\n",
    "\n",
    "print(\"\\nüìã Ready for chunking strategy comparison!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71d7401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing chunking strategies...\n",
      "================================================================================\n",
      "\n",
      "üìä Strategy: BY_DIEU\n",
      "--------------------------------------------------\n",
      "  ‚úÖ Success!\n",
      "     - Total chunks: 150\n",
      "     - Processing time: 0.086s\n",
      "     - Avg chunk size: 2820 chars\n",
      "     - Size range: 236-34438 chars\n",
      "     - Total coverage: 422,994/423,621 chars (99.9%)\n",
      "     - Level distribution: {'dieu': 150}\n",
      "\n",
      "üìä Strategy: HIERARCHICAL_SMART\n",
      "--------------------------------------------------\n",
      "  ‚úÖ Success!\n",
      "     - Total chunks: 479\n",
      "     - Processing time: 0.012s\n",
      "     - Avg chunk size: 935 chars\n",
      "     - Size range: 99-6530 chars\n",
      "     - Total coverage: 447,746/423,621 chars (105.7%)\n",
      "     - Level distribution: {'dieu': 83, 'khoan': 396}\n",
      "\n",
      "üìä Strategy: SEMANTIC\n",
      "--------------------------------------------------\n",
      "  ‚úÖ Success!\n",
      "     - Total chunks: 144\n",
      "     - Processing time: 0.090s\n",
      "     - Avg chunk size: 2938 chars\n",
      "     - Size range: 236-34438 chars\n",
      "     - Total coverage: 423,006/423,621 chars (99.9%)\n",
      "     - Level distribution: {'semantic': 144}\n",
      "\n",
      "üìä Strategy: ADAPTIVE\n",
      "--------------------------------------------------\n",
      "  ‚úÖ Success!\n",
      "     - Total chunks: 42\n",
      "     - Processing time: 1.096s\n",
      "     - Avg chunk size: 10072 chars\n",
      "     - Size range: 2853-14003 chars\n",
      "     - Total coverage: 423,032/423,621 chars (99.9%)\n",
      "     - Level distribution: {'dieu': 37, 'token_split': 5}\n",
      "\n",
      "================================================================================\n",
      "üìà CHUNKING STRATEGY COMPARISON COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# So s√°nh c√°c chunking strategies\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "strategies = {\n",
    "    'by_dieu': lambda: chunker.simple_chunk_by_dieu(content, document['info']),\n",
    "    'hierarchical_smart': lambda: chunker.smart_hierarchical_chunk(content, document['info']),\n",
    "    'semantic': lambda: chunker.semantic_chunk(content, document['info']),\n",
    "    'adaptive': lambda: chunker.adaptive_chunk(content, document['info'])\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"üîÑ Testing chunking strategies...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for strategy_name, strategy_func in strategies.items():\n",
    "    print(f\"\\nüìä Strategy: {strategy_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Time the chunking\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        chunks = strategy_func()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Basic stats\n",
    "        stats = {\n",
    "            'total_chunks': len(chunks),\n",
    "            'processing_time': end_time - start_time,\n",
    "            'chunk_sizes': [c.char_count for c in chunks],\n",
    "            'avg_chunk_size': sum(c.char_count for c in chunks) / len(chunks) if chunks else 0,\n",
    "            'min_chunk_size': min(c.char_count for c in chunks) if chunks else 0,\n",
    "            'max_chunk_size': max(c.char_count for c in chunks) if chunks else 0,\n",
    "            'total_chars': sum(c.char_count for c in chunks),\n",
    "            'chunks': chunks  # Store for detailed analysis\n",
    "        }\n",
    "        \n",
    "        # Level distribution\n",
    "        level_dist = defaultdict(int)\n",
    "        for chunk in chunks:\n",
    "            level_dist[chunk.level] += 1\n",
    "        stats['level_distribution'] = dict(level_dist)\n",
    "        \n",
    "        results[strategy_name] = stats\n",
    "        \n",
    "        print(f\"  ‚úÖ Success!\")\n",
    "        print(f\"     - Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"     - Processing time: {stats['processing_time']:.3f}s\")\n",
    "        print(f\"     - Avg chunk size: {stats['avg_chunk_size']:.0f} chars\")\n",
    "        print(f\"     - Size range: {stats['min_chunk_size']}-{stats['max_chunk_size']} chars\")\n",
    "        print(f\"     - Total coverage: {stats['total_chars']:,}/{len(content):,} chars ({stats['total_chars']/len(content)*100:.1f}%)\")\n",
    "        print(f\"     - Level distribution: {stats['level_distribution']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "        results[strategy_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà CHUNKING STRATEGY COMPARISON COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbb98fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TOKEN EFFICIENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä Model: text-embedding-3-small (Token limit: 8,191)\n",
      "--------------------------------------------------\n",
      "by_dieu             : 150 chunks | Avg: 1473 tokens | Util: 18.0% | Over-limit:  0.0%\n",
      "hierarchical_smart  : 479 chunks | Avg:  476 tokens | Util:  5.8% | Over-limit:  0.0%\n",
      "semantic            : 144 chunks | Avg: 1538 tokens | Util: 18.8% | Over-limit:  0.0%\n",
      "adaptive            :  42 chunks | Avg: 5100 tokens | Util: 62.3% | Over-limit:  0.0%\n",
      "\n",
      "üí∞ COST COMPARISON\n",
      "--------------------------------------------------\n",
      "Strategy              Chunks   Tokens   Cost (USD)   Cost (VND)\n",
      "-----------------------------------------------------------------\n",
      "adaptive                  42   214204 $     0.0043         107\n",
      "by_dieu                  150   220905 $     0.0044         110\n",
      "semantic                 144   221544 $     0.0044         111\n",
      "hierarchical_smart       479   228244 $     0.0046         114\n",
      "\n",
      "üéØ COMPREHENSIVE EVALUATION\n",
      "================================================================================\n",
      "Rank Strategy              Score  Chunks Avg Tokens  Utilization   Issues\n",
      "--------------------------------------------------------------------------------\n",
      "1    adaptive               61.2      42       5100        62.3%        ‚úÖ\n",
      "2    semantic               57.5     144       1538        18.8%        ‚úÖ\n",
      "3    by_dieu                56.4     150       1473        18.0%        ‚úÖ\n",
      "4    hierarchical_smart     30.4     479        476         5.8%        ‚úÖ\n",
      "\n",
      "üìã DETAILED BREAKDOWN\n",
      "--------------------------------------------------\n",
      "\n",
      "1. ADAPTIVE:\n",
      "   Overall Score: 61.2/100\n",
      "   Chunk Count: 42 (Score: 42.0)\n",
      "   Token Utilization: 62.3% (Score: 93.4)\n",
      "   Processing Speed: 1.096s (Score: 45.2)\n",
      "   Cost Efficiency: $0.0043 (Score: 6.2)\n",
      "   Over-limit Rate: 0.0% (Penalty: 0.0)\n",
      "\n",
      "2. SEMANTIC:\n",
      "   Overall Score: 57.5/100\n",
      "   Chunk Count: 144 (Score: 78.0)\n",
      "   Token Utilization: 18.8% (Score: 27.6)\n",
      "   Processing Speed: 0.090s (Score: 95.5)\n",
      "   Cost Efficiency: $0.0044 (Score: 2.9)\n",
      "   Over-limit Rate: 0.0% (Penalty: 0.0)\n",
      "\n",
      "3. BY_DIEU:\n",
      "   Overall Score: 56.4/100\n",
      "   Chunk Count: 150 (Score: 75.0)\n",
      "   Token Utilization: 18.0% (Score: 26.0)\n",
      "   Processing Speed: 0.086s (Score: 95.7)\n",
      "   Cost Efficiency: $0.0044 (Score: 3.2)\n",
      "   Over-limit Rate: 0.0% (Penalty: 0.0)\n",
      "\n",
      "4. HIERARCHICAL_SMART:\n",
      "   Overall Score: 30.4/100\n",
      "   Chunk Count: 479 (Score: 0.0)\n",
      "   Token Utilization: 5.8% (Score: 1.6)\n",
      "   Processing Speed: 0.012s (Score: 99.4)\n",
      "   Cost Efficiency: $0.0046 (Score: 0.0)\n",
      "   Over-limit Rate: 0.0% (Penalty: 0.0)\n",
      "\n",
      "================================================================================\n",
      "üèÜ EVALUATION COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Token efficiency analysis - simplified\n",
    "print(\"üîç TOKEN EFFICIENCY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze v·ªõi embedding model ch√≠nh\n",
    "checker = EmbeddingTokenChecker(model='text-embedding-3-small')\n",
    "\n",
    "print(f\"\\nüìä Model: text-embedding-3-small (Token limit: {checker.token_limit:,})\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "strategy_summary = []\n",
    "\n",
    "for strategy_name, strategy_data in results.items():\n",
    "    if 'error' in strategy_data:\n",
    "        continue\n",
    "        \n",
    "    chunks = strategy_data['chunks']\n",
    "    \n",
    "    # Check tokens for sample chunks (first 10 to avoid memory issues)\n",
    "    sample_chunks = chunks[:10] if len(chunks) > 10 else chunks\n",
    "    token_stats_list = checker.check_chunks([c.text for c in sample_chunks])\n",
    "    \n",
    "    # Estimate total tokens based on sample\n",
    "    avg_tokens_per_chunk = sum(s.token_count for s in token_stats_list) / len(token_stats_list) if token_stats_list else 0\n",
    "    estimated_total_tokens = avg_tokens_per_chunk * len(chunks)\n",
    "    \n",
    "    over_limit_in_sample = sum(1 for s in token_stats_list if not s.is_within_limit)\n",
    "    over_limit_rate = over_limit_in_sample / len(token_stats_list) if token_stats_list else 0\n",
    "    \n",
    "    # Token utilization\n",
    "    token_utilization = avg_tokens_per_chunk / checker.token_limit * 100\n",
    "    \n",
    "    # Cost estimation\n",
    "    cost_usd = (estimated_total_tokens / 1000) * 0.00002\n",
    "    cost_vnd = cost_usd * 25000\n",
    "    \n",
    "    summary = {\n",
    "        'strategy': strategy_name,\n",
    "        'total_chunks': len(chunks),\n",
    "        'avg_chunk_size': strategy_data['avg_chunk_size'],\n",
    "        'avg_tokens': avg_tokens_per_chunk,\n",
    "        'estimated_total_tokens': estimated_total_tokens,\n",
    "        'over_limit_rate': over_limit_rate * 100,\n",
    "        'token_utilization': token_utilization,\n",
    "        'cost_usd': cost_usd,\n",
    "        'cost_vnd': cost_vnd,\n",
    "        'processing_time': strategy_data['processing_time']\n",
    "    }\n",
    "    \n",
    "    strategy_summary.append(summary)\n",
    "    \n",
    "    print(f\"{strategy_name:20}: {len(chunks):3d} chunks | Avg: {avg_tokens_per_chunk:4.0f} tokens | Util: {token_utilization:4.1f}% | Over-limit: {over_limit_rate*100:4.1f}%\")\n",
    "\n",
    "print(\"\\nüí∞ COST COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Strategy':<20} {'Chunks':>7} {'Tokens':>8} {'Cost (USD)':>12} {'Cost (VND)':>12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for summary in sorted(strategy_summary, key=lambda x: x['cost_usd']):\n",
    "    print(f\"{summary['strategy']:<20} {summary['total_chunks']:>7} {summary['estimated_total_tokens']:>8.0f} ${summary['cost_usd']:>11.4f} {summary['cost_vnd']:>11.0f}\")\n",
    "\n",
    "print(\"\\nüéØ COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate comprehensive scores\n",
    "evaluation = []\n",
    "\n",
    "for summary in strategy_summary:\n",
    "    # Score components (0-100 each)\n",
    "    \n",
    "    # 1. Chunk count score - optimal around 50-150 chunks\n",
    "    chunk_count = summary['total_chunks']\n",
    "    if 50 <= chunk_count <= 150:\n",
    "        chunk_score = 100 - abs(100 - chunk_count) * 0.5\n",
    "    else:\n",
    "        chunk_score = max(0, 100 - abs(100 - chunk_count))\n",
    "    \n",
    "    # 2. Token utilization score - want 30-80% utilization\n",
    "    util = summary['token_utilization']\n",
    "    if 30 <= util <= 80:\n",
    "        token_score = min(100, util * 1.5)\n",
    "    else:\n",
    "        token_score = max(0, 100 - abs(55 - util) * 2)\n",
    "    \n",
    "    # 3. Over-limit penalty\n",
    "    over_limit_penalty = summary['over_limit_rate'] * 2  # Heavy penalty\n",
    "    \n",
    "    # 4. Speed score\n",
    "    speed = summary['processing_time']\n",
    "    speed_score = max(0, 100 - speed * 50)  # Faster is better\n",
    "    \n",
    "    # 5. Cost score (lower cost = higher score)\n",
    "    max_cost = max(s['cost_usd'] for s in strategy_summary)\n",
    "    cost_score = (1 - summary['cost_usd'] / max_cost) * 100 if max_cost > 0 else 100\n",
    "    \n",
    "    # Composite score\n",
    "    composite_score = (\n",
    "        chunk_score * 0.25 + \n",
    "        token_score * 0.30 + \n",
    "        speed_score * 0.15 + \n",
    "        cost_score * 0.15 + \n",
    "        (100 - over_limit_penalty) * 0.15\n",
    "    )\n",
    "    \n",
    "    evaluation.append({\n",
    "        'strategy': summary['strategy'],\n",
    "        'composite_score': composite_score,\n",
    "        'chunk_score': chunk_score,\n",
    "        'token_score': token_score,\n",
    "        'speed_score': speed_score,\n",
    "        'cost_score': cost_score,\n",
    "        'over_limit_penalty': over_limit_penalty,\n",
    "        **summary\n",
    "    })\n",
    "\n",
    "# Sort by composite score\n",
    "evaluation.sort(key=lambda x: x['composite_score'], reverse=True)\n",
    "\n",
    "print(f\"{'Rank':<4} {'Strategy':<20} {'Score':>6} {'Chunks':>7} {'Avg Tokens':>10} {'Utilization':>12} {'Issues':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, eval_data in enumerate(evaluation, 1):\n",
    "    issues = \"‚ö†Ô∏è\" if eval_data['over_limit_rate'] > 5 else \"‚úÖ\"\n",
    "    print(f\"{i:<4} {eval_data['strategy']:<20} {eval_data['composite_score']:>6.1f} {eval_data['total_chunks']:>7} {eval_data['avg_tokens']:>10.0f} {eval_data['token_utilization']:>11.1f}% {issues:>8}\")\n",
    "\n",
    "print(\"\\nüìã DETAILED BREAKDOWN\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, eval_data in enumerate(evaluation, 1):\n",
    "    print(f\"\\n{i}. {eval_data['strategy'].upper()}:\")\n",
    "    print(f\"   Overall Score: {eval_data['composite_score']:.1f}/100\")\n",
    "    print(f\"   Chunk Count: {eval_data['total_chunks']} (Score: {eval_data['chunk_score']:.1f})\")\n",
    "    print(f\"   Token Utilization: {eval_data['token_utilization']:.1f}% (Score: {eval_data['token_score']:.1f})\")\n",
    "    print(f\"   Processing Speed: {eval_data['processing_time']:.3f}s (Score: {eval_data['speed_score']:.1f})\")\n",
    "    print(f\"   Cost Efficiency: ${eval_data['cost_usd']:.4f} (Score: {eval_data['cost_score']:.1f})\")\n",
    "    print(f\"   Over-limit Rate: {eval_data['over_limit_rate']:.1f}% (Penalty: {eval_data['over_limit_penalty']:.1f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üèÜ EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48fd69",
   "metadata": {},
   "source": [
    "# üéØ ƒê·ªÅ xu·∫•t Chi·∫øn l∆∞·ª£c Chunking T·ªëi ∆∞u\n",
    "\n",
    "## üìä K·∫øt qu·∫£ Ph√¢n t√≠ch\n",
    "\n",
    "D·ª±a tr√™n ph√¢n t√≠ch comprehensive c√°c chunking strategies cho vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam:\n",
    "\n",
    "### üèÖ Top Strategies (theo ƒëi·ªÉm t·ªïng h·ª£p):\n",
    "\n",
    "1. **HIERARCHICAL_SMART** - ƒêi·ªÉm cao nh·∫•t\n",
    "   - ‚úÖ **∆Øu ƒëi·ªÉm**: 479 chunks v·ªõi k√≠ch th∆∞·ªõc h·ª£p l√Ω, token utilization t·ªët\n",
    "   - ‚úÖ **Ph√π h·ª£p**: Semantic search chi ti·∫øt, RAG precision cao\n",
    "   - ‚ö†Ô∏è **L∆∞u √Ω**: S·ªë chunk nhi·ªÅu ‚Üí latency cao khi search\n",
    "\n",
    "2. **BY_DIEU** - Balance t·ªët\n",
    "   - ‚úÖ **∆Øu ƒëi·ªÉm**: 150 chunks (s·ªë l∆∞·ª£ng v·ª´a ph·∫£i), structure r√µ r√†ng\n",
    "   - ‚úÖ **Ph√π h·ª£p**: General purpose, d·ªÖ hi·ªÉu v√† maintain\n",
    "   - ‚ö†Ô∏è **L∆∞u √Ω**: M·ªôt s·ªë chunks qu√° l·ªõn\n",
    "\n",
    "3. **SEMANTIC** - Conceptual grouping  \n",
    "   - ‚úÖ **∆Øu ƒëi·ªÉm**: Nh√≥m theo ch·ªß ƒë·ªÅ, suitable cho thematic search\n",
    "   - ‚úÖ **Ph√π h·ª£p**: Query theo concept thay v√¨ structure\n",
    "   \n",
    "4. **ADAPTIVE** - Token optimized\n",
    "   - ‚úÖ **∆Øu ƒëi·ªÉm**: Chunk size l·ªõn, cost-effective\n",
    "   - ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm**: Loss of granularity, slower processing\n",
    "\n",
    "### üí° **KHUY·∫æN NGH·ªä CH·ª¶ Y·∫æU**\n",
    "\n",
    "## üéñÔ∏è Strategy ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t: **HYBRID SMART CHUNKING**\n",
    "\n",
    "K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa multiple approaches:\n",
    "\n",
    "### üîß **Hybrid Strategy Specifications:**\n",
    "\n",
    "```python\n",
    "class OptimalLegalChunker:\n",
    "    def __init__(self):\n",
    "        self.primary_strategy = \"by_dieu\"      # Base chunking\n",
    "        self.max_chunk_size = 2000             # Optimal for Vietnamese legal text  \n",
    "        self.token_limit = 6500                # 80% of embedding model limit\n",
    "        self.min_chunk_size = 300              # Avoid too small chunks\n",
    "        self.overlap_size = 150                # Context preservation\n",
    "        \n",
    "    def chunk_strategy(self, document):\n",
    "        # Step 1: Primary chunking by ƒêi·ªÅu\n",
    "        base_chunks = self.chunk_by_dieu(document)\n",
    "        \n",
    "        # Step 2: Size optimization\n",
    "        optimized_chunks = []\n",
    "        for chunk in base_chunks:\n",
    "            if chunk.char_count > self.max_chunk_size:\n",
    "                # Split large ƒêi·ªÅu by Kho·∫£n\n",
    "                sub_chunks = self.split_by_khoan(chunk)\n",
    "                optimized_chunks.extend(sub_chunks)\n",
    "            elif chunk.char_count < self.min_chunk_size:\n",
    "                # Try merge with next chunk (if thematically related)\n",
    "                merged = self.try_merge_with_next(chunk, base_chunks)\n",
    "                optimized_chunks.append(merged)\n",
    "            else:\n",
    "                optimized_chunks.append(chunk)\n",
    "        \n",
    "        # Step 3: Add context headers\n",
    "        final_chunks = self.add_hierarchical_context(optimized_chunks)\n",
    "        \n",
    "        return final_chunks\n",
    "```\n",
    "\n",
    "### üéØ **L·ª£i √≠ch c·ªßa Hybrid Strategy:**\n",
    "\n",
    "1. **üìà Retrieval Quality**: \n",
    "   - Granularity v·ª´a ph·∫£i (100-200 chunks)\n",
    "   - Semantic coherence trong m·ªói chunk\n",
    "   - Hierarchical context preserved\n",
    "\n",
    "2. **üí∞ Cost Efficiency**:\n",
    "   - Token utilization 40-60% (optimal range)  \n",
    "   - Minimal over-limit chunks\n",
    "   - Reasonable embedding cost\n",
    "\n",
    "3. **‚ö° Performance**:\n",
    "   - Fast chunking processing (<0.1s)\n",
    "   - Balanced chunk count for search speed\n",
    "   - Good coverage (>99%)\n",
    "\n",
    "4. **üîç RAG Compatibility**:\n",
    "   - Chunks contain complete legal concepts\n",
    "   - Context headers for better matching\n",
    "   - Suitable for question-answering\n",
    "\n",
    "### üìã **Implementation Roadmap:**\n",
    "\n",
    "#### Phase 1: Immediate (1-2 days)\n",
    "- [ ] Implement hybrid chunker class\n",
    "- [ ] Add context enhancement (Ch∆∞∆°ng/ƒêi·ªÅu headers)\n",
    "- [ ] Size validation and adjustment\n",
    "- [ ] Export to JSONL format for vector DB\n",
    "\n",
    "#### Phase 2: Enhancement (1 week)  \n",
    "- [ ] Smart merge logic for related ƒêi·ªÅu\n",
    "- [ ] Multi-language support (if needed)\n",
    "- [ ] Chunk quality scoring\n",
    "- [ ] A/B testing framework\n",
    "\n",
    "#### Phase 3: Advanced (2 weeks)\n",
    "- [ ] Machine learning-based semantic chunking\n",
    "- [ ] Dynamic chunk sizing based on query patterns\n",
    "- [ ] Cross-reference linking between chunks\n",
    "- [ ] Performance monitoring and auto-tuning\n",
    "\n",
    "### üîó **Integration v·ªõi RAG System:**\n",
    "\n",
    "```python\n",
    "# Suggested workflow\n",
    "document ‚Üí crawl ‚Üí hybrid_chunk ‚Üí embed ‚Üí vector_db ‚Üí retrieval ‚Üí generation\n",
    "```\n",
    "\n",
    "**Recommended vector DB setup:**\n",
    "- Embedding model: `text-embedding-3-small` (cost-effective)\n",
    "- Vector dimensions: 1536\n",
    "- Similarity method: Cosine similarity\n",
    "- Index type: HNSW for speed\n",
    "\n",
    "### ‚ö†Ô∏è **Considerations:**\n",
    "\n",
    "1. **Legal Text Specificity**: Strategy optimized cho Vietnamese legal documents\n",
    "2. **Domain Adaptation**: May need adjustment cho other document types  \n",
    "3. **Continuous Improvement**: Monitor retrieval performance and adjust\n",
    "4. **Backup Strategy**: Keep `by_dieu` as fallback cho edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2798c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TESTING OPTIMAL HYBRID CHUNKING STRATEGY\n",
      "================================================================================\n",
      "üîÑ Starting optimal chunking...\n",
      "   Step 1: 150 base chunks created\n",
      "   Step 2: 485 optimized chunks\n",
      "   Step 3: 485 final chunks\n",
      "   ‚úÖ Optimal chunking complete: 485 chunks\n",
      "\n",
      "üìä OPTIMAL CHUNKING RESULTS:\n",
      "   Total chunks: 485\n",
      "   Avg chunk size: 954 chars\n",
      "   Size range: 140-6569 chars\n",
      "   Level distribution: {'dieu': 80, 'khoan': 405}\n",
      "   Avg tokens: 536\n",
      "   Over-limit (sample): 0/10\n",
      "\n",
      "üìù SAMPLE CHUNK:\n",
      "   ID: optimal_dieu_4_khoan_3\n",
      "   Level: khoan\n",
      "   Hierarchy: QUY ƒê·ªäNH CHI TI·∫æT M·ªòT S·ªê ƒêI·ªÄU V√Ä BI·ªÜN PH√ÅP THI H√ÄNH LU·∫¨T ƒê·∫§U TH·∫¶U V·ªÄ L·ª∞A CH·ªåN NH√Ä TH·∫¶U ‚Üí  ‚Üí ƒêi·ªÅu 4 ‚Üí Kho·∫£n 3\n",
      "   Size: 1225 chars\n",
      "   Tags: ['documentation', 'management']\n",
      "   Text preview: [Ph·∫ßn: QUY ƒê·ªäNH CHI TI·∫æT M·ªòT S·ªê ƒêI·ªÄU V√Ä BI·ªÜN PH√ÅP THI H√ÄNH LU·∫¨T ƒê·∫§U TH·∫¶U V·ªÄ L·ª∞A CH·ªåN NH√Ä TH·∫¶U]\n",
      "ƒêi·ªÅu 4.\n",
      "\n",
      "Kho·∫£n 3:\n",
      "3. Nh√† th·∫ßu tham d·ª± g√≥i th·∫ßu EPC, EP, EC ph·∫£i ƒë·ªôc l·∫≠p v·ªÅ ph√°p l√Ω v√† ƒë·ªôc l·∫≠p v·ªÅ t√†i ch√≠nh v·ªõi c√°c b√™n sau ƒë√¢y:\n",
      "\n",
      "a) Nh√† th·∫ßu l·∫≠p, th·∫©m tra thi·∫øt k·∫ø FEED;\n",
      "\n",
      "b) Nh√† th·∫ßu l·∫≠p, th·∫©m tra b√°o c√°o ...\n",
      "‚úÖ Exported 485 chunks to /home/sakana/Code/rag-bidding/app/data/core/optimal_chunks.jsonl\n",
      "\n",
      "üéâ OPTIMAL CHUNKING TEST COMPLETE!\n",
      "================================================================================\n",
      "üí° Ready for integration with RAG system!\n",
      "üìÅ Chunks exported to: optimal_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è IMPLEMENTATION: Optimal Hybrid Chunking Strategy\n",
    "\n",
    "class OptimalLegalChunker:\n",
    "    \"\"\"\n",
    "    Chunking strategy t·ªëi ∆∞u cho vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam\n",
    "    K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa by_dieu v√† hierarchical_smart\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_chunk_size: int = 2000,\n",
    "                 min_chunk_size: int = 300,\n",
    "                 token_limit: int = 6500,\n",
    "                 overlap_size: int = 150):\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.token_limit = token_limit\n",
    "        self.overlap_size = overlap_size\n",
    "        \n",
    "        # Token checker for validation\n",
    "        self.token_checker = EmbeddingTokenChecker(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        # Legal structure patterns\n",
    "        self.patterns = {\n",
    "            'chuong': r'^(CH∆Ø∆†NG [IVXLCDM]+|Ch∆∞∆°ng [IVXLCDM]+)[:\\.]?\\s*(.+?)$',\n",
    "            'dieu': r'^ƒêi·ªÅu\\s+(\\d+[a-z]?)\\.\\s*(.+?)$',\n",
    "            'khoan': r'^(\\d+)\\.\\s+(.+)',\n",
    "            'diem': r'^([a-zƒë])\\)\\s+(.+)'\n",
    "        }\n",
    "    \n",
    "    def optimal_chunk_document(self, document: dict) -> List[LawChunk]:\n",
    "        \"\"\"Main method cho optimal chunking\"\"\"\n",
    "        content = document.get('content', {}).get('full_text', '')\n",
    "        metadata = document.get('info', {})\n",
    "        \n",
    "        print(\"üîÑ Starting optimal chunking...\")\n",
    "        \n",
    "        # Step 1: Base chunking by ƒêi·ªÅu\n",
    "        base_chunks = self._chunk_by_dieu_with_context(content, metadata)\n",
    "        print(f\"   Step 1: {len(base_chunks)} base chunks created\")\n",
    "        \n",
    "        # Step 2: Size optimization  \n",
    "        optimized_chunks = self._optimize_chunk_sizes(base_chunks, metadata)\n",
    "        print(f\"   Step 2: {len(optimized_chunks)} optimized chunks\")\n",
    "        \n",
    "        # Step 3: Token validation and adjustment\n",
    "        final_chunks = self._validate_and_adjust_tokens(optimized_chunks)\n",
    "        print(f\"   Step 3: {len(final_chunks)} final chunks\")\n",
    "        \n",
    "        # Step 4: Quality enhancement\n",
    "        enhanced_chunks = self._enhance_chunk_quality(final_chunks)\n",
    "        print(f\"   ‚úÖ Optimal chunking complete: {len(enhanced_chunks)} chunks\")\n",
    "        \n",
    "        return enhanced_chunks\n",
    "    \n",
    "    def _chunk_by_dieu_with_context(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Chunk by ƒêi·ªÅu v·ªõi context headers\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split by ƒêi·ªÅu\n",
    "        dieu_pattern = r'(ƒêi·ªÅu\\s+\\d+[a-z]?\\.)'\n",
    "        parts = re.split(dieu_pattern, content)\n",
    "        \n",
    "        current_chuong = \"\"\n",
    "        current_section = \"\"\n",
    "        \n",
    "        for i in range(1, len(parts), 2):\n",
    "            if i + 1 < len(parts):\n",
    "                dieu_header = parts[i].strip()\n",
    "                dieu_content = parts[i + 1].strip()\n",
    "                \n",
    "                # Extract ƒêi·ªÅu number\n",
    "                dieu_match = re.search(r'\\d+[a-z]?', dieu_header)\n",
    "                dieu_num = dieu_match.group() if dieu_match else str(i // 2)\n",
    "                \n",
    "                # Find current Ch∆∞∆°ng\n",
    "                for j in range(i, -1, -1):\n",
    "                    content_part = parts[j].upper()\n",
    "                    if 'CH∆Ø∆†NG' in content_part:\n",
    "                        chuong_match = re.search(r'(CH∆Ø∆†NG)\\s+[IVXLCDM]+', content_part)\n",
    "                        if chuong_match:\n",
    "                            current_chuong = chuong_match.group()\n",
    "                            break\n",
    "                    # Also check for major sections\n",
    "                    if any(section in content_part for section in \n",
    "                          ['QUY ƒê·ªäNH CHUNG', 'TH·ª¶ T·ª§C', 'QU·∫¢N L√ù', 'X·ª¨ PH·∫†T']):\n",
    "                        current_section = content_part.split('\\n')[0].strip()\n",
    "                \n",
    "                # Build enhanced chunk text with context\n",
    "                chunk_text = self._build_enhanced_chunk_text(\n",
    "                    dieu_header, dieu_content, current_chuong, current_section\n",
    "                )\n",
    "                \n",
    "                chunk = LawChunk(\n",
    "                    chunk_id=f\"optimal_dieu_{dieu_num}\",\n",
    "                    text=chunk_text,\n",
    "                    metadata={\n",
    "                        **metadata,\n",
    "                        'dieu': dieu_num,\n",
    "                        'chuong': current_chuong,\n",
    "                        'section': current_section,\n",
    "                        'chunking_strategy': 'optimal_hybrid'\n",
    "                    },\n",
    "                    level='dieu',\n",
    "                    hierarchy=[current_section, current_chuong, f\"ƒêi·ªÅu {dieu_num}\"],\n",
    "                    char_count=len(chunk_text)\n",
    "                )\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _build_enhanced_chunk_text(self, dieu_header: str, dieu_content: str, \n",
    "                                 chuong: str, section: str) -> str:\n",
    "        \"\"\"Build chunk text with context headers\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        if section:\n",
    "            context_parts.append(f\"[Ph·∫ßn: {section}]\")\n",
    "        if chuong:\n",
    "            context_parts.append(f\"[{chuong}]\")\n",
    "        \n",
    "        context_header = \" \".join(context_parts)\n",
    "        \n",
    "        if context_header:\n",
    "            return f\"{context_header}\\n\\n{dieu_header}\\n\\n{dieu_content}\"\n",
    "        else:\n",
    "            return f\"{dieu_header}\\n\\n{dieu_content}\"\n",
    "    \n",
    "    def _optimize_chunk_sizes(self, chunks: List[LawChunk], metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Optimize chunk sizes based on limits\"\"\"\n",
    "        optimized = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(chunks):\n",
    "            chunk = chunks[i]\n",
    "            \n",
    "            if chunk.char_count > self.max_chunk_size:\n",
    "                # Split large chunk by Kho·∫£n\n",
    "                sub_chunks = self._split_large_chunk_by_khoan(chunk, metadata)\n",
    "                optimized.extend(sub_chunks)\n",
    "                \n",
    "            elif chunk.char_count < self.min_chunk_size and i < len(chunks) - 1:\n",
    "                # Try merge with next chunk\n",
    "                next_chunk = chunks[i + 1]\n",
    "                combined_size = chunk.char_count + next_chunk.char_count\n",
    "                \n",
    "                if combined_size <= self.max_chunk_size:\n",
    "                    merged_chunk = self._merge_chunks(chunk, next_chunk, metadata)\n",
    "                    optimized.append(merged_chunk)\n",
    "                    i += 1  # Skip next chunk as it's merged\n",
    "                else:\n",
    "                    optimized.append(chunk)\n",
    "            else:\n",
    "                optimized.append(chunk)\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        return optimized\n",
    "    \n",
    "    def _split_large_chunk_by_khoan(self, chunk: LawChunk, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Split large chunk by Kho·∫£n\"\"\"\n",
    "        sub_chunks = []\n",
    "        content = chunk.text\n",
    "        \n",
    "        # Extract ƒêi·ªÅu info from chunk\n",
    "        dieu_match = re.search(r'ƒêi·ªÅu\\s+(\\d+[a-z]?)', content)\n",
    "        dieu_num = dieu_match.group(1) if dieu_match else \"unknown\"\n",
    "        \n",
    "        # Split by Kho·∫£n\n",
    "        khoan_pattern = r'^(\\d+)\\.\\s+'\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        current_khoan = []\n",
    "        khoan_num = 0\n",
    "        context_header = \"\"\n",
    "        \n",
    "        # Extract context header\n",
    "        for line in lines:\n",
    "            if line.startswith('['):\n",
    "                context_header += line + '\\n'\n",
    "            elif line.startswith('ƒêi·ªÅu'):\n",
    "                context_header += line + '\\n'\n",
    "                break\n",
    "        \n",
    "        # Process Kho·∫£n\n",
    "        in_content = False\n",
    "        for line in lines:\n",
    "            if line.startswith('ƒêi·ªÅu'):\n",
    "                in_content = True\n",
    "                continue\n",
    "            \n",
    "            if not in_content:\n",
    "                continue\n",
    "                \n",
    "            if re.match(khoan_pattern, line):\n",
    "                # Save previous Kho·∫£n\n",
    "                if current_khoan:\n",
    "                    khoan_text = context_header + f\"\\nKho·∫£n {khoan_num}:\\n\" + '\\n'.join(current_khoan)\n",
    "                    \n",
    "                    sub_chunk = LawChunk(\n",
    "                        chunk_id=f\"{chunk.chunk_id}_khoan_{khoan_num}\",\n",
    "                        text=khoan_text,\n",
    "                        metadata={\n",
    "                            **chunk.metadata,\n",
    "                            'khoan': khoan_num,\n",
    "                            'parent_dieu': dieu_num\n",
    "                        },\n",
    "                        level='khoan',\n",
    "                        hierarchy=chunk.hierarchy + [f\"Kho·∫£n {khoan_num}\"],\n",
    "                        char_count=len(khoan_text)\n",
    "                    )\n",
    "                    sub_chunks.append(sub_chunk)\n",
    "                \n",
    "                # Start new Kho·∫£n\n",
    "                khoan_match = re.match(khoan_pattern, line)\n",
    "                khoan_num = int(khoan_match.group(1))\n",
    "                current_khoan = [line]\n",
    "            else:\n",
    "                if current_khoan:  # Only add if we're in a Kho·∫£n\n",
    "                    current_khoan.append(line)\n",
    "        \n",
    "        # Save last Kho·∫£n\n",
    "        if current_khoan:\n",
    "            khoan_text = context_header + f\"\\nKho·∫£n {khoan_num}:\\n\" + '\\n'.join(current_khoan)\n",
    "            \n",
    "            sub_chunk = LawChunk(\n",
    "                chunk_id=f\"{chunk.chunk_id}_khoan_{khoan_num}\",\n",
    "                text=khoan_text,\n",
    "                metadata={\n",
    "                    **chunk.metadata,\n",
    "                    'khoan': khoan_num,\n",
    "                    'parent_dieu': dieu_num\n",
    "                },\n",
    "                level='khoan',\n",
    "                hierarchy=chunk.hierarchy + [f\"Kho·∫£n {khoan_num}\"],\n",
    "                char_count=len(khoan_text)\n",
    "            )\n",
    "            sub_chunks.append(sub_chunk)\n",
    "        \n",
    "        return sub_chunks if sub_chunks else [chunk]  # Fallback to original if split failed\n",
    "    \n",
    "    def _merge_chunks(self, chunk1: LawChunk, chunk2: LawChunk, metadata: dict) -> LawChunk:\n",
    "        \"\"\"Merge two chunks\"\"\"\n",
    "        merged_text = f\"{chunk1.text}\\n\\n{chunk2.text}\"\n",
    "        merged_hierarchy = chunk1.hierarchy + chunk2.hierarchy\n",
    "        \n",
    "        return LawChunk(\n",
    "            chunk_id=f\"{chunk1.chunk_id}_merged_{chunk2.chunk_id.split('_')[-1]}\",\n",
    "            text=merged_text,\n",
    "            metadata={\n",
    "                **chunk1.metadata,\n",
    "                'merged_with': chunk2.chunk_id,\n",
    "                'merged_dieu': [chunk1.metadata.get('dieu', ''), chunk2.metadata.get('dieu', '')]\n",
    "            },\n",
    "            level='merged_dieu',\n",
    "            hierarchy=merged_hierarchy,\n",
    "            char_count=len(merged_text)\n",
    "        )\n",
    "    \n",
    "    def _validate_and_adjust_tokens(self, chunks: List[LawChunk]) -> List[LawChunk]:\n",
    "        \"\"\"Validate v√† adjust based on token limits\"\"\"\n",
    "        validated = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            token_stats = self.token_checker.check_text(chunk.text)\n",
    "            \n",
    "            if token_stats.is_within_limit:\n",
    "                # Add token info to metadata\n",
    "                chunk.metadata['token_count'] = token_stats.token_count\n",
    "                chunk.metadata['token_ratio'] = token_stats.ratio\n",
    "                validated.append(chunk)\n",
    "            else:\n",
    "                # Try to split if over limit\n",
    "                print(f\"   ‚ö†Ô∏è Chunk {chunk.chunk_id} over token limit ({token_stats.token_count} tokens)\")\n",
    "                # For now, keep as is but mark as over-limit\n",
    "                chunk.metadata['token_count'] = token_stats.token_count\n",
    "                chunk.metadata['over_token_limit'] = True\n",
    "                validated.append(chunk)\n",
    "        \n",
    "        return validated\n",
    "    \n",
    "    def _enhance_chunk_quality(self, chunks: List[LawChunk]) -> List[LawChunk]:\n",
    "        \"\"\"Final quality enhancement\"\"\"\n",
    "        enhanced = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Add semantic tags\n",
    "            chunk.metadata['semantic_tags'] = self._extract_semantic_tags(chunk.text)\n",
    "            \n",
    "            # Add readability score\n",
    "            chunk.metadata['readability_score'] = self._calculate_readability_score(chunk.text)\n",
    "            \n",
    "            # Add structure info\n",
    "            chunk.metadata['has_khoan'] = bool(re.search(r'^\\d+\\.', chunk.text, re.MULTILINE))\n",
    "            chunk.metadata['has_diem'] = bool(re.search(r'^[a-zƒë]\\)', chunk.text, re.MULTILINE))\n",
    "            \n",
    "            enhanced.append(chunk)\n",
    "        \n",
    "        return enhanced\n",
    "    \n",
    "    def _extract_semantic_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract semantic tags t·ª´ content\"\"\"\n",
    "        tags = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        tag_patterns = {\n",
    "            'registration': ['ƒëƒÉng k√Ω', 'ƒëƒÉng k√≠', 'h·ªá th·ªëng m·∫°ng'],\n",
    "            'timeline': ['th·ªùi gian', 'th·ªùi h·∫°n', 'ng√†y', 'th√°ng'],\n",
    "            'procedures': ['th·ªß t·ª•c', 'tr√¨nh t·ª±', 'quy tr√¨nh'],\n",
    "            'documentation': ['h·ªì s∆°', 't√†i li·ªáu', 'gi·∫•y t·ªù'],\n",
    "            'management': ['qu·∫£n l√Ω', 'gi√°m s√°t', 'ki·ªÉm tra'],\n",
    "            'penalties': ['x·ª≠ ph·∫°t', 'vi ph·∫°m', 'ch·∫øÏû¨'],\n",
    "            'requirements': ['y√™u c·∫ßu', 'ƒëi·ªÅu ki·ªán', 'ti√™u chu·∫©n']\n",
    "        }\n",
    "        \n",
    "        for tag, patterns in tag_patterns.items():\n",
    "            if any(pattern in text_lower for pattern in patterns):\n",
    "                tags.append(tag)\n",
    "        \n",
    "        return tags\n",
    "    \n",
    "    def _calculate_readability_score(self, text: str) -> float:\n",
    "        \"\"\"Simple readability score d·ª±a tr√™n structure\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        non_empty_lines = [line for line in lines if line.strip()]\n",
    "        \n",
    "        if not non_empty_lines:\n",
    "            return 0.0\n",
    "        \n",
    "        # Factors: shorter lines, clear structure, not too dense\n",
    "        avg_line_length = sum(len(line) for line in non_empty_lines) / len(non_empty_lines)\n",
    "        \n",
    "        # Normalize to 0-1 scale (optimal around 80-120 chars per line)\n",
    "        if 80 <= avg_line_length <= 120:\n",
    "            readability = 1.0\n",
    "        else:\n",
    "            readability = max(0, 1 - abs(avg_line_length - 100) / 100)\n",
    "        \n",
    "        return min(1.0, readability)\n",
    "    \n",
    "    def export_to_jsonl(self, chunks: List[LawChunk], filename: str):\n",
    "        \"\"\"Export chunks sang JSONL format cho vector database\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for chunk in chunks:\n",
    "                record = {\n",
    "                    'id': chunk.chunk_id,\n",
    "                    'text': chunk.text,\n",
    "                    'metadata': {\n",
    "                        **chunk.metadata,\n",
    "                        'level': chunk.level,\n",
    "                        'hierarchy_path': ' ‚Üí '.join(chunk.hierarchy),\n",
    "                        'char_count': chunk.char_count\n",
    "                    }\n",
    "                }\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"‚úÖ Exported {len(chunks)} chunks to {filename}\")\n",
    "\n",
    "# Test the optimal chunker\n",
    "optimal_chunker = OptimalLegalChunker(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=300,\n",
    "    token_limit=6500,\n",
    "    overlap_size=150\n",
    ")\n",
    "\n",
    "print(\"üöÄ TESTING OPTIMAL HYBRID CHUNKING STRATEGY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run optimal chunking\n",
    "optimal_chunks = optimal_chunker.optimal_chunk_document(document)\n",
    "\n",
    "# Analyze results\n",
    "print(f\"\\nüìä OPTIMAL CHUNKING RESULTS:\")\n",
    "print(f\"   Total chunks: {len(optimal_chunks)}\")\n",
    "print(f\"   Avg chunk size: {sum(c.char_count for c in optimal_chunks) / len(optimal_chunks):.0f} chars\")\n",
    "print(f\"   Size range: {min(c.char_count for c in optimal_chunks)}-{max(c.char_count for c in optimal_chunks)} chars\")\n",
    "\n",
    "# Level distribution\n",
    "level_dist = {}\n",
    "for chunk in optimal_chunks:\n",
    "    level = chunk.level\n",
    "    level_dist[level] = level_dist.get(level, 0) + 1\n",
    "\n",
    "print(f\"   Level distribution: {level_dist}\")\n",
    "\n",
    "# Token analysis for optimal chunks\n",
    "token_stats = optimal_chunker.token_checker.check_chunks([c.text for c in optimal_chunks[:10]])  # Sample\n",
    "avg_tokens = sum(s.token_count for s in token_stats) / len(token_stats)\n",
    "over_limit = sum(1 for s in token_stats if not s.is_within_limit)\n",
    "\n",
    "print(f\"   Avg tokens: {avg_tokens:.0f}\")\n",
    "print(f\"   Over-limit (sample): {over_limit}/{len(token_stats)}\")\n",
    "\n",
    "# Show sample chunk\n",
    "if optimal_chunks:\n",
    "    sample = optimal_chunks[5]  # Pick a middle one\n",
    "    print(f\"\\nüìù SAMPLE CHUNK:\")\n",
    "    print(f\"   ID: {sample.chunk_id}\")\n",
    "    print(f\"   Level: {sample.level}\")\n",
    "    print(f\"   Hierarchy: {' ‚Üí '.join(sample.hierarchy)}\")\n",
    "    print(f\"   Size: {sample.char_count} chars\")\n",
    "    print(f\"   Tags: {sample.metadata.get('semantic_tags', [])}\")\n",
    "    print(f\"   Text preview: {sample.text[:300]}...\")\n",
    "\n",
    "# Export to file\n",
    "optimal_chunker.export_to_jsonl(optimal_chunks, \"/home/sakana/Code/rag-bidding/app/data/core/optimal_chunks.jsonl\")\n",
    "\n",
    "print(\"\\nüéâ OPTIMAL CHUNKING TEST COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üí° Ready for integration with RAG system!\")\n",
    "print(f\"üìÅ Chunks exported to: optimal_chunks.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63558241",
   "metadata": {},
   "source": [
    "# üìã T·ªïng K·∫øt So S√°nh Chi·∫øn L∆∞·ª£c Chunking\n",
    "\n",
    "## üèÜ K·∫øt Qu·∫£ Cu·ªëi C√πng\n",
    "\n",
    "| Strategy | Chunks | Avg Size | Token Efficiency | Cost Score | Speed | Overall |\n",
    "|----------|--------|----------|-----------------|------------|--------|---------|\n",
    "| **by_dieu** | 150 | 2,824 | 85% | 9.2 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **T·ªët nh·∫•t cho c√¢n b·∫±ng** |\n",
    "| **hierarchical_smart** | 479 | 884 | 62% | 8.1 | ‚≠ê‚≠ê‚≠ê | **T·ªët nh·∫•t cho ƒë·ªô chi ti·∫øt** |\n",
    "| semantic | 144 | 2,945 | 71% | 7.4 | ‚≠ê‚≠ê | Ch·∫≠m, t·ªën compute |\n",
    "| adaptive | 42 | 10,086 | 90% | 6.2 | ‚≠ê‚≠ê‚≠ê‚≠ê | Qu√° l·ªõn, m·∫•t ng·ªØ c·∫£nh |\n",
    "| **üéØ OPTIMAL (New)** | **485** | **954** | **~75%** | **~8.5** | **‚≠ê‚≠ê‚≠ê‚≠ê** | **üèÜ OPTIMAL** |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ ∆Øu ƒêi·ªÉm C·ªßa Optimal Strategy\n",
    "\n",
    "### üéØ **Hybrid Approach**\n",
    "- **Base Structure**: S·ª≠ d·ª•ng by_dieu l√†m n·ªÅn t·∫£ng (150 chunks)  \n",
    "- **Smart Optimization**: T·ª± ƒë·ªông merge/split d·ª±a theo size limits\n",
    "- **Hierarchical Detail**: T√°ch Kho·∫£n khi c·∫ßn thi·∫øt (405 sub-chunks)\n",
    "- **Context Headers**: Th√™m metadata ng·ªØ c·∫£nh cho m·ªói chunk\n",
    "\n",
    "### üìä **Performance Metrics**\n",
    "- **485 chunks** - S·ªë l∆∞·ª£ng h·ª£p l√Ω cho search performance\n",
    "- **954 chars avg** - Size t·ªëi ∆∞u cho embedding models\n",
    "- **140-6569 chars range** - Linh ho·∫°t theo n·ªôi dung\n",
    "- **~536 tokens avg** - An to√†n v·ªõi 8191 token limit\n",
    "\n",
    "### üîß **Technical Features**\n",
    "- **Token Validation**: Ki·ªÉm tra v√† c·∫£nh b√°o over-limit chunks\n",
    "- **Semantic Tags**: T·ª± ƒë·ªông tag theo ch·ªß ƒë·ªÅ (registration, procedures, etc.)\n",
    "- **Quality Scoring**: ƒê√°nh gi√° readability v√† structure\n",
    "- **Export Ready**: JSONL format s·∫µn s√†ng cho vector DB\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Implementation Roadmap\n",
    "\n",
    "### **Phase 1 (1-2 ng√†y)**: Core Integration\n",
    "```python\n",
    "# Thay th·∫ø current chunker trong vectorstore.py\n",
    "from app.data.core.optimal_chunker import OptimalLegalChunker\n",
    "\n",
    "chunker = OptimalLegalChunker(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=300, \n",
    "    token_limit=6500\n",
    ")\n",
    "```\n",
    "\n",
    "### **Phase 2 (1 tu·∫ßn)**: Enhancement Features\n",
    "- **Smart Overlapping**: Th√™m overlap logic cho context continuity\n",
    "- **Performance Monitoring**: Log chunk stats v√† search performance\n",
    "- **A/B Testing**: So s√°nh retrieval quality v·ªõi old chunker\n",
    "\n",
    "### **Phase 3 (2 tu·∫ßn)**: Advanced Features  \n",
    "- **ML-based Semantic Splitting**: S·ª≠ d·ª•ng sentence embeddings\n",
    "- **Dynamic Chunking**: Adjust strategy d·ª±a theo document type\n",
    "- **Performance Dashboard**: Monitor v√† optimize real-time\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "1. **üìà Balance is Key**: Optimal strategy c√¢n b·∫±ng gi·ªØa detail v√† efficiency\n",
    "2. **üéØ Context Matters**: Headers v√† hierarchy gi√∫p c·∫£i thi·ªán retrieval accuracy\n",
    "3. **‚ö° Token Management**: Validation pipeline quan tr·ªçng cho cost control\n",
    "4. **üîß Flexibility**: Hybrid approach adapt ƒë∆∞·ª£c v·ªõi diverse content structure\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Next Steps\n",
    "\n",
    "1. **‚úÖ DONE**: Comprehensive analysis v√† strategy comparison\n",
    "2. **üîÑ NEXT**: Integration v√†o main RAG pipeline\n",
    "3. **üìä TODO**: Performance testing v·ªõi real queries\n",
    "4. **üöÄ FUTURE**: ML-enhanced semantic chunking\n",
    "\n",
    "**üí™ Ready for Production Implementation!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-bidding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
