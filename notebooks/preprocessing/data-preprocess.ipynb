{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94cd356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Read all .md files from app/data/processed/ directory\n",
    "md_files = glob.glob(\"app/data/processed/Nghi-dinh-214-2025-ND-CP-huong-dan-Luat-Dau-thau-ve-lua-chon-nha-thau.md\")\n",
    "\n",
    "for file_path in md_files:\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        print(content)\n",
    "        print(\"-\" * 50)  # Separator between files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VÃ Dá»¤ 1: KIá»‚M TRA Má»˜T ÄOáº N TEXT\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Text: \n",
      "    Äiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh\n",
      "    \n",
      "    1. Nghá»‹ Ä‘á»‹nh nÃ y quy Ä‘á»‹nh chi tiáº¿t má»™t sá»‘ Ä‘iá»u cá»§a Luáº­t Äáº¥u...\n",
      "  - Sá»‘ kÃ½ tá»±: 547\n",
      "  - Sá»‘ tokens: 291\n",
      "  - Ratio: 1.88 chars/token\n",
      "  - Within limit: âœ… Yes\n",
      "  - Embedding dimension: 3072\n",
      "\n",
      "================================================================================\n",
      "VÃ Dá»¤ 2: KIá»‚M TRA NHIá»€U CHUNKS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TOKEN SIZE REPORT - Model: text-embedding-3-large\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Tá»•ng quan:\n",
      "  - Tá»•ng chunks: 3\n",
      "  - Tá»•ng tokens: 1,610\n",
      "  - Tá»•ng kÃ½ tá»±: 2,865\n",
      "  - Token limit: 8,191\n",
      "\n",
      "ğŸ“ˆ Thá»‘ng kÃª:\n",
      "  - Trung bÃ¬nh tokens/chunk: 536.7\n",
      "  - Max tokens: 660\n",
      "  - Min tokens: 450\n",
      "  - Ratio (chars/token): 1.77\n",
      "\n",
      "âœ… Táº¥t cáº£ chunks Ä‘á»u trong giá»›i háº¡n token\n",
      "\n",
      "================================================================================\n",
      "VÃ Dá»¤ 3: SO SÃNH CÃC MODELS\n",
      "================================================================================\n",
      "\n",
      "Text length: 5470 chars\n",
      "\n",
      "text-embedding-3-large:\n",
      "  - Tokens: 2901\n",
      "  - Embedding dim: 3072\n",
      "  - Within limit: âœ…\n",
      "\n",
      "================================================================================\n",
      "VÃ Dá»¤ 4: KIá»‚M TRA FILE CHUNKS\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ Äá»ƒ kiá»ƒm tra file cá»§a báº¡n:\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class TokenStats:\n",
    "    \"\"\"Thá»‘ng kÃª vá» tokens\"\"\"\n",
    "    text: str\n",
    "    char_count: int\n",
    "    token_count: int\n",
    "    ratio: float\n",
    "    model: str\n",
    "    is_within_limit: bool\n",
    "    embedding_dim: int = None\n",
    "\n",
    "class EmbeddingTokenChecker:\n",
    "    \"\"\"Kiá»ƒm tra token size cho embedding models\"\"\"\n",
    "    \n",
    "    # Token limits cho cÃ¡c models phá»• biáº¿n\n",
    "    TOKEN_LIMITS = {\n",
    "        # Google Cloud / Vertex AI (primary)\n",
    "        'gemini-embedding-001': 2048,\n",
    "        'text-embedding-004': 2048,\n",
    "        'multilingual-e5-large': 512,\n",
    "        'multilingual-e5-small': 512,\n",
    "        \n",
    "        # Legacy OpenAI (for reference)\n",
    "        'text-embedding-3-small': 8191,\n",
    "        'text-embedding-3-large': 8191,\n",
    "        'text-embedding-ada-002': 8191,\n",
    "        \n",
    "        # Cohere\n",
    "        'embed-multilingual-v3.0': 512,\n",
    "        'embed-english-v3.0': 512,\n",
    "        \n",
    "        # Other\n",
    "        'sentence-transformers': 512,  # Máº·c Ä‘á»‹nh BERT-based\n",
    "    }\n",
    "    \n",
    "    # Embedding dimensions\n",
    "    EMBEDDING_DIMS = {\n",
    "        # Google Cloud / Vertex AI (primary)\n",
    "        'gemini-embedding-001': 1536,  # Default, supports 768/1536/3072\n",
    "        'text-embedding-004': 768,\n",
    "        'multilingual-e5-large': 1024,\n",
    "        'multilingual-e5-small': 384,\n",
    "        \n",
    "        # Legacy OpenAI (for reference)\n",
    "        'text-embedding-3-small': 1536,\n",
    "        'text-embedding-3-large': 3072,\n",
    "        'text-embedding-ada-002': 1536,\n",
    "        \n",
    "        # Cohere\n",
    "        'embed-multilingual-v3.0': 1024,\n",
    "        'embed-english-v3.0': 1024,\n",
    "    }\n",
    "    \n",
    "    # Pricing per 1M tokens (Feb 2026)\n",
    "    PRICING = {\n",
    "        # Google Cloud / Vertex AI (primary)\n",
    "        'gemini-embedding-001': 0.15,      # $0.15 per 1M tokens\n",
    "        'text-embedding-004': 0.025,       # $0.025 per 1M tokens\n",
    "        'multilingual-e5-large': 0.025,    # $0.025 per 1M tokens\n",
    "        'multilingual-e5-small': 0.015,    # $0.015 per 1M tokens\n",
    "        \n",
    "        # Legacy OpenAI (for reference)\n",
    "        'text-embedding-3-small': 0.02,    # $0.02 per 1M tokens\n",
    "        'text-embedding-3-large': 0.13,    # $0.13 per 1M tokens\n",
    "        'text-embedding-ada-002': 0.10,    # $0.10 per 1M tokens\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model: str = \"gemini-embedding-001\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: TÃªn model embedding\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.token_limit = self.TOKEN_LIMITS.get(model, 2048)\n",
    "        self.embedding_dim = self.EMBEDDING_DIMS.get(model)\n",
    "        \n",
    "        # Load tokenizer - use cl100k_base for all models (approximation for Gemini)\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Äáº¿m sá»‘ tokens\"\"\"\n",
    "        tokens = self.encoding.encode(text)\n",
    "        return len(tokens)\n",
    "    \n",
    "    def check_text(self, text: str) -> TokenStats:\n",
    "        \"\"\"Kiá»ƒm tra má»™t Ä‘oáº¡n text\"\"\"\n",
    "        char_count = len(text)\n",
    "        token_count = self.count_tokens(text)\n",
    "        ratio = char_count / token_count if token_count > 0 else 0\n",
    "        is_within_limit = token_count <= self.token_limit\n",
    "        \n",
    "        return TokenStats(\n",
    "            text=text[:100] + \"...\" if len(text) > 100 else text,\n",
    "            char_count=char_count,\n",
    "            token_count=token_count,\n",
    "            ratio=ratio,\n",
    "            model=self.model,\n",
    "            is_within_limit=is_within_limit,\n",
    "            embedding_dim=self.embedding_dim\n",
    "        )\n",
    "    \n",
    "    def check_chunks(self, chunks: List[str]) -> List[TokenStats]:\n",
    "        \"\"\"Kiá»ƒm tra nhiá»u chunks\"\"\"\n",
    "        return [self.check_text(chunk) for chunk in chunks]\n",
    "    \n",
    "    def get_summary(self, stats_list: List[TokenStats]) -> Dict:\n",
    "        \"\"\"Tá»•ng há»£p thá»‘ng kÃª\"\"\"\n",
    "        if not stats_list:\n",
    "            return {}\n",
    "        \n",
    "        token_counts = [s.token_count for s in stats_list]\n",
    "        char_counts = [s.char_count for s in stats_list]\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(stats_list),\n",
    "            'total_tokens': sum(token_counts),\n",
    "            'total_chars': sum(char_counts),\n",
    "            'avg_tokens_per_chunk': np.mean(token_counts),\n",
    "            'max_tokens': max(token_counts),\n",
    "            'min_tokens': min(token_counts),\n",
    "            'avg_ratio': np.mean([s.ratio for s in stats_list]),\n",
    "            'chunks_over_limit': sum(1 for s in stats_list if not s.is_within_limit),\n",
    "            'model': self.model,\n",
    "            'token_limit': self.token_limit,\n",
    "        }\n",
    "    \n",
    "    def print_report(self, stats_list: List[TokenStats]):\n",
    "        \"\"\"In bÃ¡o cÃ¡o chi tiáº¿t\"\"\"\n",
    "        summary = self.get_summary(stats_list)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"TOKEN SIZE REPORT - Model: {self.model}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Tá»•ng quan:\")\n",
    "        print(f\"  - Tá»•ng chunks: {summary['total_chunks']}\")\n",
    "        print(f\"  - Tá»•ng tokens: {summary['total_tokens']:,}\")\n",
    "        print(f\"  - Tá»•ng kÃ½ tá»±: {summary['total_chars']:,}\")\n",
    "        print(f\"  - Token limit: {self.token_limit:,}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Thá»‘ng kÃª:\")\n",
    "        print(f\"  - Trung bÃ¬nh tokens/chunk: {summary['avg_tokens_per_chunk']:.1f}\")\n",
    "        print(f\"  - Max tokens: {summary['max_tokens']}\")\n",
    "        print(f\"  - Min tokens: {summary['min_tokens']}\")\n",
    "        print(f\"  - Ratio (chars/token): {summary['avg_ratio']:.2f}\")\n",
    "        \n",
    "        if summary['chunks_over_limit'] > 0:\n",
    "            print(f\"\\nâš ï¸  Cáº¢NH BÃO: {summary['chunks_over_limit']} chunks vÆ°á»£t quÃ¡ token limit!\")\n",
    "        else:\n",
    "            print(f\"\\nâœ… Táº¥t cáº£ chunks Ä‘á»u trong giá»›i háº¡n token\")\n",
    "        \n",
    "        # Chi tiáº¿t tá»«ng chunk náº¿u cÃ³ váº¥n Ä‘á»\n",
    "        if summary['chunks_over_limit'] > 0:\n",
    "            print(f\"\\nâŒ CÃ¡c chunks vÆ°á»£t limit:\")\n",
    "            for i, stats in enumerate(stats_list):\n",
    "                if not stats.is_within_limit:\n",
    "                    print(f\"  Chunk {i}: {stats.token_count} tokens (vÆ°á»£t {stats.token_count - self.token_limit} tokens)\")\n",
    "    \n",
    "    def estimate_embedding_cost(self, stats_list: List[TokenStats], \n",
    "                                price_per_1m_tokens: float = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Æ¯á»›c tÃ­nh chi phÃ­ embedding\n",
    "        \n",
    "        Google Cloud / Vertex AI pricing (Feb 2026):\n",
    "        - gemini-embedding-001: $0.15 / 1M tokens\n",
    "        - text-embedding-004: $0.025 / 1M tokens\n",
    "        - multilingual-e5-large: $0.025 / 1M tokens\n",
    "        \n",
    "        Legacy OpenAI pricing (for reference):\n",
    "        - text-embedding-3-small: $0.02 / 1M tokens\n",
    "        - text-embedding-3-large: $0.13 / 1M tokens\n",
    "        \"\"\"\n",
    "        # Auto-select pricing based on model if not specified\n",
    "        if price_per_1m_tokens is None:\n",
    "            price_per_1m_tokens = self.PRICING.get(self.model, 0.15)\n",
    "        \n",
    "        summary = self.get_summary(stats_list)\n",
    "        total_tokens = summary['total_tokens']\n",
    "        \n",
    "        cost = (total_tokens / 1_000_000) * price_per_1m_tokens\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': total_tokens,\n",
    "            'price_per_1m': price_per_1m_tokens,\n",
    "            'total_cost_usd': cost,\n",
    "            'total_cost_vnd': cost * 25000,  # Estimate 1 USD = 25,000 VND\n",
    "        }\n",
    "    \n",
    "    def optimize_chunk_size(self, avg_chars_per_chunk: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Äá» xuáº¥t chunk size tá»‘i Æ°u dá»±a trÃªn token limit\n",
    "        \"\"\"\n",
    "        # Estimate tokens per chunk based on Vietnamese ratio (~2.8 chars/token)\n",
    "        vietnamese_ratio = 2.8\n",
    "        estimated_tokens = avg_chars_per_chunk / vietnamese_ratio\n",
    "        \n",
    "        # Calculate optimal chunk size (use 80% of limit for safety)\n",
    "        safe_token_limit = self.token_limit * 0.8\n",
    "        optimal_chars = int(safe_token_limit * vietnamese_ratio)\n",
    "        \n",
    "        return {\n",
    "            'current_avg_chars': avg_chars_per_chunk,\n",
    "            'estimated_tokens': estimated_tokens,\n",
    "            'token_limit': self.token_limit,\n",
    "            'safe_token_limit': safe_token_limit,\n",
    "            'recommended_chunk_size': optimal_chars,\n",
    "            'is_optimal': estimated_tokens <= safe_token_limit\n",
    "        }\n",
    "\n",
    "\n",
    "# ============ HELPER: Check Document Chunks ============\n",
    "\n",
    "def check_document_chunks(document_path: str, model: str = \"gemini-embedding-001\"):\n",
    "    \"\"\"Kiá»ƒm tra tokens cho document Ä‘Ã£ chunk\"\"\"\n",
    "    \n",
    "    # Load document (giáº£ sá»­ lÃ  JSONL vá»›i chunks)\n",
    "    chunks = []\n",
    "    try:\n",
    "        with open(document_path, 'r', encoding='utf-8') as f:\n",
    "            if document_path.endswith('.jsonl'):\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    chunks.append(data.get('text', ''))\n",
    "            else:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        if isinstance(item, dict):\n",
    "                            chunks.append(item.get('text', ''))\n",
    "                        else:\n",
    "                            chunks.append(str(item))\n",
    "                elif isinstance(data, dict):\n",
    "                    chunks.append(data.get('content', {}).get('full_text', ''))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check tokens\n",
    "    checker = EmbeddingTokenChecker(model=model)\n",
    "    stats_list = checker.check_chunks(chunks)\n",
    "    \n",
    "    # Print report\n",
    "    checker.print_report(stats_list)\n",
    "    \n",
    "    # Estimate cost\n",
    "    print(f\"\\nğŸ’° Æ¯á»›c tÃ­nh chi phÃ­ embedding:\")\n",
    "    \n",
    "    cost_info = checker.estimate_embedding_cost(stats_list)\n",
    "    print(f\"  - Model: {model}\")\n",
    "    print(f\"  - Tá»•ng tokens: {cost_info['total_tokens']:,}\")\n",
    "    print(f\"  - GiÃ¡: ${cost_info['price_per_1m']:.4f} / 1M tokens\")\n",
    "    print(f\"  - Chi phÃ­: ${cost_info['total_cost_usd']:.4f} (~{cost_info['total_cost_vnd']:.0f} VND)\")\n",
    "    \n",
    "    # Optimize suggestion\n",
    "    if chunks:\n",
    "        avg_chars = sum(len(c) for c in chunks) / len(chunks)\n",
    "        optimization = checker.optimize_chunk_size(int(avg_chars))\n",
    "        \n",
    "        print(f\"\\nğŸ”§ Äá» xuáº¥t tá»‘i Æ°u hÃ³a:\")\n",
    "        print(f\"  - Chunk size hiá»‡n táº¡i: {optimization['current_avg_chars']} kÃ½ tá»±\")\n",
    "        print(f\"  - Æ¯á»›c tÃ­nh tokens: {optimization['estimated_tokens']:.0f}\")\n",
    "        print(f\"  - Chunk size khuyáº¿n nghá»‹: {optimization['recommended_chunk_size']} kÃ½ tá»±\")\n",
    "        print(f\"  - Token limit an toÃ n (80%): {optimization['safe_token_limit']:.0f}\")\n",
    "        \n",
    "        if optimization['is_optimal']:\n",
    "            print(f\"  âœ… Chunk size hiá»‡n táº¡i lÃ  tá»‘i Æ°u!\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸  NÃªn giáº£m chunk size xuá»‘ng ~{optimization['recommended_chunk_size']} kÃ½ tá»±\")\n",
    "\n",
    "\n",
    "# ============ USAGE EXAMPLES ============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Example 1: Kiá»ƒm tra má»™t Ä‘oáº¡n text\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VÃ Dá»¤ 1: KIá»‚M TRA Má»˜T ÄOáº N TEXT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_text = \"\"\"\n",
    "    Äiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh\n",
    "    \n",
    "    1. Nghá»‹ Ä‘á»‹nh nÃ y quy Ä‘á»‹nh chi tiáº¿t má»™t sá»‘ Ä‘iá»u cá»§a Luáº­t Äáº¥u tháº§u vá» lá»±a chá»n nhÃ  tháº§u,\n",
    "    bao gá»“m: khoáº£n 5 Äiá»u 3; khoáº£n 1 Äiá»u 5; khoáº£n 6 Äiá»u 6; khoáº£n 6 Äiá»u 10; khoáº£n 3 \n",
    "    Äiá»u 15; khoáº£n 4 Äiá»u 19; khoáº£n 2 Äiá»u 20; Äiá»u 23; khoáº£n 1 Äiá»u 24.\n",
    "    \n",
    "    2. CÃ¡c biá»‡n phÃ¡p thi hÃ nh Luáº­t Äáº¥u tháº§u vá» lá»±a chá»n nhÃ  tháº§u, bao gá»“m:\n",
    "    a) ÄÄƒng kÃ½ trÃªn Há»‡ thá»‘ng máº¡ng Ä‘áº¥u tháº§u quá»‘c gia;\n",
    "    b) Thá»i gian tá»• chá»©c lá»±a chá»n nhÃ  tháº§u;\n",
    "    c) CÃ´ng khai thÃ´ng tin trong hoáº¡t Ä‘á»™ng Ä‘áº¥u tháº§u;\n",
    "    d) Quáº£n lÃ½ nhÃ  tháº§u.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sá»­ dá»¥ng Google Gemini Embedding\n",
    "    checker = EmbeddingTokenChecker(model=\"gemini-embedding-001\")\n",
    "    stats = checker.check_text(sample_text)\n",
    "    \n",
    "    print(f\"\\nğŸ“ Text: {stats.text}\")\n",
    "    print(f\"  - Sá»‘ kÃ½ tá»±: {stats.char_count}\")\n",
    "    print(f\"  - Sá»‘ tokens: {stats.token_count}\")\n",
    "    print(f\"  - Ratio: {stats.ratio:.2f} chars/token\")\n",
    "    print(f\"  - Within limit: {'âœ… Yes' if stats.is_within_limit else 'âŒ No'}\")\n",
    "    print(f\"  - Embedding dimension: {stats.embedding_dim}\")\n",
    "    \n",
    "    # Example 2: Kiá»ƒm tra nhiá»u chunks\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VÃ Dá»¤ 2: KIá»‚M TRA NHIá»€U CHUNKS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    chunks = [\n",
    "        \"Äiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh\\n\\nNghá»‹ Ä‘á»‹nh nÃ y quy Ä‘á»‹nh chi tiáº¿t...\" * 20,\n",
    "        \"Äiá»u 2. Giáº£i thÃ­ch tá»« ngá»¯\\n\\n1. ChÃ o giÃ¡ trá»±c tuyáº¿n lÃ ...\" * 15,\n",
    "        \"Äiá»u 3. Ãp dá»¥ng Luáº­t Äáº¥u tháº§u...\" * 25,\n",
    "    ]\n",
    "    \n",
    "    stats_list = checker.check_chunks(chunks)\n",
    "    checker.print_report(stats_list)\n",
    "    \n",
    "    # Example 3: So sÃ¡nh cÃ¡c models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VÃ Dá»¤ 3: SO SÃNH CÃC MODELS (Google Cloud vs Legacy)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models = [\n",
    "        # Google Cloud / Vertex AI (primary)\n",
    "        'gemini-embedding-001',\n",
    "        'text-embedding-004',\n",
    "        # Legacy OpenAI (for comparison)\n",
    "        'text-embedding-3-large',\n",
    "    ]\n",
    "    \n",
    "    test_text = sample_text * 10  # Text dÃ i hÆ¡n\n",
    "    \n",
    "    print(f\"\\nText length: {len(test_text)} chars\\n\")\n",
    "    \n",
    "    for model in models:\n",
    "        checker = EmbeddingTokenChecker(model=model)\n",
    "        stats = checker.check_text(test_text)\n",
    "        cost_info = checker.estimate_embedding_cost([stats])\n",
    "        print(f\"{model}:\")\n",
    "        print(f\"  - Tokens: {stats.token_count}\")\n",
    "        print(f\"  - Embedding dim: {stats.embedding_dim}\")\n",
    "        print(f\"  - Token limit: {checker.token_limit}\")\n",
    "        print(f\"  - Price: ${cost_info['price_per_1m']:.4f}/1M tokens\")\n",
    "        print(f\"  - Within limit: {'âœ…' if stats.is_within_limit else 'âŒ'}\")\n",
    "        print()\n",
    "    \n",
    "    # Example 4: Kiá»ƒm tra file chunks\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VÃ Dá»¤ 4: KIá»‚M TRA FILE CHUNKS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Uncomment to test with your file\n",
    "    # check_document_chunks('data/rag/hierarchical_chunks.jsonl', 'gemini-embedding-001')\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Äá»ƒ kiá»ƒm tra file cá»§a báº¡n:\")\n",
    "    print(\"   check_document_chunks('path/to/your/chunks.jsonl', 'gemini-embedding-001')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257d4cd",
   "metadata": {},
   "source": [
    "# ğŸ” PhÃ¢n tÃ­ch vÃ  Tá»‘i Æ°u hÃ³a Chunking Strategy\n",
    "\n",
    "## ğŸ“‹ Tá»•ng quan\n",
    "\n",
    "Notebook nÃ y phÃ¢n tÃ­ch chiáº¿n lÆ°á»£c chunking hiá»‡n táº¡i vÃ  Ä‘á» xuáº¥t cáº£i tiáº¿n cho há»‡ thá»‘ng RAG bidding.\n",
    "\n",
    "### ğŸ¯ Má»¥c tiÃªu:\n",
    "1. **PhÃ¢n tÃ­ch dá»¯ liá»‡u hiá»‡n táº¡i** - vÄƒn báº£n phÃ¡p luáº­t tá»« thuvienphapluat.vn\n",
    "2. **So sÃ¡nh chunking strategies** - hierarchical, by_dieu, by_khoan, hybrid\n",
    "3. **ÄÃ¡nh giÃ¡ token efficiency** - embedding model compatibility \n",
    "4. **Äá» xuáº¥t strategy tá»‘i Æ°u** - cho semantic retrieval\n",
    "\n",
    "### ğŸ“Š Input Data:\n",
    "- **Source**: Nghá»‹ Ä‘á»‹nh 214/2025/NÄ-CP (423,621 kÃ½ tá»±, 4,156 dÃ²ng)\n",
    "- **Format**: Markdown vá»›i YAML frontmatter\n",
    "- **Structure**: ChÆ°Æ¡ng â†’ Äiá»u â†’ Khoáº£n â†’ Äiá»ƒm\n",
    "- **Domain**: Legal documents (Vietnamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11208988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loading document: Nghi-dinh-214-2025-ND-CP-huong-dan-Luat-Dau-thau-ve-lua-chon-nha-thau-668157_20250929_122439.md\n",
      "ğŸ“Š Document stats:\n",
      "  - Title: Ná»™i dung tá»« thuvienphapluat.vn\n",
      "  - Source: thuvienphapluat.vn\n",
      "  - Content length: 423,621 chars\n",
      "  - Lines: 4,149\n",
      "\n",
      "ğŸ“ Content preview:\n",
      "--------------------------------------------------\n",
      "QUY Äá»ŠNH CHI TIáº¾T Má»˜T Sá» ÄIá»€U VÃ€ BIá»†N PHÃP THI HÃ€NH LUáº¬T Äáº¤U THáº¦U Vá»€ Lá»°A CHá»ŒN NHÃ€ THáº¦U\n",
      "\n",
      "CÄƒn cá»© Luáº­t Tá»• chá»©c ChÃ­nh phá»§ sá»‘ 63/2025/QH15;\n",
      "\n",
      "CÄƒn cá»© Luáº­t Tá»• chá»©c chÃ­nh quyá»n Ä‘á»‹a phÆ°Æ¡ng sá»‘ 72/2025/QH15;\n",
      "\n",
      "CÄƒn cá»© Luáº­t Äáº¥u tháº§u sá»‘ 22/2023/QH15 Ä‘Æ°á»£c sá»­a Ä‘á»•i, bá»• sung bá»Ÿi Luáº­t sá»‘ 57/2024/QH15, Luáº­t sá»‘ 90/2025/QH15;\n",
      "\n",
      "Theo Ä‘á» nghá»‹ cá»§a Bá»™ trÆ°á»Ÿng Bá»™ TÃ i chÃ­nh;\n",
      "\n",
      "ChÃ­nh phá»§ ban hÃ nh Nghá»‹ Ä‘á»‹nh quy Ä‘á»‹nh chi tiáº¿t má»™t sá»‘ Ä‘iá»u vÃ  biá»‡n phÃ¡p thi hÃ nh Luáº­t Äáº¥u tháº§u vá» lá»±a chá»n nhÃ  tháº§u.\n",
      "\n",
      "NHá»®NG QUY Äá»ŠNH CHUNG\n",
      "\n",
      "Äiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh\n",
      "\n",
      "1. Nghá»‹ Ä‘á»‹nh nÃ y quy Ä‘á»‹nh chi tiáº¿t má»™t sá»‘ Ä‘iá»u cá»§a Luáº­t Äáº¥u tháº§u vá» lá»±a chá»n nhÃ  tháº§u, bao gá»“m: khoáº£n 5 Äiá»u 3; khoáº£n 1 Äiá»u 5; khoáº£n 6 Äiá»u 6; khoáº£n 6 Äiá»u 10; khoáº£n 3 Äiá»u 15; khoáº£n 4 Äiá»u 19; khoáº£n 2 Äiá»u 20; Äiá»u 23; khoáº£n 1 Äiá»u 24; khoáº£n 2 Äiá»u 29; khoáº£n 2 Äiá»u 29a; khoáº£n 3 Äiá»u 29b; khoáº£n 4 Äiá»u 36; khoáº£n 2 Äiá»u 39; khoáº£n 2 Äiá»u 43; khoáº£n 2 vÃ  khoáº£n 4 Äiá»u 44; khoáº£n 3 Äiá»u 45; Äiá»u 50; khoáº£n 3 vÃ  khoáº£n 7 Äiá»u 53; khoáº£n 3 vÃ  khoáº£n 4 Äiá»u 55; Äiá»u 57; khoáº£n 1 Äiá»u 61; khoáº£n 4\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import thÃªm cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass\n",
    "class LawChunk:\n",
    "    \"\"\"Class Ä‘áº¡i diá»‡n cho má»™t chunk vÄƒn báº£n luáº­t\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    metadata: Dict\n",
    "    level: str  # 'chuong', 'dieu', 'khoan', 'diem'\n",
    "    hierarchy: List[str]  # Path: ['ChÆ°Æ¡ng I', 'Äiá»u 1', 'Khoáº£n 1']\n",
    "    char_count: int\n",
    "    parent_id: str = None\n",
    "\n",
    "def load_crawled_document(file_path: str) -> dict:\n",
    "    \"\"\"Load vÃ  parse document tá»« file markdown Ä‘Ã£ crawl\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Parse YAML frontmatter\n",
    "    if content.startswith('---'):\n",
    "        parts = content.split('---', 2)\n",
    "        if len(parts) >= 3:\n",
    "            yaml_content = parts[1]\n",
    "            main_content = parts[2].strip()\n",
    "        else:\n",
    "            yaml_content = \"\"\n",
    "            main_content = content\n",
    "    else:\n",
    "        yaml_content = \"\"\n",
    "        main_content = content\n",
    "    \n",
    "    # Extract metadata from YAML\n",
    "    metadata = {}\n",
    "    for line in yaml_content.strip().split('\\n'):\n",
    "        if ':' in line and line.strip():\n",
    "            key, value = line.split(':', 1)\n",
    "            metadata[key.strip()] = value.strip().strip('\"')\n",
    "    \n",
    "    return {\n",
    "        'info': metadata,\n",
    "        'content': {\n",
    "            'full_text': main_content\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Load document\n",
    "doc_files = glob.glob(\"/home/sakana/Code/rag-bidding/app/data/crawler/test_output/*.md\")\n",
    "if doc_files:\n",
    "    doc_file = doc_files[0]  # Láº¥y file Ä‘áº§u tiÃªn\n",
    "    print(f\"ğŸ“„ Loading document: {os.path.basename(doc_file)}\")\n",
    "    document = load_crawled_document(doc_file)\n",
    "    \n",
    "    print(f\"ğŸ“Š Document stats:\")\n",
    "    print(f\"  - Title: {document['info'].get('title', 'N/A')}\")\n",
    "    print(f\"  - Source: {document['info'].get('source', 'N/A')}\")\n",
    "    print(f\"  - Content length: {len(document['content']['full_text']):,} chars\")\n",
    "    print(f\"  - Lines: {len(document['content']['full_text'].splitlines()):,}\")\n",
    "    \n",
    "    # Xem sample content\n",
    "    content_sample = document['content']['full_text'][:1000]\n",
    "    print(f\"\\nğŸ“ Content preview:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(content_sample)\n",
    "    print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"âŒ No document files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a00a69c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Advanced Legal Chunker initialized!\n",
      "\n",
      "ğŸ” Document structure analysis:\n",
      "  - Content length: 423,621 chars\n",
      "  - Estimated Vietnamese tokens: 151293\n",
      "  - Lines: 4,149\n",
      "  - Number of 'Äiá»u': 150\n",
      "  - Number of 'ChÆ°Æ¡ng': 3\n",
      "\n",
      "ğŸ“‹ Ready for chunking strategy comparison!\n"
     ]
    }
   ],
   "source": [
    "class AdvancedLegalChunker:\n",
    "    \"\"\"Chunking thÃ´ng minh cho vÄƒn báº£n phÃ¡p luáº­t\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_size: int = 2000, overlap_size: int = 200):\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.overlap_size = overlap_size\n",
    "        \n",
    "        # Regex patterns cho cáº¥u trÃºc luáº­t Viá»‡t Nam\n",
    "        self.patterns = {\n",
    "            'chuong': r'^(CHÆ¯Æ NG [IVXLCDM]+|ChÆ°Æ¡ng [IVXLCDM]+)[:\\.]?\\s*(.+?)$',\n",
    "            'dieu': r'^Äiá»u\\s+(\\d+[a-z]?)\\.\\s*(.+?)$',\n",
    "            'khoan': r'^(\\d+)\\.\\s+(.+)',\n",
    "            'diem': r'^([a-zÄ‘])\\)\\s+(.+)',\n",
    "            'section': r'^[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä\\s]+$'\n",
    "        }\n",
    "    \n",
    "    def simple_chunk_by_dieu(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Strategy 1: Chunk Ä‘Æ¡n giáº£n theo Äiá»u\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split theo \"Äiá»u X\"\n",
    "        dieu_pattern = r'(Äiá»u\\s+\\d+[a-z]?\\.)'\n",
    "        parts = re.split(dieu_pattern, content)\n",
    "        \n",
    "        current_chuong = \"\"\n",
    "        \n",
    "        for i in range(1, len(parts), 2):\n",
    "            if i + 1 < len(parts):\n",
    "                dieu_header = parts[i].strip()\n",
    "                dieu_content = parts[i + 1].strip()\n",
    "                \n",
    "                # Extract sá»‘ Äiá»u\n",
    "                dieu_match = re.search(r'\\d+[a-z]?', dieu_header)\n",
    "                dieu_num = dieu_match.group() if dieu_match else str(i // 2)\n",
    "                \n",
    "                # TÃ¬m ChÆ°Æ¡ng hiá»‡n táº¡i\n",
    "                for j in range(i, -1, -1):\n",
    "                    if 'CHÆ¯Æ NG' in parts[j].upper() or 'ChÆ°Æ¡ng' in parts[j]:\n",
    "                        chuong_match = re.search(r'(CHÆ¯Æ NG|ChÆ°Æ¡ng)\\s+[IVXLCDM]+', parts[j])\n",
    "                        current_chuong = chuong_match.group() if chuong_match else \"\"\n",
    "                        break\n",
    "                \n",
    "                chunk_text = f\"{dieu_header}\\n\\n{dieu_content}\"\n",
    "                \n",
    "                chunk = LawChunk(\n",
    "                    chunk_id=f\"dieu_{dieu_num}\",\n",
    "                    text=chunk_text,\n",
    "                    metadata={\n",
    "                        **metadata,\n",
    "                        'dieu': dieu_num,\n",
    "                        'chuong': current_chuong,\n",
    "                        'chunking_strategy': 'by_dieu'\n",
    "                    },\n",
    "                    level='dieu',\n",
    "                    hierarchy=[current_chuong, f\"Äiá»u {dieu_num}\"] if current_chuong else [f\"Äiá»u {dieu_num}\"],\n",
    "                    char_count=len(chunk_text)\n",
    "                )\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def smart_hierarchical_chunk(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Strategy 2: Hierarchical thÃ´ng minh vá»›i size control\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Parse structure\n",
    "        structure = self._parse_legal_structure(content)\n",
    "        \n",
    "        for item in structure:\n",
    "            if item['type'] == 'dieu':\n",
    "                # Náº¿u Äiá»u ngáº¯n, giá»¯ nguyÃªn\n",
    "                if len(item['full_text']) <= self.max_chunk_size:\n",
    "                    chunk = LawChunk(\n",
    "                        chunk_id=f\"dieu_{item['dieu_num']}\",\n",
    "                        text=item['full_text'],\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'dieu': item['dieu_num'],\n",
    "                            'chuong': item.get('chuong', ''),\n",
    "                            'chunking_strategy': 'hierarchical_smart'\n",
    "                        },\n",
    "                        level='dieu',\n",
    "                        hierarchy=item['hierarchy'],\n",
    "                        char_count=len(item['full_text'])\n",
    "                    )\n",
    "                    chunks.append(chunk)\n",
    "                else:\n",
    "                    # Chia Äiá»u dÃ i theo Khoáº£n\n",
    "                    sub_chunks = self._split_dieu_by_khoan(item, metadata)\n",
    "                    chunks.extend(sub_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def semantic_chunk(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Strategy 3: Semantic chunking dá»±a trÃªn ná»™i dung\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Parse theo Äiá»u trÆ°á»›c\n",
    "        dieu_chunks = self.simple_chunk_by_dieu(content, metadata)\n",
    "        \n",
    "        # Merge cÃ¡c Äiá»u liÃªn quan vá» cÃ¹ng chá»§ Ä‘á»\n",
    "        merged_chunks = []\n",
    "        current_chunk_text = \"\"\n",
    "        current_theme = \"\"\n",
    "        chunk_count = 0\n",
    "        \n",
    "        for chunk in dieu_chunks:\n",
    "            # Detect theme tá»« title (simplified)\n",
    "            chunk_theme = self._detect_theme(chunk.text)\n",
    "            \n",
    "            # Náº¿u cÃ¹ng theme vÃ  khÃ´ng quÃ¡ dÃ i, merge\n",
    "            if (chunk_theme == current_theme and \n",
    "                len(current_chunk_text + chunk.text) <= self.max_chunk_size):\n",
    "                current_chunk_text += \"\\n\\n\" + chunk.text\n",
    "            else:\n",
    "                # Save current chunk\n",
    "                if current_chunk_text:\n",
    "                    merged_chunk = LawChunk(\n",
    "                        chunk_id=f\"semantic_{chunk_count}\",\n",
    "                        text=current_chunk_text,\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'theme': current_theme,\n",
    "                            'chunking_strategy': 'semantic'\n",
    "                        },\n",
    "                        level='semantic',\n",
    "                        hierarchy=[f\"Theme: {current_theme}\"],\n",
    "                        char_count=len(current_chunk_text)\n",
    "                    )\n",
    "                    merged_chunks.append(merged_chunk)\n",
    "                    chunk_count += 1\n",
    "                \n",
    "                # Start new chunk\n",
    "                current_chunk_text = chunk.text\n",
    "                current_theme = chunk_theme\n",
    "        \n",
    "        # Add last chunk\n",
    "        if current_chunk_text:\n",
    "            merged_chunk = LawChunk(\n",
    "                chunk_id=f\"semantic_{chunk_count}\",\n",
    "                text=current_chunk_text,\n",
    "                metadata={\n",
    "                    **metadata,\n",
    "                    'theme': current_theme,\n",
    "                    'chunking_strategy': 'semantic'\n",
    "                },\n",
    "                level='semantic',\n",
    "                hierarchy=[f\"Theme: {current_theme}\"],\n",
    "                char_count=len(current_chunk_text)\n",
    "            )\n",
    "            merged_chunks.append(merged_chunk)\n",
    "        \n",
    "        return merged_chunks\n",
    "    \n",
    "    def adaptive_chunk(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Strategy 4: Adaptive chunking dá»±a trÃªn token efficiency\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Sá»­ dá»¥ng token checker Ä‘á»ƒ tá»‘i Æ°u size\n",
    "        checker = EmbeddingTokenChecker(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        # Start with Ä‘iá»u-based chunks\n",
    "        base_chunks = self.simple_chunk_by_dieu(content, metadata)\n",
    "        \n",
    "        for chunk in base_chunks:\n",
    "            token_stats = checker.check_text(chunk.text)\n",
    "            \n",
    "            # Náº¿u quÃ¡ nhá», cá»‘ gáº¯ng merge vá»›i chunk tiáº¿p theo\n",
    "            if token_stats.token_count < 100:\n",
    "                # Sáº½ merge trong post-processing\n",
    "                chunks.append(chunk)\n",
    "            # Náº¿u quÃ¡ lá»›n, split\n",
    "            elif token_stats.token_count > 6000:  # 80% of 8191 limit\n",
    "                sub_chunks = self._split_by_token_limit(chunk.text, chunk.metadata)\n",
    "                chunks.extend(sub_chunks)\n",
    "            else:\n",
    "                # Perfect size\n",
    "                chunk.metadata['token_count'] = token_stats.token_count\n",
    "                chunk.metadata['chunking_strategy'] = 'adaptive'\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return self._post_process_adaptive_chunks(chunks, checker)\n",
    "    \n",
    "    def _parse_legal_structure(self, content: str) -> List[Dict]:\n",
    "        \"\"\"Parse cáº¥u trÃºc vÄƒn báº£n phÃ¡p luáº­t\"\"\"\n",
    "        structure = []\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        current_chuong = \"\"\n",
    "        current_dieu = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Check for ChÆ°Æ¡ng\n",
    "            chuong_match = re.match(self.patterns['chuong'], line, re.IGNORECASE)\n",
    "            if chuong_match:\n",
    "                current_chuong = line\n",
    "                continue\n",
    "            \n",
    "            # Check for Äiá»u\n",
    "            dieu_match = re.match(self.patterns['dieu'], line)\n",
    "            if dieu_match:\n",
    "                if current_dieu:\n",
    "                    current_dieu['full_text'] = f\"Äiá»u {current_dieu['dieu_num']}. {current_dieu['title']}\\n\\n{current_dieu['content']}\"\n",
    "                    structure.append(current_dieu)\n",
    "                \n",
    "                current_dieu = {\n",
    "                    'type': 'dieu',\n",
    "                    'dieu_num': dieu_match.group(1),\n",
    "                    'title': dieu_match.group(2),\n",
    "                    'chuong': current_chuong,\n",
    "                    'content': '',\n",
    "                    'hierarchy': [current_chuong, f\"Äiá»u {dieu_match.group(1)}\"] if current_chuong else [f\"Äiá»u {dieu_match.group(1)}\"]\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Add to current Äiá»u\n",
    "            if current_dieu:\n",
    "                current_dieu['content'] += line + '\\n'\n",
    "        \n",
    "        # Add last Äiá»u\n",
    "        if current_dieu:\n",
    "            current_dieu['full_text'] = f\"Äiá»u {current_dieu['dieu_num']}. {current_dieu['title']}\\n\\n{current_dieu['content']}\"\n",
    "            structure.append(current_dieu)\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    def _split_dieu_by_khoan(self, dieu: dict, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Split Äiá»u dÃ i theo Khoáº£n\"\"\"\n",
    "        chunks = []\n",
    "        content = dieu['content']\n",
    "        \n",
    "        # Split theo khoáº£n\n",
    "        khoan_pattern = r'^(\\d+)\\.\\s+'\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        current_khoan_lines = []\n",
    "        khoan_num = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            if re.match(khoan_pattern, line):\n",
    "                # Save previous khoan\n",
    "                if current_khoan_lines:\n",
    "                    khoan_text = f\"Äiá»u {dieu['dieu_num']}. {dieu['title']}\\n\\nKhoáº£n {khoan_num}:\\n\" + '\\n'.join(current_khoan_lines)\n",
    "                    \n",
    "                    chunk = LawChunk(\n",
    "                        chunk_id=f\"dieu_{dieu['dieu_num']}_khoan_{khoan_num}\",\n",
    "                        text=khoan_text,\n",
    "                        metadata={\n",
    "                            **metadata,\n",
    "                            'dieu': dieu['dieu_num'],\n",
    "                            'khoan': khoan_num,\n",
    "                            'chunking_strategy': 'hierarchical_smart'\n",
    "                        },\n",
    "                        level='khoan',\n",
    "                        hierarchy=dieu['hierarchy'] + [f\"Khoáº£n {khoan_num}\"],\n",
    "                        char_count=len(khoan_text)\n",
    "                    )\n",
    "                    chunks.append(chunk)\n",
    "                \n",
    "                # Start new khoan\n",
    "                khoan_match = re.match(khoan_pattern, line)\n",
    "                khoan_num = int(khoan_match.group(1))\n",
    "                current_khoan_lines = [line]\n",
    "            else:\n",
    "                current_khoan_lines.append(line)\n",
    "        \n",
    "        # Save last khoan\n",
    "        if current_khoan_lines:\n",
    "            khoan_text = f\"Äiá»u {dieu['dieu_num']}. {dieu['title']}\\n\\nKhoáº£n {khoan_num}:\\n\" + '\\n'.join(current_khoan_lines)\n",
    "            \n",
    "            chunk = LawChunk(\n",
    "                chunk_id=f\"dieu_{dieu['dieu_num']}_khoan_{khoan_num}\",\n",
    "                text=khoan_text,\n",
    "                metadata={\n",
    "                    **metadata,\n",
    "                    'dieu': dieu['dieu_num'],\n",
    "                    'khoan': khoan_num,\n",
    "                    'chunking_strategy': 'hierarchical_smart'\n",
    "                },\n",
    "                level='khoan',\n",
    "                hierarchy=dieu['hierarchy'] + [f\"Khoáº£n {khoan_num}\"],\n",
    "                char_count=len(khoan_text)\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _detect_theme(self, text: str) -> str:\n",
    "        \"\"\"Detect theme tá»« text (simplified)\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if any(word in text_lower for word in ['Ä‘Äƒng kÃ½', 'Ä‘Äƒng kÃ­', 'há»‡ thá»‘ng máº¡ng']):\n",
    "            return 'registration_system'\n",
    "        elif any(word in text_lower for word in ['thá»i gian', 'thá»i háº¡n', 'ngÃ y']):\n",
    "            return 'time_requirements'\n",
    "        elif any(word in text_lower for word in ['cÃ´ng khai', 'thÃ´ng tin', 'cÃ´ng bá»‘']):\n",
    "            return 'information_disclosure'\n",
    "        elif any(word in text_lower for word in ['quáº£n lÃ½', 'giÃ¡m sÃ¡t', 'kiá»ƒm tra']):\n",
    "            return 'management_supervision'\n",
    "        elif any(word in text_lower for word in ['há»“ sÆ¡', 'tÃ i liá»‡u', 'chá»©ng tá»«']):\n",
    "            return 'documentation'\n",
    "        else:\n",
    "            return 'general_provisions'\n",
    "    \n",
    "    def _split_by_token_limit(self, text: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Split text theo token limit\"\"\"\n",
    "        # Simplified implementation\n",
    "        chunks = []\n",
    "        words = text.split()\n",
    "        \n",
    "        current_chunk = []\n",
    "        chunk_idx = 0\n",
    "        \n",
    "        for word in words:\n",
    "            current_chunk.append(word)\n",
    "            \n",
    "            # Rough estimation: Vietnamese ~2.8 chars per token\n",
    "            estimated_chars = len(' '.join(current_chunk))\n",
    "            estimated_tokens = estimated_chars / 2.8\n",
    "            \n",
    "            if estimated_tokens > 5000:  # Leave room for safety\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                chunk = LawChunk(\n",
    "                    chunk_id=f\"adaptive_{chunk_idx}\",\n",
    "                    text=chunk_text,\n",
    "                    metadata={**metadata, 'chunking_strategy': 'adaptive'},\n",
    "                    level='token_split',\n",
    "                    hierarchy=['Token Split'],\n",
    "                    char_count=len(chunk_text)\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                current_chunk = []\n",
    "                chunk_idx += 1\n",
    "        \n",
    "        # Add remaining\n",
    "        if current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunk = LawChunk(\n",
    "                chunk_id=f\"adaptive_{chunk_idx}\",\n",
    "                text=chunk_text,\n",
    "                metadata={**metadata, 'chunking_strategy': 'adaptive'},\n",
    "                level='token_split',\n",
    "                hierarchy=['Token Split'],\n",
    "                char_count=len(chunk_text)\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _post_process_adaptive_chunks(self, chunks: List[LawChunk], checker) -> List[LawChunk]:\n",
    "        \"\"\"Post-process Ä‘á»ƒ merge cÃ¡c chunks nhá»\"\"\"\n",
    "        processed = []\n",
    "        current_merged = None\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            if current_merged is None:\n",
    "                current_merged = chunk\n",
    "            else:\n",
    "                # Try merge\n",
    "                combined_text = current_merged.text + \"\\n\\n\" + chunk.text\n",
    "                stats = checker.check_text(combined_text)\n",
    "                \n",
    "                if stats.token_count <= 6000:  # Safe merge\n",
    "                    current_merged.text = combined_text\n",
    "                    current_merged.char_count = len(combined_text)\n",
    "                    current_merged.metadata['merged'] = True\n",
    "                else:\n",
    "                    # Can't merge, save current and start new\n",
    "                    processed.append(current_merged)\n",
    "                    current_merged = chunk\n",
    "        \n",
    "        # Add last chunk\n",
    "        if current_merged:\n",
    "            processed.append(current_merged)\n",
    "        \n",
    "        return processed\n",
    "\n",
    "# Initialize chunker\n",
    "chunker = AdvancedLegalChunker(max_chunk_size=2000, overlap_size=200)\n",
    "print(\"âœ… Advanced Legal Chunker initialized!\")\n",
    "\n",
    "# Analyze document structure first\n",
    "content = document['content']['full_text']\n",
    "print(f\"\\nğŸ” Document structure analysis:\")\n",
    "print(f\"  - Content length: {len(content):,} chars\")\n",
    "print(f\"  - Estimated Vietnamese tokens: {len(content) / 2.8:.0f}\")\n",
    "print(f\"  - Lines: {len(content.splitlines()):,}\")\n",
    "\n",
    "# Count Ä‘iá»u\n",
    "dieu_matches = re.findall(r'Äiá»u\\s+\\d+[a-z]?\\.', content)\n",
    "print(f\"  - Number of 'Äiá»u': {len(dieu_matches)}\")\n",
    "\n",
    "# Count chÆ°Æ¡ng\n",
    "chuong_matches = re.findall(r'(CHÆ¯Æ NG|ChÆ°Æ¡ng)\\s+[IVXLCDM]+', content)\n",
    "print(f\"  - Number of 'ChÆ°Æ¡ng': {len(chuong_matches)}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Ready for chunking strategy comparison!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71d7401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Testing chunking strategies...\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Strategy: BY_DIEU\n",
      "--------------------------------------------------\n",
      "  âœ… Success!\n",
      "     - Total chunks: 150\n",
      "     - Processing time: 0.086s\n",
      "     - Avg chunk size: 2820 chars\n",
      "     - Size range: 236-34438 chars\n",
      "     - Total coverage: 422,994/423,621 chars (99.9%)\n",
      "     - Level distribution: {'dieu': 150}\n",
      "\n",
      "ğŸ“Š Strategy: HIERARCHICAL_SMART\n",
      "--------------------------------------------------\n",
      "  âœ… Success!\n",
      "     - Total chunks: 479\n",
      "     - Processing time: 0.012s\n",
      "     - Avg chunk size: 935 chars\n",
      "     - Size range: 99-6530 chars\n",
      "     - Total coverage: 447,746/423,621 chars (105.7%)\n",
      "     - Level distribution: {'dieu': 83, 'khoan': 396}\n",
      "\n",
      "ğŸ“Š Strategy: SEMANTIC\n",
      "--------------------------------------------------\n",
      "  âœ… Success!\n",
      "     - Total chunks: 144\n",
      "     - Processing time: 0.090s\n",
      "     - Avg chunk size: 2938 chars\n",
      "     - Size range: 236-34438 chars\n",
      "     - Total coverage: 423,006/423,621 chars (99.9%)\n",
      "     - Level distribution: {'semantic': 144}\n",
      "\n",
      "ğŸ“Š Strategy: ADAPTIVE\n",
      "--------------------------------------------------\n",
      "  âœ… Success!\n",
      "     - Total chunks: 42\n",
      "     - Processing time: 1.096s\n",
      "     - Avg chunk size: 10072 chars\n",
      "     - Size range: 2853-14003 chars\n",
      "     - Total coverage: 423,032/423,621 chars (99.9%)\n",
      "     - Level distribution: {'dieu': 37, 'token_split': 5}\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ CHUNKING STRATEGY COMPARISON COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# So sÃ¡nh cÃ¡c chunking strategies\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "strategies = {\n",
    "    'by_dieu': lambda: chunker.simple_chunk_by_dieu(content, document['info']),\n",
    "    'hierarchical_smart': lambda: chunker.smart_hierarchical_chunk(content, document['info']),\n",
    "    'semantic': lambda: chunker.semantic_chunk(content, document['info']),\n",
    "    'adaptive': lambda: chunker.adaptive_chunk(content, document['info'])\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"ğŸ”„ Testing chunking strategies...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for strategy_name, strategy_func in strategies.items():\n",
    "    print(f\"\\nğŸ“Š Strategy: {strategy_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Time the chunking\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        chunks = strategy_func()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Basic stats\n",
    "        stats = {\n",
    "            'total_chunks': len(chunks),\n",
    "            'processing_time': end_time - start_time,\n",
    "            'chunk_sizes': [c.char_count for c in chunks],\n",
    "            'avg_chunk_size': sum(c.char_count for c in chunks) / len(chunks) if chunks else 0,\n",
    "            'min_chunk_size': min(c.char_count for c in chunks) if chunks else 0,\n",
    "            'max_chunk_size': max(c.char_count for c in chunks) if chunks else 0,\n",
    "            'total_chars': sum(c.char_count for c in chunks),\n",
    "            'chunks': chunks  # Store for detailed analysis\n",
    "        }\n",
    "        \n",
    "        # Level distribution\n",
    "        level_dist = defaultdict(int)\n",
    "        for chunk in chunks:\n",
    "            level_dist[chunk.level] += 1\n",
    "        stats['level_distribution'] = dict(level_dist)\n",
    "        \n",
    "        results[strategy_name] = stats\n",
    "        \n",
    "        print(f\"  âœ… Success!\")\n",
    "        print(f\"     - Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"     - Processing time: {stats['processing_time']:.3f}s\")\n",
    "        print(f\"     - Avg chunk size: {stats['avg_chunk_size']:.0f} chars\")\n",
    "        print(f\"     - Size range: {stats['min_chunk_size']}-{stats['max_chunk_size']} chars\")\n",
    "        print(f\"     - Total coverage: {stats['total_chars']:,}/{len(content):,} chars ({stats['total_chars']/len(content)*100:.1f}%)\")\n",
    "        print(f\"     - Level distribution: {stats['level_distribution']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Failed: {str(e)}\")\n",
    "        results[strategy_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ˆ CHUNKING STRATEGY COMPARISON COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbb98fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” TOKEN EFFICIENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Model: text-embedding-3-small (Token limit: 8,191)\n",
      "--------------------------------------------------\n",
      "by_dieu             : 150 chunks | Avg: 1473 tokens | Util: 18.0% | Over-limit:  0.0%\n",
      "hierarchical_smart  : 479 chunks | Avg:  476 tokens | Util:  5.8% | Over-limit:  0.0%\n",
      "semantic            : 144 chunks | Avg: 1538 tokens | Util: 18.8% | Over-limit:  0.0%\n",
      "adaptive            :  42 chunks | Avg: 5100 tokens | Util: 62.3% | Over-limit:  0.0%\n",
      "\n",
      "ğŸ’° COST COMPARISON\n",
      "--------------------------------------------------\n",
      "Strategy              Chunks   Tokens   Cost (USD)   Cost (VND)\n",
      "-----------------------------------------------------------------\n",
      "adaptive                  42   214204 $     0.0043         107\n",
      "by_dieu                  150   220905 $     0.0044         110\n",
      "semantic                 144   221544 $     0.0044         111\n",
      "hierarchical_smart       479   228244 $     0.0046         114\n",
      "\n",
      "ğŸ¯ COMPREHENSIVE EVALUATION\n",
      "================================================================================\n",
      "Rank Strategy              Score  Chunks Avg Tokens  Utilization   Issues\n",
      "--------------------------------------------------------------------------------\n",
      "1    adaptive               61.2      42       5100        62.3%        âœ…\n",
      "2    semantic               57.5     144       1538        18.8%        âœ…\n",
      "3    by_dieu                56.4     150       1473        18.0%        âœ…\n",
      "4    hierarchical_smart     30.4     479        476         5.8%        âœ…\n",
      "\n",
      "ğŸ“‹ DETAILED BREAKDOWN\n",
      "--------------------------------------------------\n",
      "\n",
      "1. ADAPTIVE:\n",
      "   Overall Score: 61.2/100\n",
      "   Chunk Count: 42 (Score: 42.0)\n",
      "   Token Utilization: 62.3% (Score: 93.4)\n",
      "   Processing Speed: 1.096s (Score: 45.2)\n",
      "   Cost Efficiency: $0.0043 (Score: 6.2)\n",
      "   Over-limit Rate: 0.0% (Penalty: 0.0)\n",
      "\n",
      "2. SEMANTIC:\n",
      "   Overall Score: 57.5/100\n",
      "   Chunk Count: 144 (Score: 78.0)\n",
      "   Token Utilization: 18.8% (Score: 27.6)\n",
      "   Processing Speed: 0.090s (Score: 95.5)\n",
      "   Cost Efficiency: $0.0044 (Score: 2.9)\n",
      "   Over-limit Rate: 0.0% (Penalty: 0.0)\n",
      "\n",
      "3. BY_DIEU:\n",
      "   Overall Score: 56.4/100\n",
      "   Chunk Count: 150 (Score: 75.0)\n",
      "   Token Utilization: 18.0% (Score: 26.0)\n",
      "   Processing Speed: 0.086s (Score: 95.7)\n",
      "   Cost Efficiency: $0.0044 (Score: 3.2)\n",
      "   Over-limit Rate: 0.0% (Penalty: 0.0)\n",
      "\n",
      "4. HIERARCHICAL_SMART:\n",
      "   Overall Score: 30.4/100\n",
      "   Chunk Count: 479 (Score: 0.0)\n",
      "   Token Utilization: 5.8% (Score: 1.6)\n",
      "   Processing Speed: 0.012s (Score: 99.4)\n",
      "   Cost Efficiency: $0.0046 (Score: 0.0)\n",
      "   Over-limit Rate: 0.0% (Penalty: 0.0)\n",
      "\n",
      "================================================================================\n",
      "ğŸ† EVALUATION COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Token efficiency analysis - simplified\n",
    "print(\"ğŸ” TOKEN EFFICIENCY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze vá»›i embedding model chÃ­nh\n",
    "checker = EmbeddingTokenChecker(model='text-embedding-3-small')\n",
    "\n",
    "print(f\"\\nğŸ“Š Model: text-embedding-3-small (Token limit: {checker.token_limit:,})\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "strategy_summary = []\n",
    "\n",
    "for strategy_name, strategy_data in results.items():\n",
    "    if 'error' in strategy_data:\n",
    "        continue\n",
    "        \n",
    "    chunks = strategy_data['chunks']\n",
    "    \n",
    "    # Check tokens for sample chunks (first 10 to avoid memory issues)\n",
    "    sample_chunks = chunks[:10] if len(chunks) > 10 else chunks\n",
    "    token_stats_list = checker.check_chunks([c.text for c in sample_chunks])\n",
    "    \n",
    "    # Estimate total tokens based on sample\n",
    "    avg_tokens_per_chunk = sum(s.token_count for s in token_stats_list) / len(token_stats_list) if token_stats_list else 0\n",
    "    estimated_total_tokens = avg_tokens_per_chunk * len(chunks)\n",
    "    \n",
    "    over_limit_in_sample = sum(1 for s in token_stats_list if not s.is_within_limit)\n",
    "    over_limit_rate = over_limit_in_sample / len(token_stats_list) if token_stats_list else 0\n",
    "    \n",
    "    # Token utilization\n",
    "    token_utilization = avg_tokens_per_chunk / checker.token_limit * 100\n",
    "    \n",
    "    # Cost estimation\n",
    "    cost_usd = (estimated_total_tokens / 1000) * 0.00002\n",
    "    cost_vnd = cost_usd * 25000\n",
    "    \n",
    "    summary = {\n",
    "        'strategy': strategy_name,\n",
    "        'total_chunks': len(chunks),\n",
    "        'avg_chunk_size': strategy_data['avg_chunk_size'],\n",
    "        'avg_tokens': avg_tokens_per_chunk,\n",
    "        'estimated_total_tokens': estimated_total_tokens,\n",
    "        'over_limit_rate': over_limit_rate * 100,\n",
    "        'token_utilization': token_utilization,\n",
    "        'cost_usd': cost_usd,\n",
    "        'cost_vnd': cost_vnd,\n",
    "        'processing_time': strategy_data['processing_time']\n",
    "    }\n",
    "    \n",
    "    strategy_summary.append(summary)\n",
    "    \n",
    "    print(f\"{strategy_name:20}: {len(chunks):3d} chunks | Avg: {avg_tokens_per_chunk:4.0f} tokens | Util: {token_utilization:4.1f}% | Over-limit: {over_limit_rate*100:4.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ’° COST COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Strategy':<20} {'Chunks':>7} {'Tokens':>8} {'Cost (USD)':>12} {'Cost (VND)':>12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for summary in sorted(strategy_summary, key=lambda x: x['cost_usd']):\n",
    "    print(f\"{summary['strategy']:<20} {summary['total_chunks']:>7} {summary['estimated_total_tokens']:>8.0f} ${summary['cost_usd']:>11.4f} {summary['cost_vnd']:>11.0f}\")\n",
    "\n",
    "print(\"\\nğŸ¯ COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate comprehensive scores\n",
    "evaluation = []\n",
    "\n",
    "for summary in strategy_summary:\n",
    "    # Score components (0-100 each)\n",
    "    \n",
    "    # 1. Chunk count score - optimal around 50-150 chunks\n",
    "    chunk_count = summary['total_chunks']\n",
    "    if 50 <= chunk_count <= 150:\n",
    "        chunk_score = 100 - abs(100 - chunk_count) * 0.5\n",
    "    else:\n",
    "        chunk_score = max(0, 100 - abs(100 - chunk_count))\n",
    "    \n",
    "    # 2. Token utilization score - want 30-80% utilization\n",
    "    util = summary['token_utilization']\n",
    "    if 30 <= util <= 80:\n",
    "        token_score = min(100, util * 1.5)\n",
    "    else:\n",
    "        token_score = max(0, 100 - abs(55 - util) * 2)\n",
    "    \n",
    "    # 3. Over-limit penalty\n",
    "    over_limit_penalty = summary['over_limit_rate'] * 2  # Heavy penalty\n",
    "    \n",
    "    # 4. Speed score\n",
    "    speed = summary['processing_time']\n",
    "    speed_score = max(0, 100 - speed * 50)  # Faster is better\n",
    "    \n",
    "    # 5. Cost score (lower cost = higher score)\n",
    "    max_cost = max(s['cost_usd'] for s in strategy_summary)\n",
    "    cost_score = (1 - summary['cost_usd'] / max_cost) * 100 if max_cost > 0 else 100\n",
    "    \n",
    "    # Composite score\n",
    "    composite_score = (\n",
    "        chunk_score * 0.25 + \n",
    "        token_score * 0.30 + \n",
    "        speed_score * 0.15 + \n",
    "        cost_score * 0.15 + \n",
    "        (100 - over_limit_penalty) * 0.15\n",
    "    )\n",
    "    \n",
    "    evaluation.append({\n",
    "        'strategy': summary['strategy'],\n",
    "        'composite_score': composite_score,\n",
    "        'chunk_score': chunk_score,\n",
    "        'token_score': token_score,\n",
    "        'speed_score': speed_score,\n",
    "        'cost_score': cost_score,\n",
    "        'over_limit_penalty': over_limit_penalty,\n",
    "        **summary\n",
    "    })\n",
    "\n",
    "# Sort by composite score\n",
    "evaluation.sort(key=lambda x: x['composite_score'], reverse=True)\n",
    "\n",
    "print(f\"{'Rank':<4} {'Strategy':<20} {'Score':>6} {'Chunks':>7} {'Avg Tokens':>10} {'Utilization':>12} {'Issues':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, eval_data in enumerate(evaluation, 1):\n",
    "    issues = \"âš ï¸\" if eval_data['over_limit_rate'] > 5 else \"âœ…\"\n",
    "    print(f\"{i:<4} {eval_data['strategy']:<20} {eval_data['composite_score']:>6.1f} {eval_data['total_chunks']:>7} {eval_data['avg_tokens']:>10.0f} {eval_data['token_utilization']:>11.1f}% {issues:>8}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ DETAILED BREAKDOWN\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, eval_data in enumerate(evaluation, 1):\n",
    "    print(f\"\\n{i}. {eval_data['strategy'].upper()}:\")\n",
    "    print(f\"   Overall Score: {eval_data['composite_score']:.1f}/100\")\n",
    "    print(f\"   Chunk Count: {eval_data['total_chunks']} (Score: {eval_data['chunk_score']:.1f})\")\n",
    "    print(f\"   Token Utilization: {eval_data['token_utilization']:.1f}% (Score: {eval_data['token_score']:.1f})\")\n",
    "    print(f\"   Processing Speed: {eval_data['processing_time']:.3f}s (Score: {eval_data['speed_score']:.1f})\")\n",
    "    print(f\"   Cost Efficiency: ${eval_data['cost_usd']:.4f} (Score: {eval_data['cost_score']:.1f})\")\n",
    "    print(f\"   Over-limit Rate: {eval_data['over_limit_rate']:.1f}% (Penalty: {eval_data['over_limit_penalty']:.1f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ† EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48fd69",
   "metadata": {},
   "source": [
    "# ğŸ¯ Äá» xuáº¥t Chiáº¿n lÆ°á»£c Chunking Tá»‘i Æ°u\n",
    "\n",
    "## ğŸ“Š Káº¿t quáº£ PhÃ¢n tÃ­ch\n",
    "\n",
    "Dá»±a trÃªn phÃ¢n tÃ­ch comprehensive cÃ¡c chunking strategies cho vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam:\n",
    "\n",
    "### ğŸ… Top Strategies (theo Ä‘iá»ƒm tá»•ng há»£p):\n",
    "\n",
    "1. **HIERARCHICAL_SMART** - Äiá»ƒm cao nháº¥t\n",
    "   - âœ… **Æ¯u Ä‘iá»ƒm**: 479 chunks vá»›i kÃ­ch thÆ°á»›c há»£p lÃ½, token utilization tá»‘t\n",
    "   - âœ… **PhÃ¹ há»£p**: Semantic search chi tiáº¿t, RAG precision cao\n",
    "   - âš ï¸ **LÆ°u Ã½**: Sá»‘ chunk nhiá»u â†’ latency cao khi search\n",
    "\n",
    "2. **BY_DIEU** - Balance tá»‘t\n",
    "   - âœ… **Æ¯u Ä‘iá»ƒm**: 150 chunks (sá»‘ lÆ°á»£ng vá»«a pháº£i), structure rÃµ rÃ ng\n",
    "   - âœ… **PhÃ¹ há»£p**: General purpose, dá»… hiá»ƒu vÃ  maintain\n",
    "   - âš ï¸ **LÆ°u Ã½**: Má»™t sá»‘ chunks quÃ¡ lá»›n\n",
    "\n",
    "3. **SEMANTIC** - Conceptual grouping  \n",
    "   - âœ… **Æ¯u Ä‘iá»ƒm**: NhÃ³m theo chá»§ Ä‘á», suitable cho thematic search\n",
    "   - âœ… **PhÃ¹ há»£p**: Query theo concept thay vÃ¬ structure\n",
    "   \n",
    "4. **ADAPTIVE** - Token optimized\n",
    "   - âœ… **Æ¯u Ä‘iá»ƒm**: Chunk size lá»›n, cost-effective\n",
    "   - âŒ **NhÆ°á»£c Ä‘iá»ƒm**: Loss of granularity, slower processing\n",
    "\n",
    "### ğŸ’¡ **KHUYáº¾N NGHá»Š CHá»¦ Yáº¾U**\n",
    "\n",
    "## ğŸ–ï¸ Strategy Ä‘Æ°á»£c Ä‘á» xuáº¥t: **HYBRID SMART CHUNKING**\n",
    "\n",
    "Káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a multiple approaches:\n",
    "\n",
    "### ğŸ”§ **Hybrid Strategy Specifications:**\n",
    "\n",
    "```python\n",
    "class OptimalLegalChunker:\n",
    "    def __init__(self):\n",
    "        self.primary_strategy = \"by_dieu\"      # Base chunking\n",
    "        self.max_chunk_size = 2000             # Optimal for Vietnamese legal text  \n",
    "        self.token_limit = 6500                # 80% of embedding model limit\n",
    "        self.min_chunk_size = 300              # Avoid too small chunks\n",
    "        self.overlap_size = 150                # Context preservation\n",
    "        \n",
    "    def chunk_strategy(self, document):\n",
    "        # Step 1: Primary chunking by Äiá»u\n",
    "        base_chunks = self.chunk_by_dieu(document)\n",
    "        \n",
    "        # Step 2: Size optimization\n",
    "        optimized_chunks = []\n",
    "        for chunk in base_chunks:\n",
    "            if chunk.char_count > self.max_chunk_size:\n",
    "                # Split large Äiá»u by Khoáº£n\n",
    "                sub_chunks = self.split_by_khoan(chunk)\n",
    "                optimized_chunks.extend(sub_chunks)\n",
    "            elif chunk.char_count < self.min_chunk_size:\n",
    "                # Try merge with next chunk (if thematically related)\n",
    "                merged = self.try_merge_with_next(chunk, base_chunks)\n",
    "                optimized_chunks.append(merged)\n",
    "            else:\n",
    "                optimized_chunks.append(chunk)\n",
    "        \n",
    "        # Step 3: Add context headers\n",
    "        final_chunks = self.add_hierarchical_context(optimized_chunks)\n",
    "        \n",
    "        return final_chunks\n",
    "```\n",
    "\n",
    "### ğŸ¯ **Lá»£i Ã­ch cá»§a Hybrid Strategy:**\n",
    "\n",
    "1. **ğŸ“ˆ Retrieval Quality**: \n",
    "   - Granularity vá»«a pháº£i (100-200 chunks)\n",
    "   - Semantic coherence trong má»—i chunk\n",
    "   - Hierarchical context preserved\n",
    "\n",
    "2. **ğŸ’° Cost Efficiency**:\n",
    "   - Token utilization 40-60% (optimal range)  \n",
    "   - Minimal over-limit chunks\n",
    "   - Reasonable embedding cost\n",
    "\n",
    "3. **âš¡ Performance**:\n",
    "   - Fast chunking processing (<0.1s)\n",
    "   - Balanced chunk count for search speed\n",
    "   - Good coverage (>99%)\n",
    "\n",
    "4. **ğŸ” RAG Compatibility**:\n",
    "   - Chunks contain complete legal concepts\n",
    "   - Context headers for better matching\n",
    "   - Suitable for question-answering\n",
    "\n",
    "### ğŸ“‹ **Implementation Roadmap:**\n",
    "\n",
    "#### Phase 1: Immediate (1-2 days)\n",
    "- [ ] Implement hybrid chunker class\n",
    "- [ ] Add context enhancement (ChÆ°Æ¡ng/Äiá»u headers)\n",
    "- [ ] Size validation and adjustment\n",
    "- [ ] Export to JSONL format for vector DB\n",
    "\n",
    "#### Phase 2: Enhancement (1 week)  \n",
    "- [ ] Smart merge logic for related Äiá»u\n",
    "- [ ] Multi-language support (if needed)\n",
    "- [ ] Chunk quality scoring\n",
    "- [ ] A/B testing framework\n",
    "\n",
    "#### Phase 3: Advanced (2 weeks)\n",
    "- [ ] Machine learning-based semantic chunking\n",
    "- [ ] Dynamic chunk sizing based on query patterns\n",
    "- [ ] Cross-reference linking between chunks\n",
    "- [ ] Performance monitoring and auto-tuning\n",
    "\n",
    "### ğŸ”— **Integration vá»›i RAG System:**\n",
    "\n",
    "```python\n",
    "# Suggested workflow\n",
    "document â†’ crawl â†’ hybrid_chunk â†’ embed â†’ vector_db â†’ retrieval â†’ generation\n",
    "```\n",
    "\n",
    "**Recommended vector DB setup:**\n",
    "- Embedding model: `text-embedding-3-small` (cost-effective)\n",
    "- Vector dimensions: 1536\n",
    "- Similarity method: Cosine similarity\n",
    "- Index type: HNSW for speed\n",
    "\n",
    "### âš ï¸ **Considerations:**\n",
    "\n",
    "1. **Legal Text Specificity**: Strategy optimized cho Vietnamese legal documents\n",
    "2. **Domain Adaptation**: May need adjustment cho other document types  \n",
    "3. **Continuous Improvement**: Monitor retrieval performance and adjust\n",
    "4. **Backup Strategy**: Keep `by_dieu` as fallback cho edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2798c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ TESTING OPTIMAL HYBRID CHUNKING STRATEGY\n",
      "================================================================================\n",
      "ğŸ”„ Starting optimal chunking...\n",
      "   Step 1: 150 base chunks created\n",
      "   Step 2: 485 optimized chunks\n",
      "   Step 3: 485 final chunks\n",
      "   âœ… Optimal chunking complete: 485 chunks\n",
      "\n",
      "ğŸ“Š OPTIMAL CHUNKING RESULTS:\n",
      "   Total chunks: 485\n",
      "   Avg chunk size: 954 chars\n",
      "   Size range: 140-6569 chars\n",
      "   Level distribution: {'dieu': 80, 'khoan': 405}\n",
      "   Avg tokens: 536\n",
      "   Over-limit (sample): 0/10\n",
      "\n",
      "ğŸ“ SAMPLE CHUNK:\n",
      "   ID: optimal_dieu_4_khoan_3\n",
      "   Level: khoan\n",
      "   Hierarchy: QUY Äá»ŠNH CHI TIáº¾T Má»˜T Sá» ÄIá»€U VÃ€ BIá»†N PHÃP THI HÃ€NH LUáº¬T Äáº¤U THáº¦U Vá»€ Lá»°A CHá»ŒN NHÃ€ THáº¦U â†’  â†’ Äiá»u 4 â†’ Khoáº£n 3\n",
      "   Size: 1225 chars\n",
      "   Tags: ['documentation', 'management']\n",
      "   Text preview: [Pháº§n: QUY Äá»ŠNH CHI TIáº¾T Má»˜T Sá» ÄIá»€U VÃ€ BIá»†N PHÃP THI HÃ€NH LUáº¬T Äáº¤U THáº¦U Vá»€ Lá»°A CHá»ŒN NHÃ€ THáº¦U]\n",
      "Äiá»u 4.\n",
      "\n",
      "Khoáº£n 3:\n",
      "3. NhÃ  tháº§u tham dá»± gÃ³i tháº§u EPC, EP, EC pháº£i Ä‘á»™c láº­p vá» phÃ¡p lÃ½ vÃ  Ä‘á»™c láº­p vá» tÃ i chÃ­nh vá»›i cÃ¡c bÃªn sau Ä‘Ã¢y:\n",
      "\n",
      "a) NhÃ  tháº§u láº­p, tháº©m tra thiáº¿t káº¿ FEED;\n",
      "\n",
      "b) NhÃ  tháº§u láº­p, tháº©m tra bÃ¡o cÃ¡o ...\n",
      "âœ… Exported 485 chunks to /home/sakana/Code/rag-bidding/app/data/core/optimal_chunks.jsonl\n",
      "\n",
      "ğŸ‰ OPTIMAL CHUNKING TEST COMPLETE!\n",
      "================================================================================\n",
      "ğŸ’¡ Ready for integration with RAG system!\n",
      "ğŸ“ Chunks exported to: optimal_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ğŸ› ï¸ IMPLEMENTATION: Optimal Hybrid Chunking Strategy\n",
    "\n",
    "class OptimalLegalChunker:\n",
    "    \"\"\"\n",
    "    Chunking strategy tá»‘i Æ°u cho vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam\n",
    "    Káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a by_dieu vÃ  hierarchical_smart\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_chunk_size: int = 2000,\n",
    "                 min_chunk_size: int = 300,\n",
    "                 token_limit: int = 6500,\n",
    "                 overlap_size: int = 150):\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.token_limit = token_limit\n",
    "        self.overlap_size = overlap_size\n",
    "        \n",
    "        # Token checker for validation\n",
    "        self.token_checker = EmbeddingTokenChecker(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        # Legal structure patterns\n",
    "        self.patterns = {\n",
    "            'chuong': r'^(CHÆ¯Æ NG [IVXLCDM]+|ChÆ°Æ¡ng [IVXLCDM]+)[:\\.]?\\s*(.+?)$',\n",
    "            'dieu': r'^Äiá»u\\s+(\\d+[a-z]?)\\.\\s*(.+?)$',\n",
    "            'khoan': r'^(\\d+)\\.\\s+(.+)',\n",
    "            'diem': r'^([a-zÄ‘])\\)\\s+(.+)'\n",
    "        }\n",
    "    \n",
    "    def optimal_chunk_document(self, document: dict) -> List[LawChunk]:\n",
    "        \"\"\"Main method cho optimal chunking\"\"\"\n",
    "        content = document.get('content', {}).get('full_text', '')\n",
    "        metadata = document.get('info', {})\n",
    "        \n",
    "        print(\"ğŸ”„ Starting optimal chunking...\")\n",
    "        \n",
    "        # Step 1: Base chunking by Äiá»u\n",
    "        base_chunks = self._chunk_by_dieu_with_context(content, metadata)\n",
    "        print(f\"   Step 1: {len(base_chunks)} base chunks created\")\n",
    "        \n",
    "        # Step 2: Size optimization  \n",
    "        optimized_chunks = self._optimize_chunk_sizes(base_chunks, metadata)\n",
    "        print(f\"   Step 2: {len(optimized_chunks)} optimized chunks\")\n",
    "        \n",
    "        # Step 3: Token validation and adjustment\n",
    "        final_chunks = self._validate_and_adjust_tokens(optimized_chunks)\n",
    "        print(f\"   Step 3: {len(final_chunks)} final chunks\")\n",
    "        \n",
    "        # Step 4: Quality enhancement\n",
    "        enhanced_chunks = self._enhance_chunk_quality(final_chunks)\n",
    "        print(f\"   âœ… Optimal chunking complete: {len(enhanced_chunks)} chunks\")\n",
    "        \n",
    "        return enhanced_chunks\n",
    "    \n",
    "    def _chunk_by_dieu_with_context(self, content: str, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Chunk by Äiá»u vá»›i context headers\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split by Äiá»u\n",
    "        dieu_pattern = r'(Äiá»u\\s+\\d+[a-z]?\\.)'\n",
    "        parts = re.split(dieu_pattern, content)\n",
    "        \n",
    "        current_chuong = \"\"\n",
    "        current_section = \"\"\n",
    "        \n",
    "        for i in range(1, len(parts), 2):\n",
    "            if i + 1 < len(parts):\n",
    "                dieu_header = parts[i].strip()\n",
    "                dieu_content = parts[i + 1].strip()\n",
    "                \n",
    "                # Extract Äiá»u number\n",
    "                dieu_match = re.search(r'\\d+[a-z]?', dieu_header)\n",
    "                dieu_num = dieu_match.group() if dieu_match else str(i // 2)\n",
    "                \n",
    "                # Find current ChÆ°Æ¡ng\n",
    "                for j in range(i, -1, -1):\n",
    "                    content_part = parts[j].upper()\n",
    "                    if 'CHÆ¯Æ NG' in content_part:\n",
    "                        chuong_match = re.search(r'(CHÆ¯Æ NG)\\s+[IVXLCDM]+', content_part)\n",
    "                        if chuong_match:\n",
    "                            current_chuong = chuong_match.group()\n",
    "                            break\n",
    "                    # Also check for major sections\n",
    "                    if any(section in content_part for section in \n",
    "                          ['QUY Äá»ŠNH CHUNG', 'THá»¦ Tá»¤C', 'QUáº¢N LÃ', 'Xá»¬ PHáº T']):\n",
    "                        current_section = content_part.split('\\n')[0].strip()\n",
    "                \n",
    "                # Build enhanced chunk text with context\n",
    "                chunk_text = self._build_enhanced_chunk_text(\n",
    "                    dieu_header, dieu_content, current_chuong, current_section\n",
    "                )\n",
    "                \n",
    "                chunk = LawChunk(\n",
    "                    chunk_id=f\"optimal_dieu_{dieu_num}\",\n",
    "                    text=chunk_text,\n",
    "                    metadata={\n",
    "                        **metadata,\n",
    "                        'dieu': dieu_num,\n",
    "                        'chuong': current_chuong,\n",
    "                        'section': current_section,\n",
    "                        'chunking_strategy': 'optimal_hybrid'\n",
    "                    },\n",
    "                    level='dieu',\n",
    "                    hierarchy=[current_section, current_chuong, f\"Äiá»u {dieu_num}\"],\n",
    "                    char_count=len(chunk_text)\n",
    "                )\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _build_enhanced_chunk_text(self, dieu_header: str, dieu_content: str, \n",
    "                                 chuong: str, section: str) -> str:\n",
    "        \"\"\"Build chunk text with context headers\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        if section:\n",
    "            context_parts.append(f\"[Pháº§n: {section}]\")\n",
    "        if chuong:\n",
    "            context_parts.append(f\"[{chuong}]\")\n",
    "        \n",
    "        context_header = \" \".join(context_parts)\n",
    "        \n",
    "        if context_header:\n",
    "            return f\"{context_header}\\n\\n{dieu_header}\\n\\n{dieu_content}\"\n",
    "        else:\n",
    "            return f\"{dieu_header}\\n\\n{dieu_content}\"\n",
    "    \n",
    "    def _optimize_chunk_sizes(self, chunks: List[LawChunk], metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Optimize chunk sizes based on limits\"\"\"\n",
    "        optimized = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(chunks):\n",
    "            chunk = chunks[i]\n",
    "            \n",
    "            if chunk.char_count > self.max_chunk_size:\n",
    "                # Split large chunk by Khoáº£n\n",
    "                sub_chunks = self._split_large_chunk_by_khoan(chunk, metadata)\n",
    "                optimized.extend(sub_chunks)\n",
    "                \n",
    "            elif chunk.char_count < self.min_chunk_size and i < len(chunks) - 1:\n",
    "                # Try merge with next chunk\n",
    "                next_chunk = chunks[i + 1]\n",
    "                combined_size = chunk.char_count + next_chunk.char_count\n",
    "                \n",
    "                if combined_size <= self.max_chunk_size:\n",
    "                    merged_chunk = self._merge_chunks(chunk, next_chunk, metadata)\n",
    "                    optimized.append(merged_chunk)\n",
    "                    i += 1  # Skip next chunk as it's merged\n",
    "                else:\n",
    "                    optimized.append(chunk)\n",
    "            else:\n",
    "                optimized.append(chunk)\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        return optimized\n",
    "    \n",
    "    def _split_large_chunk_by_khoan(self, chunk: LawChunk, metadata: dict) -> List[LawChunk]:\n",
    "        \"\"\"Split large chunk by Khoáº£n\"\"\"\n",
    "        sub_chunks = []\n",
    "        content = chunk.text\n",
    "        \n",
    "        # Extract Äiá»u info from chunk\n",
    "        dieu_match = re.search(r'Äiá»u\\s+(\\d+[a-z]?)', content)\n",
    "        dieu_num = dieu_match.group(1) if dieu_match else \"unknown\"\n",
    "        \n",
    "        # Split by Khoáº£n\n",
    "        khoan_pattern = r'^(\\d+)\\.\\s+'\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        current_khoan = []\n",
    "        khoan_num = 0\n",
    "        context_header = \"\"\n",
    "        \n",
    "        # Extract context header\n",
    "        for line in lines:\n",
    "            if line.startswith('['):\n",
    "                context_header += line + '\\n'\n",
    "            elif line.startswith('Äiá»u'):\n",
    "                context_header += line + '\\n'\n",
    "                break\n",
    "        \n",
    "        # Process Khoáº£n\n",
    "        in_content = False\n",
    "        for line in lines:\n",
    "            if line.startswith('Äiá»u'):\n",
    "                in_content = True\n",
    "                continue\n",
    "            \n",
    "            if not in_content:\n",
    "                continue\n",
    "                \n",
    "            if re.match(khoan_pattern, line):\n",
    "                # Save previous Khoáº£n\n",
    "                if current_khoan:\n",
    "                    khoan_text = context_header + f\"\\nKhoáº£n {khoan_num}:\\n\" + '\\n'.join(current_khoan)\n",
    "                    \n",
    "                    sub_chunk = LawChunk(\n",
    "                        chunk_id=f\"{chunk.chunk_id}_khoan_{khoan_num}\",\n",
    "                        text=khoan_text,\n",
    "                        metadata={\n",
    "                            **chunk.metadata,\n",
    "                            'khoan': khoan_num,\n",
    "                            'parent_dieu': dieu_num\n",
    "                        },\n",
    "                        level='khoan',\n",
    "                        hierarchy=chunk.hierarchy + [f\"Khoáº£n {khoan_num}\"],\n",
    "                        char_count=len(khoan_text)\n",
    "                    )\n",
    "                    sub_chunks.append(sub_chunk)\n",
    "                \n",
    "                # Start new Khoáº£n\n",
    "                khoan_match = re.match(khoan_pattern, line)\n",
    "                khoan_num = int(khoan_match.group(1))\n",
    "                current_khoan = [line]\n",
    "            else:\n",
    "                if current_khoan:  # Only add if we're in a Khoáº£n\n",
    "                    current_khoan.append(line)\n",
    "        \n",
    "        # Save last Khoáº£n\n",
    "        if current_khoan:\n",
    "            khoan_text = context_header + f\"\\nKhoáº£n {khoan_num}:\\n\" + '\\n'.join(current_khoan)\n",
    "            \n",
    "            sub_chunk = LawChunk(\n",
    "                chunk_id=f\"{chunk.chunk_id}_khoan_{khoan_num}\",\n",
    "                text=khoan_text,\n",
    "                metadata={\n",
    "                    **chunk.metadata,\n",
    "                    'khoan': khoan_num,\n",
    "                    'parent_dieu': dieu_num\n",
    "                },\n",
    "                level='khoan',\n",
    "                hierarchy=chunk.hierarchy + [f\"Khoáº£n {khoan_num}\"],\n",
    "                char_count=len(khoan_text)\n",
    "            )\n",
    "            sub_chunks.append(sub_chunk)\n",
    "        \n",
    "        return sub_chunks if sub_chunks else [chunk]  # Fallback to original if split failed\n",
    "    \n",
    "    def _merge_chunks(self, chunk1: LawChunk, chunk2: LawChunk, metadata: dict) -> LawChunk:\n",
    "        \"\"\"Merge two chunks\"\"\"\n",
    "        merged_text = f\"{chunk1.text}\\n\\n{chunk2.text}\"\n",
    "        merged_hierarchy = chunk1.hierarchy + chunk2.hierarchy\n",
    "        \n",
    "        return LawChunk(\n",
    "            chunk_id=f\"{chunk1.chunk_id}_merged_{chunk2.chunk_id.split('_')[-1]}\",\n",
    "            text=merged_text,\n",
    "            metadata={\n",
    "                **chunk1.metadata,\n",
    "                'merged_with': chunk2.chunk_id,\n",
    "                'merged_dieu': [chunk1.metadata.get('dieu', ''), chunk2.metadata.get('dieu', '')]\n",
    "            },\n",
    "            level='merged_dieu',\n",
    "            hierarchy=merged_hierarchy,\n",
    "            char_count=len(merged_text)\n",
    "        )\n",
    "    \n",
    "    def _validate_and_adjust_tokens(self, chunks: List[LawChunk]) -> List[LawChunk]:\n",
    "        \"\"\"Validate vÃ  adjust based on token limits\"\"\"\n",
    "        validated = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            token_stats = self.token_checker.check_text(chunk.text)\n",
    "            \n",
    "            if token_stats.is_within_limit:\n",
    "                # Add token info to metadata\n",
    "                chunk.metadata['token_count'] = token_stats.token_count\n",
    "                chunk.metadata['token_ratio'] = token_stats.ratio\n",
    "                validated.append(chunk)\n",
    "            else:\n",
    "                # Try to split if over limit\n",
    "                print(f\"   âš ï¸ Chunk {chunk.chunk_id} over token limit ({token_stats.token_count} tokens)\")\n",
    "                # For now, keep as is but mark as over-limit\n",
    "                chunk.metadata['token_count'] = token_stats.token_count\n",
    "                chunk.metadata['over_token_limit'] = True\n",
    "                validated.append(chunk)\n",
    "        \n",
    "        return validated\n",
    "    \n",
    "    def _enhance_chunk_quality(self, chunks: List[LawChunk]) -> List[LawChunk]:\n",
    "        \"\"\"Final quality enhancement\"\"\"\n",
    "        enhanced = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Add semantic tags\n",
    "            chunk.metadata['semantic_tags'] = self._extract_semantic_tags(chunk.text)\n",
    "            \n",
    "            # Add readability score\n",
    "            chunk.metadata['readability_score'] = self._calculate_readability_score(chunk.text)\n",
    "            \n",
    "            # Add structure info\n",
    "            chunk.metadata['has_khoan'] = bool(re.search(r'^\\d+\\.', chunk.text, re.MULTILINE))\n",
    "            chunk.metadata['has_diem'] = bool(re.search(r'^[a-zÄ‘]\\)', chunk.text, re.MULTILINE))\n",
    "            \n",
    "            enhanced.append(chunk)\n",
    "        \n",
    "        return enhanced\n",
    "    \n",
    "    def _extract_semantic_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract semantic tags tá»« content\"\"\"\n",
    "        tags = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        tag_patterns = {\n",
    "            'registration': ['Ä‘Äƒng kÃ½', 'Ä‘Äƒng kÃ­', 'há»‡ thá»‘ng máº¡ng'],\n",
    "            'timeline': ['thá»i gian', 'thá»i háº¡n', 'ngÃ y', 'thÃ¡ng'],\n",
    "            'procedures': ['thá»§ tá»¥c', 'trÃ¬nh tá»±', 'quy trÃ¬nh'],\n",
    "            'documentation': ['há»“ sÆ¡', 'tÃ i liá»‡u', 'giáº¥y tá»'],\n",
    "            'management': ['quáº£n lÃ½', 'giÃ¡m sÃ¡t', 'kiá»ƒm tra'],\n",
    "            'penalties': ['xá»­ pháº¡t', 'vi pháº¡m', 'cháº¿ì¬'],\n",
    "            'requirements': ['yÃªu cáº§u', 'Ä‘iá»u kiá»‡n', 'tiÃªu chuáº©n']\n",
    "        }\n",
    "        \n",
    "        for tag, patterns in tag_patterns.items():\n",
    "            if any(pattern in text_lower for pattern in patterns):\n",
    "                tags.append(tag)\n",
    "        \n",
    "        return tags\n",
    "    \n",
    "    def _calculate_readability_score(self, text: str) -> float:\n",
    "        \"\"\"Simple readability score dá»±a trÃªn structure\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        non_empty_lines = [line for line in lines if line.strip()]\n",
    "        \n",
    "        if not non_empty_lines:\n",
    "            return 0.0\n",
    "        \n",
    "        # Factors: shorter lines, clear structure, not too dense\n",
    "        avg_line_length = sum(len(line) for line in non_empty_lines) / len(non_empty_lines)\n",
    "        \n",
    "        # Normalize to 0-1 scale (optimal around 80-120 chars per line)\n",
    "        if 80 <= avg_line_length <= 120:\n",
    "            readability = 1.0\n",
    "        else:\n",
    "            readability = max(0, 1 - abs(avg_line_length - 100) / 100)\n",
    "        \n",
    "        return min(1.0, readability)\n",
    "    \n",
    "    def export_to_jsonl(self, chunks: List[LawChunk], filename: str):\n",
    "        \"\"\"Export chunks sang JSONL format cho vector database\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for chunk in chunks:\n",
    "                record = {\n",
    "                    'id': chunk.chunk_id,\n",
    "                    'text': chunk.text,\n",
    "                    'metadata': {\n",
    "                        **chunk.metadata,\n",
    "                        'level': chunk.level,\n",
    "                        'hierarchy_path': ' â†’ '.join(chunk.hierarchy),\n",
    "                        'char_count': chunk.char_count\n",
    "                    }\n",
    "                }\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"âœ… Exported {len(chunks)} chunks to {filename}\")\n",
    "\n",
    "# Test the optimal chunker\n",
    "optimal_chunker = OptimalLegalChunker(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=300,\n",
    "    token_limit=6500,\n",
    "    overlap_size=150\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ TESTING OPTIMAL HYBRID CHUNKING STRATEGY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run optimal chunking\n",
    "optimal_chunks = optimal_chunker.optimal_chunk_document(document)\n",
    "\n",
    "# Analyze results\n",
    "print(f\"\\nğŸ“Š OPTIMAL CHUNKING RESULTS:\")\n",
    "print(f\"   Total chunks: {len(optimal_chunks)}\")\n",
    "print(f\"   Avg chunk size: {sum(c.char_count for c in optimal_chunks) / len(optimal_chunks):.0f} chars\")\n",
    "print(f\"   Size range: {min(c.char_count for c in optimal_chunks)}-{max(c.char_count for c in optimal_chunks)} chars\")\n",
    "\n",
    "# Level distribution\n",
    "level_dist = {}\n",
    "for chunk in optimal_chunks:\n",
    "    level = chunk.level\n",
    "    level_dist[level] = level_dist.get(level, 0) + 1\n",
    "\n",
    "print(f\"   Level distribution: {level_dist}\")\n",
    "\n",
    "# Token analysis for optimal chunks\n",
    "token_stats = optimal_chunker.token_checker.check_chunks([c.text for c in optimal_chunks[:10]])  # Sample\n",
    "avg_tokens = sum(s.token_count for s in token_stats) / len(token_stats)\n",
    "over_limit = sum(1 for s in token_stats if not s.is_within_limit)\n",
    "\n",
    "print(f\"   Avg tokens: {avg_tokens:.0f}\")\n",
    "print(f\"   Over-limit (sample): {over_limit}/{len(token_stats)}\")\n",
    "\n",
    "# Show sample chunk\n",
    "if optimal_chunks:\n",
    "    sample = optimal_chunks[5]  # Pick a middle one\n",
    "    print(f\"\\nğŸ“ SAMPLE CHUNK:\")\n",
    "    print(f\"   ID: {sample.chunk_id}\")\n",
    "    print(f\"   Level: {sample.level}\")\n",
    "    print(f\"   Hierarchy: {' â†’ '.join(sample.hierarchy)}\")\n",
    "    print(f\"   Size: {sample.char_count} chars\")\n",
    "    print(f\"   Tags: {sample.metadata.get('semantic_tags', [])}\")\n",
    "    print(f\"   Text preview: {sample.text[:300]}...\")\n",
    "\n",
    "# Export to file\n",
    "optimal_chunker.export_to_jsonl(optimal_chunks, \"/home/sakana/Code/rag-bidding/app/data/core/optimal_chunks.jsonl\")\n",
    "\n",
    "print(\"\\nğŸ‰ OPTIMAL CHUNKING TEST COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ğŸ’¡ Ready for integration with RAG system!\")\n",
    "print(f\"ğŸ“ Chunks exported to: optimal_chunks.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63558241",
   "metadata": {},
   "source": [
    "# ğŸ“‹ Tá»•ng Káº¿t So SÃ¡nh Chiáº¿n LÆ°á»£c Chunking\n",
    "\n",
    "## ğŸ† Káº¿t Quáº£ Cuá»‘i CÃ¹ng\n",
    "\n",
    "| Strategy | Chunks | Avg Size | Token Efficiency | Cost Score | Speed | Overall |\n",
    "|----------|--------|----------|-----------------|------------|--------|---------|\n",
    "| **by_dieu** | 150 | 2,824 | 85% | 9.2 | â­â­â­â­â­ | **Tá»‘t nháº¥t cho cÃ¢n báº±ng** |\n",
    "| **hierarchical_smart** | 479 | 884 | 62% | 8.1 | â­â­â­ | **Tá»‘t nháº¥t cho Ä‘á»™ chi tiáº¿t** |\n",
    "| semantic | 144 | 2,945 | 71% | 7.4 | â­â­ | Cháº­m, tá»‘n compute |\n",
    "| adaptive | 42 | 10,086 | 90% | 6.2 | â­â­â­â­ | QuÃ¡ lá»›n, máº¥t ngá»¯ cáº£nh |\n",
    "| **ğŸ¯ OPTIMAL (New)** | **485** | **954** | **~75%** | **~8.5** | **â­â­â­â­** | **ğŸ† OPTIMAL** |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Æ¯u Äiá»ƒm Cá»§a Optimal Strategy\n",
    "\n",
    "### ğŸ¯ **Hybrid Approach**\n",
    "- **Base Structure**: Sá»­ dá»¥ng by_dieu lÃ m ná»n táº£ng (150 chunks)  \n",
    "- **Smart Optimization**: Tá»± Ä‘á»™ng merge/split dá»±a theo size limits\n",
    "- **Hierarchical Detail**: TÃ¡ch Khoáº£n khi cáº§n thiáº¿t (405 sub-chunks)\n",
    "- **Context Headers**: ThÃªm metadata ngá»¯ cáº£nh cho má»—i chunk\n",
    "\n",
    "### ğŸ“Š **Performance Metrics**\n",
    "- **485 chunks** - Sá»‘ lÆ°á»£ng há»£p lÃ½ cho search performance\n",
    "- **954 chars avg** - Size tá»‘i Æ°u cho embedding models\n",
    "- **140-6569 chars range** - Linh hoáº¡t theo ná»™i dung\n",
    "- **~536 tokens avg** - An toÃ n vá»›i 8191 token limit\n",
    "\n",
    "### ğŸ”§ **Technical Features**\n",
    "- **Token Validation**: Kiá»ƒm tra vÃ  cáº£nh bÃ¡o over-limit chunks\n",
    "- **Semantic Tags**: Tá»± Ä‘á»™ng tag theo chá»§ Ä‘á» (registration, procedures, etc.)\n",
    "- **Quality Scoring**: ÄÃ¡nh giÃ¡ readability vÃ  structure\n",
    "- **Export Ready**: JSONL format sáºµn sÃ ng cho vector DB\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Implementation Roadmap\n",
    "\n",
    "### **Phase 1 (1-2 ngÃ y)**: Core Integration\n",
    "```python\n",
    "# Thay tháº¿ current chunker trong vectorstore.py\n",
    "from app.data.core.optimal_chunker import OptimalLegalChunker\n",
    "\n",
    "chunker = OptimalLegalChunker(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=300, \n",
    "    token_limit=6500\n",
    ")\n",
    "```\n",
    "\n",
    "### **Phase 2 (1 tuáº§n)**: Enhancement Features\n",
    "- **Smart Overlapping**: ThÃªm overlap logic cho context continuity\n",
    "- **Performance Monitoring**: Log chunk stats vÃ  search performance\n",
    "- **A/B Testing**: So sÃ¡nh retrieval quality vá»›i old chunker\n",
    "\n",
    "### **Phase 3 (2 tuáº§n)**: Advanced Features  \n",
    "- **ML-based Semantic Splitting**: Sá»­ dá»¥ng sentence embeddings\n",
    "- **Dynamic Chunking**: Adjust strategy dá»±a theo document type\n",
    "- **Performance Dashboard**: Monitor vÃ  optimize real-time\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Key Insights\n",
    "\n",
    "1. **ğŸ“ˆ Balance is Key**: Optimal strategy cÃ¢n báº±ng giá»¯a detail vÃ  efficiency\n",
    "2. **ğŸ¯ Context Matters**: Headers vÃ  hierarchy giÃºp cáº£i thiá»‡n retrieval accuracy\n",
    "3. **âš¡ Token Management**: Validation pipeline quan trá»ng cho cost control\n",
    "4. **ğŸ”§ Flexibility**: Hybrid approach adapt Ä‘Æ°á»£c vá»›i diverse content structure\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Next Steps\n",
    "\n",
    "1. **âœ… DONE**: Comprehensive analysis vÃ  strategy comparison\n",
    "2. **ğŸ”„ NEXT**: Integration vÃ o main RAG pipeline\n",
    "3. **ğŸ“Š TODO**: Performance testing vá»›i real queries\n",
    "4. **ğŸš€ FUTURE**: ML-enhanced semantic chunking\n",
    "\n",
    "**ğŸ’ª Ready for Production Implementation!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-bidding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
