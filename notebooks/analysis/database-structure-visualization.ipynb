{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0560dd83",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6c2d82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Project: /home/sakana/Code/RAG-bidding\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"üìÅ Project: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c12e005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Database config\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'rag_bidding_v2',\n",
    "    'user': 'sakana',\n",
    "    'password': 'sakana123'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ee95bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_connection():\n",
    "    \"\"\"Get database connection.\"\"\"\n",
    "    return psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "def run_query(query: str, params: tuple = None) -> pd.DataFrame:\n",
    "    \"\"\"Run query and return DataFrame.\"\"\"\n",
    "    conn = get_connection()\n",
    "    try:\n",
    "        df = pd.read_sql_query(query, conn, params=params)\n",
    "        return df\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def run_query_dict(query: str, params: tuple = None) -> List[Dict]:\n",
    "    \"\"\"Run query and return list of dicts.\"\"\"\n",
    "    conn = get_connection()\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, params)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        results = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "        return results\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print formatted section header.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üìä {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfccf47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Part 1: Database Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19ab91",
   "metadata": {},
   "source": [
    "### 1.1: List All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a318da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Database Tables\n",
      "================================================================================\n",
      "\n",
      "schemaname               tablename   size\n",
      "    public  langchain_pg_embedding 112 MB\n",
      "    public               documents 176 kB\n",
      "    public langchain_pg_collection  48 kB\n",
      "\n",
      "üìã Total tables: 3\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Database Tables\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    schemaname,\n",
    "    tablename,\n",
    "    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\n",
    "FROM pg_tables\n",
    "WHERE schemaname NOT IN ('pg_catalog', 'information_schema')\n",
    "ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n",
    "\"\"\"\n",
    "\n",
    "tables_df = run_query(query)\n",
    "print(tables_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìã Total tables: {len(tables_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35ba7c",
   "metadata": {},
   "source": [
    "### 1.2: Vector DB Table Structure (langchain_pg_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3179589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Vector DB Table Structure\n",
      "================================================================================\n",
      "\n",
      "üìã Columns:\n",
      "  column_name         data_type character_maximum_length is_nullable\n",
      "           id character varying                     None          NO\n",
      "collection_id              uuid                     None         YES\n",
      "    embedding      USER-DEFINED                     None         YES\n",
      "     document character varying                     None         YES\n",
      "    cmetadata             jsonb                     None         YES\n",
      "\n",
      "üìä Total chunks: 6,242\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Vector DB Table Structure\")\n",
    "\n",
    "# Get column information\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    column_name,\n",
    "    data_type,\n",
    "    character_maximum_length,\n",
    "    is_nullable\n",
    "FROM information_schema.columns\n",
    "WHERE table_name = 'langchain_pg_embedding'\n",
    "ORDER BY ordinal_position;\n",
    "\"\"\"\n",
    "\n",
    "columns_df = run_query(query)\n",
    "print(\"üìã Columns:\")\n",
    "print(columns_df.to_string(index=False))\n",
    "\n",
    "# Get row count\n",
    "count_query = \"SELECT COUNT(*) as total_chunks FROM langchain_pg_embedding;\"\n",
    "count_df = run_query(count_query)\n",
    "\n",
    "print(f\"\\nüìä Total chunks: {count_df['total_chunks'].iloc[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35472f91",
   "metadata": {},
   "source": [
    "### 1.3: Documents Table Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "867db017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Documents Table Structure\n",
      "================================================================================\n",
      "\n",
      "üìã Columns:\n",
      "  column_name                   data_type  character_maximum_length is_nullable              column_default\n",
      "           id                        uuid                       NaN          NO           gen_random_uuid()\n",
      "  document_id           character varying                     255.0          NO                        None\n",
      "document_name                        text                       NaN          NO                        None\n",
      "     category           character varying                     100.0          NO                        None\n",
      "document_type           character varying                      50.0          NO                        None\n",
      "  source_file                        text                       NaN          NO                        None\n",
      "    file_name                        text                       NaN          NO                        None\n",
      " total_chunks                     integer                       NaN         YES                           0\n",
      "       status           character varying                      50.0         YES 'active'::character varying\n",
      "   created_at timestamp without time zone                       NaN         YES                       now()\n",
      "   updated_at timestamp without time zone                       NaN         YES                       now()\n",
      "\n",
      "üìä Total documents: 61\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Documents Table Structure\")\n",
    "\n",
    "# Get column information\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    column_name,\n",
    "    data_type,\n",
    "    character_maximum_length,\n",
    "    is_nullable,\n",
    "    column_default\n",
    "FROM information_schema.columns\n",
    "WHERE table_name = 'documents'\n",
    "ORDER BY ordinal_position;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    columns_df = run_query(query)\n",
    "    print(\"üìã Columns:\")\n",
    "    print(columns_df.to_string(index=False))\n",
    "    \n",
    "    # Get row count\n",
    "    count_query = \"SELECT COUNT(*) as total_documents FROM documents;\"\n",
    "    count_df = run_query(count_query)\n",
    "    \n",
    "    print(f\"\\nüìä Total documents: {count_df['total_documents'].iloc[0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"   Documents table may not exist yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d818717",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Part 2: Sample Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f391d1",
   "metadata": {},
   "source": [
    "### 2.1: Sample Chunks from Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aef2cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Sample Chunks - Vector DB\n",
      "================================================================================\n",
      "\n",
      "üìÑ First 10 chunks:\n",
      "\n",
      "[1] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: M·∫´u s·ªë 01A. M·∫´u T·ªù tr√¨nh k·∫ø ho·∫°ch t·ªïng th·ªÉ l·ª±a ch·ªçn nh√† th·∫ßu]\n",
      "\n",
      "M·∫´u s·ªë 01A. M·∫´u T·ªù tr√¨nh k·∫ø...\n",
      "\n",
      "[2] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: M·∫´u s·ªë 01 (Webform tr√™n H·ªá th·ªëng)]\n",
      "\n",
      "M·∫´u s·ªë 01 (Webform tr√™n H·ªá th·ªëng)\n",
      "B·∫¢NG K√ä H·∫†NG M·ª§C C√îN...\n",
      "\n",
      "[3] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: M·∫´u s·ªë 4.5. M·∫´u B√°o c√°o t√¨nh h√¨nh th·ª±c hi·ªán K·∫øt lu·∫≠n ki·ªÉm tra]\n",
      "\n",
      "M·∫´u s·ªë 4.5. M·∫´u B√°o c√°o t√¨...\n",
      "\n",
      "[4] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: M·∫´u s·ªë 4.3. M·∫´u B√°o c√°o ki·ªÉm tra ho·∫°t ƒë·ªông]\n",
      "\n",
      "M·∫´u s·ªë 4.3. M·∫´u B√°o c√°o ki·ªÉm tra ho·∫°t ƒë·ªông\n",
      "l·ª±...\n",
      "\n",
      "[5] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: M·∫´u s·ªë 4.1B. M·∫´u K·∫ø ho·∫°ch ki·ªÉm tra chi ti·∫øt]\n",
      "\n",
      "M·∫´u s·ªë 4.1B. M·∫´u K·∫ø ho·∫°ch ki·ªÉm tra chi ti·∫øt\n",
      "...\n",
      "\n",
      "[6] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: M·∫´u s·ªë 4.1A. M·∫´u K·∫ø ho·∫°ch ki·ªÉm tra ƒë·ªãnh k·ª≥ ho·∫°t ƒë·ªông ƒë·∫•u th·∫ßu]\n",
      "\n",
      "M·∫´u s·ªë 4.1A. M·∫´u K·∫ø ho·∫°ch ...\n",
      "\n",
      "[7] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: Ph·ª• l·ª•c 1: Bi√™n b·∫£n ƒë√≥ng th·∫ßu]\n",
      "\n",
      "      Ph·ª• l·ª•c 1: Bi√™n b·∫£n ƒë√≥ng th·∫ßu\n",
      "...\n",
      "\n",
      "[8] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: M·∫´u s·ªë 4.4. M·∫´u K·∫øt lu·∫≠n ki·ªÉm tra ho·∫°t ƒë·ªông l·ª±a ch·ªçn nh√† th·∫ßu, nh√† ƒë·∫ßu]\n",
      "\n",
      "M·∫´u s·ªë 4.4. M·∫´u K...\n",
      "\n",
      "[9] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: M·∫´u s·ªë 4.2. M·∫´u ƒê·ªÅ c∆∞∆°ng b√°o c√°o t√¨nh h√¨nh th·ª±c hi·ªán ho·∫°t ƒë·ªông l·ª±a ch·ªçn nh√†]\n",
      "\n",
      "M·∫´u s·ªë 4.2. ...\n",
      "\n",
      "[10] document_id: bidding_untitled\n",
      "    chunk_id: bidding_untitled_form_0000\n",
      "    chunk_index: 0\n",
      "    type: bidding\n",
      "    title: None\n",
      "    source: N/A\n",
      "    content: [Section: M·∫´u s·ªë 01 (webform tr√™n H·ªá th·ªëng)]\n",
      "\n",
      "M·∫´u s·ªë 01 (webform tr√™n H·ªá th·ªëng)\n",
      "TH√îNG B√ÅO M·ªúI TH·∫¶U\n",
      "(...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Sample Chunks - Vector DB\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    id as chunk_uuid,\n",
    "    LEFT(document, 100) as content_preview,\n",
    "    cmetadata->>'document_id' as document_id,\n",
    "    cmetadata->>'chunk_id' as chunk_id,\n",
    "    cmetadata->>'chunk_index' as chunk_index,\n",
    "    cmetadata->>'document_type' as doc_type,\n",
    "    cmetadata->>'title' as title,\n",
    "    cmetadata->>'source_file' as source_file\n",
    "FROM langchain_pg_embedding\n",
    "ORDER BY cmetadata->>'document_id', (cmetadata->>'chunk_index')::int\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "samples_df = run_query(query)\n",
    "\n",
    "print(\"üìÑ First 10 chunks:\\n\")\n",
    "for idx, row in samples_df.iterrows():\n",
    "    print(f\"[{idx+1}] document_id: {row['document_id']}\")\n",
    "    print(f\"    chunk_id: {row['chunk_id']}\")\n",
    "    print(f\"    chunk_index: {row['chunk_index']}\")\n",
    "    print(f\"    type: {row['doc_type']}\")\n",
    "    print(f\"    title: {row['title'][:60]}...\" if row['title'] and len(str(row['title'])) > 60 else f\"    title: {row['title']}\")\n",
    "    print(f\"    source: {Path(row['source_file']).name if row['source_file'] else 'N/A'}\")\n",
    "    print(f\"    content: {row['content_preview']}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6cff1",
   "metadata": {},
   "source": [
    "### 2.2: Analyze document_id Format Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "629aa78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Document ID Format Distribution\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             format_type  unique_documents  total_chunks   pct\n",
      "        Bi·ªÉu m·∫´u (FORM-)                38          2876 100.0\n",
      "             Lu·∫≠t (LUA-)                 4          1154 100.0\n",
      "‚ö†Ô∏è Old Format (untitled)                 1           767 100.0\n",
      "         M·∫´u (TEMPLATE-)                10           722 100.0\n",
      "         Ngh·ªã ƒë·ªãnh (ND-)                 1           595 100.0\n",
      "          Th√¥ng t∆∞ (TT-)                 2           123 100.0\n",
      "        Quy·∫øt ƒë·ªãnh (QD-)                 1             5 100.0\n",
      "\n",
      "‚ö†Ô∏è WARNING: Found 767 chunks with OLD format!\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Document ID Format Distribution\")\n",
    "\n",
    "query = \"\"\"\n",
    "WITH doc_formats AS (\n",
    "    SELECT \n",
    "        cmetadata->>'document_id' as document_id,\n",
    "        CASE \n",
    "            WHEN cmetadata->>'document_id' ~ '^LUA-' THEN 'Lu·∫≠t (LUA-)'\n",
    "            WHEN cmetadata->>'document_id' ~ '^ND-' THEN 'Ngh·ªã ƒë·ªãnh (ND-)'\n",
    "            WHEN cmetadata->>'document_id' ~ '^TT-' THEN 'Th√¥ng t∆∞ (TT-)'\n",
    "            WHEN cmetadata->>'document_id' ~ '^QD-' THEN 'Quy·∫øt ƒë·ªãnh (QD-)'\n",
    "            WHEN cmetadata->>'document_id' ~ '^FORM-' THEN 'Bi·ªÉu m·∫´u (FORM-)'\n",
    "            WHEN cmetadata->>'document_id' ~ '^TEMPLATE-' THEN 'M·∫´u (TEMPLATE-)'\n",
    "            WHEN cmetadata->>'document_id' ~ '^EXAM-' THEN 'C√¢u h·ªèi thi (EXAM-)'\n",
    "            WHEN cmetadata->>'document_id' LIKE '%untitled%' THEN '‚ö†Ô∏è Old Format (untitled)'\n",
    "            ELSE 'Other'\n",
    "        END as format_type\n",
    "    FROM langchain_pg_embedding\n",
    ")\n",
    "SELECT \n",
    "    format_type,\n",
    "    COUNT(DISTINCT document_id) as unique_documents,\n",
    "    COUNT(*) as total_chunks,\n",
    "    ROUND(AVG(CASE WHEN document_id IS NOT NULL THEN 1 ELSE 0 END) * 100, 2) as pct\n",
    "FROM doc_formats\n",
    "GROUP BY format_type\n",
    "ORDER BY total_chunks DESC;\n",
    "\"\"\"\n",
    "\n",
    "format_df = run_query(query)\n",
    "print(format_df.to_string(index=False))\n",
    "\n",
    "# Check for old format\n",
    "old_format = format_df[format_df['format_type'].str.contains('untitled', case=False)]\n",
    "if not old_format.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Found {old_format['total_chunks'].sum()} chunks with OLD format!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All chunks use NEW format (no 'untitled' found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b534b",
   "metadata": {},
   "source": [
    "### 2.3: Sample Metadata Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4591bb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Sample Metadata Fields Analysis\n",
      "================================================================================\n",
      "\n",
      "üìã Example Metadata (LUA document):\n",
      "\n",
      "{\n",
      "  \"level\": \"dieu\",\n",
      "  \"chunk_id\": \"LUA-57-2024-QH15_dieu_0143\",\n",
      "  \"has_list\": true,\n",
      "  \"has_table\": false,\n",
      "  \"hierarchy\": [\n",
      "    \"ƒêi·ªÅu 5. ƒêi·ªÅu kho·∫£n thi h√†nh\"\n",
      "  ],\n",
      "  \"char_count\": 618,\n",
      "  \"chunk_index\": 118,\n",
      "  \"document_id\": \"LUA-57-2024-QH15\",\n",
      "  \"source_file\": \"data/raw/Luat chinh/Luat so 57 2024 QH15.docx\",\n",
      "  \"total_chunks\": 128,\n",
      "  \"document_info\": {\n",
      "    \"document_status\": \"active\"\n",
      "  },\n",
      "  \"document_type\": \"law\",\n",
      "  \"section_title\": \"ƒêi·ªÅu kho·∫£n thi h√†nh\",\n",
      "  \"extra_metadata\": {\n",
      "    \"muc\": null,\n",
      "    \"phan\": null,\n",
      "    \"chuong\": null,\n",
      "    \"batch_name\": null,\n",
      "    \"dieu_number\": \"5\",\n",
      "    \"khoan_number\": null,\n",
      "    \"pipeline_version\": \"working_upload_pipeline_v1.0\",\n",
      "    \"processing_time_ms\": 35\n",
      "  },\n",
      "  \"parent_context\": null,\n",
      "  \"is_complete_unit\": true,\n",
      "  \"processing_metadata\": {\n",
      "    \"retry_count\": 0,\n",
      "    \"error_message\": null,\n",
      "    \"last_processed_at\": \"2025-11-09T14:31:57.585012\",\n",
      "    \"processing_status\": \"completed\"\n",
      "  }\n",
      "}\n",
      "\n",
      "üîë Available Keys:\n",
      "   1. char_count                     (int       ): 618\n",
      "   2. chunk_id                       (str       ): LUA-57-2024-QH15_dieu_0143\n",
      "   3. chunk_index                    (int       ): 118\n",
      "   4. document_id                    (str       ): LUA-57-2024-QH15\n",
      "   5. document_info                  (dict      ): {'document_status': 'active'}\n",
      "   6. document_type                  (str       ): law\n",
      "   7. extra_metadata                 (dict      ): {'muc': None, 'phan': None, 'chuong': None, 'batch...\n",
      "   8. has_list                       (bool      ): True\n",
      "   9. has_table                      (bool      ): False\n",
      "  10. hierarchy                      (list      ): ['ƒêi·ªÅu 5. ƒêi·ªÅu kho·∫£n thi h√†nh']\n",
      "  11. is_complete_unit               (bool      ): True\n",
      "  12. level                          (str       ): dieu\n",
      "  13. parent_context                 (NoneType  ): None\n",
      "  14. processing_metadata            (dict      ): {'retry_count': 0, 'error_message': None, 'last_pr...\n",
      "  15. section_title                  (str       ): ƒêi·ªÅu kho·∫£n thi h√†nh\n",
      "  16. source_file                    (str       ): data/raw/Luat chinh/Luat so 57 2024 QH15.docx\n",
      "  17. total_chunks                   (int       ): 128\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Sample Metadata Fields Analysis\")\n",
    "\n",
    "# Get one full metadata example\n",
    "query = \"\"\"\n",
    "SELECT cmetadata\n",
    "FROM langchain_pg_embedding\n",
    "WHERE cmetadata->>'document_id' ~ '^LUA-'\n",
    "LIMIT 1;\n",
    "\"\"\"\n",
    "\n",
    "result = run_query_dict(query)\n",
    "\n",
    "if result:\n",
    "    metadata = result[0]['cmetadata']\n",
    "    \n",
    "    print(\"üìã Example Metadata (LUA document):\\n\")\n",
    "    print(json.dumps(metadata, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    print(\"\\nüîë Available Keys:\")\n",
    "    for i, key in enumerate(sorted(metadata.keys()), 1):\n",
    "        value_type = type(metadata[key]).__name__\n",
    "        value_preview = str(metadata[key])[:50]\n",
    "        print(f\"  {i:2d}. {key:30s} ({value_type:10s}): {value_preview}...\" if len(str(metadata[key])) > 50 else f\"  {i:2d}. {key:30s} ({value_type:10s}): {value_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526c345",
   "metadata": {},
   "source": [
    "### 2.4: Metadata Fields Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a84872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Metadata Fields Coverage\n",
      "================================================================================\n",
      "\n",
      "              field  has_field  total  coverage_pct\n",
      "        document_id       6242   6242        100.00\n",
      "           chunk_id       6242   6242        100.00\n",
      "        chunk_index       6242   6242        100.00\n",
      "      document_type       6242   6242        100.00\n",
      "          hierarchy       6242   6242        100.00\n",
      "        source_file       6239   6242         99.95\n",
      "processing_metadata       4708   6242         75.42\n",
      "      document_info       4708   6242         75.42\n",
      "              title          0   6242          0.00\n",
      "               dieu          0   6242          0.00\n",
      "               diem          0   6242          0.00\n",
      "              khoan          0   6242          0.00\n",
      "     effective_date          0   6242          0.00\n",
      "     published_date          0   6242          0.00\n",
      "\n",
      "üìä Summary:\n",
      "   ‚úÖ 100% coverage: 5 fields\n",
      "   ‚ö†Ô∏è  < 100% coverage: 9 fields\n",
      "              field  has_field  total  coverage_pct\n",
      "        document_id       6242   6242        100.00\n",
      "           chunk_id       6242   6242        100.00\n",
      "        chunk_index       6242   6242        100.00\n",
      "      document_type       6242   6242        100.00\n",
      "          hierarchy       6242   6242        100.00\n",
      "        source_file       6239   6242         99.95\n",
      "processing_metadata       4708   6242         75.42\n",
      "      document_info       4708   6242         75.42\n",
      "              title          0   6242          0.00\n",
      "               dieu          0   6242          0.00\n",
      "               diem          0   6242          0.00\n",
      "              khoan          0   6242          0.00\n",
      "     effective_date          0   6242          0.00\n",
      "     published_date          0   6242          0.00\n",
      "\n",
      "üìä Summary:\n",
      "   ‚úÖ 100% coverage: 5 fields\n",
      "   ‚ö†Ô∏è  < 100% coverage: 9 fields\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Metadata Fields Coverage\")\n",
    "\n",
    "# Check common metadata fields\n",
    "fields_to_check = [\n",
    "    'document_id',\n",
    "    'chunk_id',\n",
    "    'chunk_index',\n",
    "    'document_type',\n",
    "    'title',\n",
    "    'source_file',\n",
    "    'dieu',\n",
    "    'khoan',\n",
    "    'diem',\n",
    "    'hierarchy',\n",
    "    'published_date',\n",
    "    'effective_date',\n",
    "    'processing_metadata',\n",
    "    'document_info'\n",
    "]\n",
    "\n",
    "coverage_data = []\n",
    "\n",
    "for field in fields_to_check:\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total,\n",
    "        COUNT(CASE WHEN cmetadata->'{field}' IS NOT NULL THEN 1 END) as has_field,\n",
    "        ROUND(COUNT(CASE WHEN cmetadata->'{field}' IS NOT NULL THEN 1 END)::numeric / COUNT(*) * 100, 2) as coverage_pct\n",
    "    FROM langchain_pg_embedding;\n",
    "    \"\"\"\n",
    "    \n",
    "    result = run_query(query)\n",
    "    coverage_data.append({\n",
    "        'field': field,\n",
    "        'has_field': result['has_field'].iloc[0],\n",
    "        'total': result['total'].iloc[0],\n",
    "        'coverage_pct': result['coverage_pct'].iloc[0]\n",
    "    })\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_data)\n",
    "coverage_df = coverage_df.sort_values('coverage_pct', ascending=False)\n",
    "\n",
    "print(coverage_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"   ‚úÖ 100% coverage: {len(coverage_df[coverage_df['coverage_pct'] == 100.0])} fields\")\n",
    "print(f\"   ‚ö†Ô∏è  < 100% coverage: {len(coverage_df[coverage_df['coverage_pct'] < 100.0])} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fe920",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 3: Documents Table Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98dfac",
   "metadata": {},
   "source": [
    "### 3.1: Documents Table Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a78b662f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Documents Table - Sample Data\n",
      "================================================================================\n",
      "\n",
      "‚ùå Error: Execution failed on sql '\n",
      "SELECT \n",
      "    document_id,\n",
      "    document_name,\n",
      "    document_type,\n",
      "    category,\n",
      "    total_chunks,\n",
      "    status,\n",
      "    published_date,\n",
      "    effective_date,\n",
      "    created_at\n",
      "FROM documents\n",
      "ORDER BY created_at DESC\n",
      "LIMIT 10;\n",
      "': column \"published_date\" does not exist\n",
      "LINE 9:     published_date,\n",
      "            ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Documents Table - Sample Data\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    document_id,\n",
    "    document_name,\n",
    "    document_type,\n",
    "    category,\n",
    "    total_chunks,\n",
    "    status,\n",
    "    published_date,\n",
    "    effective_date,\n",
    "    created_at\n",
    "FROM documents\n",
    "ORDER BY created_at DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    docs_df = run_query(query)\n",
    "    \n",
    "    print(\"üìÑ Sample documents:\\n\")\n",
    "    for idx, row in docs_df.iterrows():\n",
    "        print(f\"[{idx+1}] {row['document_id']}\")\n",
    "        print(f\"    Name: {row['document_name']}\")\n",
    "        print(f\"    Type: {row['document_type']} | Category: {row['category']}\")\n",
    "        print(f\"    Chunks: {row['total_chunks']} | Status: {row['status']}\")\n",
    "        print(f\"    Published: {row['published_date']} | Effective: {row['effective_date']}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5bf28",
   "metadata": {},
   "source": [
    "### 3.2: Documents vs Chunks Consistency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7289813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Documents ‚Üî Chunks Consistency Check\n",
      "================================================================================\n",
      "\n",
      "              document_id  vector_db  documents_table                    status\n",
      "EXAM-Ng√¢n-h√†ng-c√¢u-h·ªèi-CC          0                0 ‚ùå Only in documents table\n",
      "EXAM-Ng√¢n-h√†ng-c√¢u-h·ªèi-th          0                0 ‚ùå Only in documents table\n",
      "  EXAM-NHCH_2692025_dot-2          0                0 ‚ùå Only in documents table\n",
      "  EXAM-NHCH_30925_bo_sung          0                0 ‚ùå Only in documents table\n",
      "         bidding_untitled        767              767                   ‚úÖ Match\n",
      "          FORM-01-Ph·ª•-l·ª•c         48               48                   ‚úÖ Match\n",
      "  FORM-041A-M·∫´u-K·∫ø-ho·∫°ch-          1                1                   ‚úÖ Match\n",
      "  FORM-041B-M·∫´u-K·∫ø-ho·∫°ch-         10               10                   ‚úÖ Match\n",
      "  FORM-042-M·∫´u-ƒê·ªÅ-c∆∞∆°ng-b         10               10                   ‚úÖ Match\n",
      "  FORM-043-M·∫´u-B√°o-c√°o-ki          8                8                   ‚úÖ Match\n",
      "  FORM-044-M·∫´u-K·∫øt-lu·∫≠n-k          4                4                   ‚úÖ Match\n",
      "  FORM-045-M·∫´u-B√°o-c√°o-ph          4                4                   ‚úÖ Match\n",
      " FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-        130              130                   ‚úÖ Match\n",
      " FORM-10-M·∫´u-s·ªë-10A_E-HSM          6                6                   ‚úÖ Match\n",
      " FORM-10-M·∫´u-s·ªë-10B_E-HSM        104              104                   ‚úÖ Match\n",
      " FORM-10-M·∫´u-s·ªë-10C_E-HSM         19               19                   ‚úÖ Match\n",
      " FORM-11-M·∫´u-s·ªë-11A_E-HSM         23               23                   ‚úÖ Match\n",
      " FORM-11-M·∫´u-s·ªë-11B_E-HSM          9                9                   ‚úÖ Match\n",
      " FORM-12-M·∫´u-s·ªë-12C-CGTT-         11               11                   ‚úÖ Match\n",
      " FORM-12-M·∫´u-s·ªë-12D-CGTT-         12               12                   ‚úÖ Match\n",
      "\n",
      "üìä Summary:\n",
      "   ‚úÖ Consistent: 16\n",
      "   ‚ö†Ô∏è  Inconsistent: 4\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Documents ‚Üî Chunks Consistency Check\")\n",
    "\n",
    "# Compare documents table with vector DB\n",
    "query = \"\"\"\n",
    "WITH vector_db_docs AS (\n",
    "    SELECT \n",
    "        cmetadata->>'document_id' as document_id,\n",
    "        COUNT(*) as chunks_in_vector_db\n",
    "    FROM langchain_pg_embedding\n",
    "    GROUP BY cmetadata->>'document_id'\n",
    "),\n",
    "documents_table AS (\n",
    "    SELECT \n",
    "        document_id,\n",
    "        total_chunks as chunks_in_documents_table\n",
    "    FROM documents\n",
    ")\n",
    "SELECT \n",
    "    COALESCE(v.document_id, d.document_id) as document_id,\n",
    "    COALESCE(v.chunks_in_vector_db, 0) as vector_db,\n",
    "    COALESCE(d.chunks_in_documents_table, 0) as documents_table,\n",
    "    CASE \n",
    "        WHEN v.chunks_in_vector_db = d.chunks_in_documents_table THEN '‚úÖ Match'\n",
    "        WHEN v.chunks_in_vector_db IS NULL THEN '‚ùå Only in documents table'\n",
    "        WHEN d.chunks_in_documents_table IS NULL THEN '‚ùå Only in vector DB'\n",
    "        ELSE '‚ö†Ô∏è Mismatch'\n",
    "    END as status\n",
    "FROM vector_db_docs v\n",
    "FULL OUTER JOIN documents_table d ON v.document_id = d.document_id\n",
    "ORDER BY \n",
    "    CASE \n",
    "        WHEN v.chunks_in_vector_db = d.chunks_in_documents_table THEN 1\n",
    "        ELSE 0\n",
    "    END,\n",
    "    document_id\n",
    "LIMIT 20;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    consistency_df = run_query(query)\n",
    "    print(consistency_df.to_string(index=False))\n",
    "    \n",
    "    # Summary\n",
    "    match_count = len(consistency_df[consistency_df['status'] == '‚úÖ Match'])\n",
    "    mismatch_count = len(consistency_df[consistency_df['status'].str.contains('Mismatch|Only')])\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   ‚úÖ Consistent: {match_count}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Inconsistent: {mismatch_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3bb736",
   "metadata": {},
   "source": [
    "### 3.3: Documents by Type and Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe18ff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Documents Distribution by Type & Category\n",
      "================================================================================\n",
      "\n",
      "  document_type       category  doc_count  total_chunks  avg_chunks_per_doc\n",
      "   bidding_form H·ªì s∆° m·ªùi th·∫ßu         37          2873               77.65\n",
      "report_template    M·∫´u b√°o c√°o         10           722               72.20\n",
      "  exam_question    C√¢u h·ªèi thi          4             0                0.00\n",
      "            law     Lu·∫≠t ch√≠nh          4          1154              288.50\n",
      "       circular       Th√¥ng t∆∞          2           123               61.50\n",
      "        bidding H·ªì s∆° m·ªùi th·∫ßu          2           770              385.00\n",
      "         decree      Ngh·ªã ƒë·ªãnh          1           595              595.00\n",
      "       decision     Quy·∫øt ƒë·ªãnh          1             5                5.00\n",
      "\n",
      "üìä Total: 61 documents, 6,242 chunks\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Documents Distribution by Type & Category\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    document_type,\n",
    "    category,\n",
    "    COUNT(*) as doc_count,\n",
    "    SUM(total_chunks) as total_chunks,\n",
    "    ROUND(AVG(total_chunks), 2) as avg_chunks_per_doc\n",
    "FROM documents\n",
    "GROUP BY document_type, category\n",
    "ORDER BY doc_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    dist_df = run_query(query)\n",
    "    print(dist_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nüìä Total: {dist_df['doc_count'].sum()} documents, {dist_df['total_chunks'].sum():,} chunks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b19ab7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Part 4: Old vs New Format Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe830ff",
   "metadata": {},
   "source": [
    "### 4.1: Check for Old Format Remnants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0db7e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Old Format Check\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è Found 10 chunks with old format:\n",
      "\n",
      "     document_id                      chunk_id  chunk_count\n",
      "bidding_untitled    bidding_untitled_form_0000           22\n",
      "bidding_untitled bidding_untitled_section_0006           18\n",
      "bidding_untitled bidding_untitled_section_0004           16\n",
      "bidding_untitled bidding_untitled_section_0007           16\n",
      "bidding_untitled bidding_untitled_section_0003           16\n",
      "bidding_untitled    bidding_untitled_form_0002           15\n",
      "bidding_untitled bidding_untitled_section_0005           15\n",
      "bidding_untitled    bidding_untitled_form_0001           15\n",
      "bidding_untitled bidding_untitled_section_0002           15\n",
      "bidding_untitled bidding_untitled_section_0001           15\n",
      "\n",
      "‚ùå Total chunks with old format: 163\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Old Format Check\")\n",
    "\n",
    "# Search for old format patterns\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    cmetadata->>'document_id' as document_id,\n",
    "    cmetadata->>'chunk_id' as chunk_id,\n",
    "    COUNT(*) as chunk_count\n",
    "FROM langchain_pg_embedding\n",
    "WHERE \n",
    "    cmetadata->>'document_id' LIKE '%untitled%'\n",
    "    OR cmetadata->>'chunk_id' LIKE '%untitled%'\n",
    "GROUP BY cmetadata->>'document_id', cmetadata->>'chunk_id'\n",
    "ORDER BY chunk_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "old_format_df = run_query(query)\n",
    "\n",
    "if old_format_df.empty:\n",
    "    print(\"‚úÖ No old format (untitled) found in vector DB\")\n",
    "    print(\"   Migration was successful!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Found {len(old_format_df)} chunks with old format:\\n\")\n",
    "    print(old_format_df.to_string(index=False))\n",
    "    \n",
    "    total_old = old_format_df['chunk_count'].sum()\n",
    "    print(f\"\\n‚ùå Total chunks with old format: {total_old}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e62e4",
   "metadata": {},
   "source": [
    "### 4.2: New Format Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "654f9bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä New Format Examples\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìÑ Lu·∫≠t (LUA-)\n",
      "   document_id: LUA-57-2024-QH15\n",
      "   chunk_id: LUA-57-2024-QH15_dieu_0143\n",
      "   title: None\n",
      "   content: [Section: ƒêi·ªÅu 5. ƒêi·ªÅu kho·∫£n thi h√†nh]\n",
      "\n",
      "ƒêi·ªÅu 5. ƒêi·ªÅu kho·∫£n thi h√†nh\n",
      "1. B√£i b·ªè ƒêi...\n",
      "\n",
      "üìÑ Ngh·ªã ƒë·ªãnh (ND-)\n",
      "   document_id: ND-214-4.8.-CP\n",
      "   chunk_id: ND-214-4.8.-CP_dieu_0000\n",
      "   title: None\n",
      "   content: [Section: ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh]\n",
      "\n",
      "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\n",
      "1. Ngh·ªã ƒë·ªãnh n...\n",
      "\n",
      "üìÑ Th√¥ng t∆∞ (TT-)\n",
      "   document_id: TT-00-Quy·∫øt-ƒë·ªãnh-Th√¥ng-t∆∞\n",
      "   chunk_id: TT-00-Quy·∫øt-ƒë·ªãnh-Th√¥ng-t∆∞_dieu_0000\n",
      "   title: None\n",
      "   content: [Section: ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh]\n",
      "\n",
      "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\n",
      "Th√¥ng t∆∞ n√†y h...\n",
      "\n",
      "üìÑ Quy·∫øt ƒë·ªãnh (QD-)\n",
      "   document_id: QD-1667-BYT\n",
      "   chunk_id: QD-1667-BYT_dieu_0000\n",
      "   title: None\n",
      "   content: [Section: ƒêi·ªÅu 1. Quy·∫øt ƒë·ªãnh n√†y quy ƒë·ªãnh vi·ªác √°p d·ª•ng h√¨nh th·ª©c l·ª±a ch·ªçn nh√† th...\n",
      "\n",
      "üìÑ Bi·ªÉu m·∫´u (FORM-)\n",
      "   document_id: FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-\n",
      "   chunk_id: FORM-05-M·∫´u-B√°o-c√°o-ƒë·∫•u-_section_0000\n",
      "   title: None\n",
      "   content: [Section: I. TH√îNG TIN C∆† B·∫¢N]\n",
      "\n",
      "1. Gi·ªõi thi·ªáu chung v·ªÅ d·ª± √°n/d·ª± to√°n mua s·∫Øm, g√≥...\n",
      "\n",
      "üìÑ M·∫´u (TEMPLATE-)\n",
      "   document_id: TEMPLATE-BC-14A\n",
      "   chunk_id: TEMPLATE-BC-14A_section_0021\n",
      "   title: None\n",
      "   content: [Section: III. C√°c h·∫°ng m·ª•c √°p d·ª•ng lo·∫°i h·ª£p ƒë·ªìng theo ƒë∆°n gi√° ƒëi·ªÅu ch·ªânh]\n",
      "\n",
      "h√¥ng...\n",
      "\n",
      "‚ö†Ô∏è C√¢u h·ªèi thi (EXAM-): No examples found\n"
     ]
    }
   ],
   "source": [
    "print_section(\"New Format Examples\")\n",
    "\n",
    "# Get examples of each new format type\n",
    "format_patterns = {\n",
    "    'Lu·∫≠t (LUA-)': '^LUA-',\n",
    "    'Ngh·ªã ƒë·ªãnh (ND-)': '^ND-',\n",
    "    'Th√¥ng t∆∞ (TT-)': '^TT-',\n",
    "    'Quy·∫øt ƒë·ªãnh (QD-)': '^QD-',\n",
    "    'Bi·ªÉu m·∫´u (FORM-)': '^FORM-',\n",
    "    'M·∫´u (TEMPLATE-)': '^TEMPLATE-',\n",
    "    'C√¢u h·ªèi thi (EXAM-)': '^EXAM-'\n",
    "}\n",
    "\n",
    "for format_name, pattern in format_patterns.items():\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        cmetadata->>'document_id' as document_id,\n",
    "        cmetadata->>'chunk_id' as chunk_id,\n",
    "        cmetadata->>'title' as title,\n",
    "        LEFT(document, 80) as content_preview\n",
    "    FROM langchain_pg_embedding\n",
    "    WHERE cmetadata->>'document_id' ~ '{pattern}'\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "    \n",
    "    result = run_query(query)\n",
    "    \n",
    "    if not result.empty:\n",
    "        row = result.iloc[0]\n",
    "        print(f\"\\nüìÑ {format_name}\")\n",
    "        print(f\"   document_id: {row['document_id']}\")\n",
    "        print(f\"   chunk_id: {row['chunk_id']}\")\n",
    "        print(f\"   title: {row['title'][:60]}...\" if row['title'] and len(str(row['title'])) > 60 else f\"   title: {row['title']}\")\n",
    "        print(f\"   content: {row['content_preview']}...\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è {format_name}: No examples found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acd7ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Part 5: Preprocessing Requirements Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9de2a5",
   "metadata": {},
   "source": [
    "### 5.1: What Preprocessing Needs to Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b13925aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Preprocessing Requirements\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìã REQUIREMENTS FOR UPLOAD PIPELINE:\n",
      "\n",
      "1Ô∏è‚É£ **Generate document_id**\n",
      "   - Format: LUA-*, ND-*, TT-*, QD-*, FORM-*, TEMPLATE-*, EXAM-*\n",
      "   - Based on document type and filename\n",
      "   - Must be unique and descriptive\n",
      "\n",
      "2Ô∏è‚É£ **Extract Document-Level Metadata**\n",
      "   - document_name: From filename or content\n",
      "   - document_type: law, decree, circular, decision, bidding, template, exam\n",
      "   - category: Classify document purpose\n",
      "   - published_date: Extract from document (if available)\n",
      "   - effective_date: Extract from document (if available)\n",
      "   - source_file: Full path to original file\n",
      "\n",
      "3Ô∏è‚É£ **Insert into documents Table**\n",
      "   - Create new row for each uploaded document\n",
      "   - Set status = 'active' (default)\n",
      "   - Set total_chunks = 0 initially\n",
      "   - Record created_at, updated_at timestamps\n",
      "\n",
      "4Ô∏è‚É£ **Generate Chunks with Metadata**\n",
      "   - chunk_id: {document_id}_{type}_{index}\n",
      "   - chunk_index: Sequential number\n",
      "   - Embed chunk content ‚Üí vector\n",
      "   - Store in langchain_pg_embedding with full cmetadata\n",
      "\n",
      "5Ô∏è‚É£ **Update documents.total_chunks**\n",
      "   - After all chunks inserted\n",
      "   - Count chunks from vector DB\n",
      "   - Update documents table\n",
      "\n",
      "6Ô∏è‚É£ **Consistency Check**\n",
      "   - Verify all chunks have document_id in documents table\n",
      "   - Verify total_chunks matches vector DB count\n",
      "   - Rollback if inconsistent\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Current State Check\n",
      "================================================================================\n",
      "\n",
      "üìä Document Count:\n",
      "   Vector DB: 57 unique documents\n",
      "   Documents table: 61 documents\n",
      "\n",
      "‚ö†Ô∏è WARNING: Documents table has 4 extra documents\n",
      "   Some documents may not have chunks\n",
      "üìä Document Count:\n",
      "   Vector DB: 57 unique documents\n",
      "   Documents table: 61 documents\n",
      "\n",
      "‚ö†Ô∏è WARNING: Documents table has 4 extra documents\n",
      "   Some documents may not have chunks\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Preprocessing Requirements\")\n",
    "\n",
    "print(\"\"\"\n",
    "üìã REQUIREMENTS FOR UPLOAD PIPELINE:\n",
    "\n",
    "1Ô∏è‚É£ **Generate document_id**\n",
    "   - Format: LUA-*, ND-*, TT-*, QD-*, FORM-*, TEMPLATE-*, EXAM-*\n",
    "   - Based on document type and filename\n",
    "   - Must be unique and descriptive\n",
    "\n",
    "2Ô∏è‚É£ **Extract Document-Level Metadata**\n",
    "   - document_name: From filename or content\n",
    "   - document_type: law, decree, circular, decision, bidding, template, exam\n",
    "   - category: Classify document purpose\n",
    "   - published_date: Extract from document (if available)\n",
    "   - effective_date: Extract from document (if available)\n",
    "   - source_file: Full path to original file\n",
    "\n",
    "3Ô∏è‚É£ **Insert into documents Table**\n",
    "   - Create new row for each uploaded document\n",
    "   - Set status = 'active' (default)\n",
    "   - Set total_chunks = 0 initially\n",
    "   - Record created_at, updated_at timestamps\n",
    "\n",
    "4Ô∏è‚É£ **Generate Chunks with Metadata**\n",
    "   - chunk_id: {document_id}_{type}_{index}\n",
    "   - chunk_index: Sequential number\n",
    "   - Embed chunk content ‚Üí vector\n",
    "   - Store in langchain_pg_embedding with full cmetadata\n",
    "\n",
    "5Ô∏è‚É£ **Update documents.total_chunks**\n",
    "   - After all chunks inserted\n",
    "   - Count chunks from vector DB\n",
    "   - Update documents table\n",
    "\n",
    "6Ô∏è‚É£ **Consistency Check**\n",
    "   - Verify all chunks have document_id in documents table\n",
    "   - Verify total_chunks matches vector DB count\n",
    "   - Rollback if inconsistent\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Current State Check\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Check if preprocessing follows these requirements\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT cmetadata->>'document_id') as unique_docs_in_vector_db,\n",
    "    (SELECT COUNT(*) FROM documents) as docs_in_documents_table\n",
    "FROM langchain_pg_embedding;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    check_df = run_query(query)\n",
    "    \n",
    "    vector_docs = check_df['unique_docs_in_vector_db'].iloc[0]\n",
    "    table_docs = check_df['docs_in_documents_table'].iloc[0]\n",
    "    \n",
    "    print(f\"üìä Document Count:\")\n",
    "    print(f\"   Vector DB: {vector_docs} unique documents\")\n",
    "    print(f\"   Documents table: {table_docs} documents\")\n",
    "    \n",
    "    if vector_docs == table_docs:\n",
    "        print(f\"\\n‚úÖ GOOD: Both tables have same document count\")\n",
    "    elif table_docs < vector_docs:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: Documents table missing {vector_docs - table_docs} documents\")\n",
    "        print(f\"   Preprocessing may not be creating documents table entries\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: Documents table has {table_docs - vector_docs} extra documents\")\n",
    "        print(f\"   Some documents may not have chunks\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9efea8",
   "metadata": {},
   "source": [
    "### 5.2: Missing Documents Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bedaaf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Missing Documents Analysis\n",
      "================================================================================\n",
      "\n",
      "‚úÖ All documents in vector DB exist in documents table\n",
      "‚úÖ All documents in vector DB exist in documents table\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Missing Documents Analysis\")\n",
    "\n",
    "# Find documents in vector DB but not in documents table\n",
    "query = \"\"\"\n",
    "WITH vector_docs AS (\n",
    "    SELECT DISTINCT cmetadata->>'document_id' as document_id\n",
    "    FROM langchain_pg_embedding\n",
    "),\n",
    "table_docs AS (\n",
    "    SELECT document_id\n",
    "    FROM documents\n",
    ")\n",
    "SELECT v.document_id\n",
    "FROM vector_docs v\n",
    "LEFT JOIN table_docs t ON v.document_id = t.document_id\n",
    "WHERE t.document_id IS NULL\n",
    "ORDER BY v.document_id;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    missing_df = run_query(query)\n",
    "    \n",
    "    if missing_df.empty:\n",
    "        print(\"‚úÖ All documents in vector DB exist in documents table\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Found {len(missing_df)} documents in vector DB but NOT in documents table:\\n\")\n",
    "        for idx, doc_id in enumerate(missing_df['document_id'], 1):\n",
    "            print(f\"  {idx:2d}. {doc_id}\")\n",
    "        \n",
    "        print(\"\\nüí° Action Required:\")\n",
    "        print(\"   - These documents were added before documents table was created\")\n",
    "        print(\"   - Need to backfill documents table with these entries\")\n",
    "        print(\"   - Or: Update preprocessing to create documents table entries for new uploads\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c09623",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7bbaab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä DATABASE STRUCTURE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üóÑÔ∏è VECTOR DB (langchain_pg_embedding):\n",
      "   - Total chunks: 6,242\n",
      "   - Unique documents: 57\n",
      "   - New format chunks: 5,475 (87.7%)\n",
      "   - Old format chunks: 767 (12.3%)\n",
      "\n",
      "üìã DOCUMENTS TABLE:\n",
      "   - Total documents: 61\n",
      "   - Sum of total_chunks: 6242\n",
      "   - Documents with 0 chunks: 4\n",
      "\n",
      "üîç CONSISTENCY:\n",
      "   - Vector DB unique docs: 57\n",
      "   - Documents table rows: 61\n",
      "   - Match: ‚ö†Ô∏è NO\n",
      "    \n",
      "\n",
      "================================================================================\n",
      "üí° RECOMMENDATIONS FOR PREPROCESSING UPDATE\n",
      "================================================================================\n",
      "\n",
      "1. ‚ö†Ô∏è Still have 767 chunks with old format\n",
      "   ‚Üí Need to investigate and re-migrate if necessary\n",
      "\n",
      "2. ‚ö†Ô∏è Inconsistency: 57 docs in vector DB vs 61 in documents table\n",
      "   ‚Üí Preprocessing must insert into BOTH tables\n",
      "\n",
      "3. ‚ö†Ô∏è 4 documents have 0 chunks\n",
      "   ‚Üí These are likely exam PDFs that were never preprocessed\n",
      "   ‚Üí Preprocessing should handle these or mark as 'pending'\n",
      "\n",
      "\n",
      "üìã NEXT STEPS:\n",
      "\n",
      "1. Analyze src/preprocessing/upload_pipeline.py\n",
      "   - Check if it creates documents table entries\n",
      "   - Verify document_id generation logic\n",
      "   - Ensure total_chunks is updated after insertion\n",
      "\n",
      "2. Test upload endpoint with sample file\n",
      "   - Upload a new document\n",
      "   - Verify entry created in documents table\n",
      "   - Verify chunks inserted into vector DB\n",
      "   - Verify document_id format is correct\n",
      "\n",
      "3. Update preprocessing if needed\n",
      "   - Add DocumentIDGenerator\n",
      "   - Add documents table insertion\n",
      "   - Add consistency checks\n",
      "    \n",
      "\n",
      "üóÑÔ∏è VECTOR DB (langchain_pg_embedding):\n",
      "   - Total chunks: 6,242\n",
      "   - Unique documents: 57\n",
      "   - New format chunks: 5,475 (87.7%)\n",
      "   - Old format chunks: 767 (12.3%)\n",
      "\n",
      "üìã DOCUMENTS TABLE:\n",
      "   - Total documents: 61\n",
      "   - Sum of total_chunks: 6242\n",
      "   - Documents with 0 chunks: 4\n",
      "\n",
      "üîç CONSISTENCY:\n",
      "   - Vector DB unique docs: 57\n",
      "   - Documents table rows: 61\n",
      "   - Match: ‚ö†Ô∏è NO\n",
      "    \n",
      "\n",
      "================================================================================\n",
      "üí° RECOMMENDATIONS FOR PREPROCESSING UPDATE\n",
      "================================================================================\n",
      "\n",
      "1. ‚ö†Ô∏è Still have 767 chunks with old format\n",
      "   ‚Üí Need to investigate and re-migrate if necessary\n",
      "\n",
      "2. ‚ö†Ô∏è Inconsistency: 57 docs in vector DB vs 61 in documents table\n",
      "   ‚Üí Preprocessing must insert into BOTH tables\n",
      "\n",
      "3. ‚ö†Ô∏è 4 documents have 0 chunks\n",
      "   ‚Üí These are likely exam PDFs that were never preprocessed\n",
      "   ‚Üí Preprocessing should handle these or mark as 'pending'\n",
      "\n",
      "\n",
      "üìã NEXT STEPS:\n",
      "\n",
      "1. Analyze src/preprocessing/upload_pipeline.py\n",
      "   - Check if it creates documents table entries\n",
      "   - Verify document_id generation logic\n",
      "   - Ensure total_chunks is updated after insertion\n",
      "\n",
      "2. Test upload endpoint with sample file\n",
      "   - Upload a new document\n",
      "   - Verify entry created in documents table\n",
      "   - Verify chunks inserted into vector DB\n",
      "   - Verify document_id format is correct\n",
      "\n",
      "3. Update preprocessing if needed\n",
      "   - Add DocumentIDGenerator\n",
      "   - Add documents table insertion\n",
      "   - Add consistency checks\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DATABASE STRUCTURE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Gather all stats\n",
    "try:\n",
    "    # Vector DB stats\n",
    "    vector_stats = run_query(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_chunks,\n",
    "            COUNT(DISTINCT cmetadata->>'document_id') as unique_documents,\n",
    "            COUNT(CASE WHEN cmetadata->>'document_id' ~ '^(LUA|ND|TT|QD|FORM|TEMPLATE|EXAM)-' THEN 1 END) as new_format_chunks,\n",
    "            COUNT(CASE WHEN cmetadata->>'document_id' LIKE '%untitled%' THEN 1 END) as old_format_chunks\n",
    "        FROM langchain_pg_embedding;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Documents table stats\n",
    "    docs_stats = run_query(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_documents,\n",
    "            SUM(total_chunks) as sum_total_chunks,\n",
    "            COUNT(CASE WHEN total_chunks = 0 THEN 1 END) as docs_with_zero_chunks\n",
    "        FROM documents;\n",
    "    \"\"\")\n",
    "    \n",
    "    vs = vector_stats.iloc[0]\n",
    "    ds = docs_stats.iloc[0]\n",
    "    \n",
    "    print(f\"\"\"\n",
    "üóÑÔ∏è VECTOR DB (langchain_pg_embedding):\n",
    "   - Total chunks: {vs['total_chunks']:,}\n",
    "   - Unique documents: {vs['unique_documents']}\n",
    "   - New format chunks: {vs['new_format_chunks']:,} ({vs['new_format_chunks']/vs['total_chunks']*100:.1f}%)\n",
    "   - Old format chunks: {vs['old_format_chunks']:,} ({vs['old_format_chunks']/vs['total_chunks']*100:.1f}%)\n",
    "\n",
    "üìã DOCUMENTS TABLE:\n",
    "   - Total documents: {ds['total_documents']}\n",
    "   - Sum of total_chunks: {ds['sum_total_chunks']}\n",
    "   - Documents with 0 chunks: {ds['docs_with_zero_chunks']}\n",
    "\n",
    "üîç CONSISTENCY:\n",
    "   - Vector DB unique docs: {vs['unique_documents']}\n",
    "   - Documents table rows: {ds['total_documents']}\n",
    "   - Match: {'‚úÖ YES' if vs['unique_documents'] == ds['total_documents'] else '‚ö†Ô∏è NO'}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üí° RECOMMENDATIONS FOR PREPROCESSING UPDATE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    if vs['old_format_chunks'] > 0:\n",
    "        print(f\"1. ‚ö†Ô∏è Still have {vs['old_format_chunks']} chunks with old format\")\n",
    "        print(\"   ‚Üí Need to investigate and re-migrate if necessary\\n\")\n",
    "    else:\n",
    "        print(\"1. ‚úÖ All chunks use new format\\n\")\n",
    "    \n",
    "    if vs['unique_documents'] != ds['total_documents']:\n",
    "        print(f\"2. ‚ö†Ô∏è Inconsistency: {vs['unique_documents']} docs in vector DB vs {ds['total_documents']} in documents table\")\n",
    "        print(\"   ‚Üí Preprocessing must insert into BOTH tables\\n\")\n",
    "    else:\n",
    "        print(\"2. ‚úÖ Document count consistent\\n\")\n",
    "    \n",
    "    if ds['docs_with_zero_chunks'] > 0:\n",
    "        print(f\"3. ‚ö†Ô∏è {ds['docs_with_zero_chunks']} documents have 0 chunks\")\n",
    "        print(\"   ‚Üí These are likely exam PDFs that were never preprocessed\")\n",
    "        print(\"   ‚Üí Preprocessing should handle these or mark as 'pending'\\n\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "üìã NEXT STEPS:\n",
    "\n",
    "1. Analyze src/preprocessing/upload_pipeline.py\n",
    "   - Check if it creates documents table entries\n",
    "   - Verify document_id generation logic\n",
    "   - Ensure total_chunks is updated after insertion\n",
    "\n",
    "2. Test upload endpoint with sample file\n",
    "   - Upload a new document\n",
    "   - Verify entry created in documents table\n",
    "   - Verify chunks inserted into vector DB\n",
    "   - Verify document_id format is correct\n",
    "\n",
    "3. Update preprocessing if needed\n",
    "   - Add DocumentIDGenerator\n",
    "   - Add documents table insertion\n",
    "   - Add consistency checks\n",
    "    \"\"\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating summary: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad90d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "**Key Findings:**\n",
    "- Vector DB structure v√† data after migration\n",
    "- Documents table structure v√† consistency v·ªõi vector DB\n",
    "- Old vs new format distribution\n",
    "- Metadata fields availability\n",
    "\n",
    "**Next Notebook:**\n",
    "- Analyze preprocessing pipeline (`src/preprocessing/`)\n",
    "- Identify what needs to be updated\n",
    "- Test upload endpoint\n",
    "\n",
    "**Related Files:**\n",
    "- Migration notebook: `notebooks/migration/document-structure-migration.ipynb`\n",
    "- Update plan: `documents/migration/POST_MIGRATION_UPDATE_PLAN.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
