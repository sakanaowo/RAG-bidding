{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42c6e5a",
   "metadata": {},
   "source": [
    "# Fix Metadata - Update output_file paths\n",
    "\n",
    "Cáº­p nháº­t `output_file` trong metadata tá»« `data/processed_enriched/chunks/` â†’ `data/processed/chunks/`\n",
    "\n",
    "Folder `processed_enriched` Ä‘Ã£ bá»‹ xÃ³a, chunks thá»±c táº¿ náº±m á»Ÿ `data/processed/chunks/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61848f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Metadata directory: ../data/processed/metadata\n",
      "ðŸ’¾ Backup directory: ../data/processed/metadata_backup\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "# Paths\n",
    "METADATA_DIR = Path(\"../data/processed/metadata\")\n",
    "BACKUP_DIR = Path(\"../data/processed/metadata_backup\")\n",
    "\n",
    "# Create backup directory\n",
    "BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Metadata directory: {METADATA_DIR}\")\n",
    "print(f\"ðŸ’¾ Backup directory: {BACKUP_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e954509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 63 metadata files\n",
      "\n",
      "ðŸ“„ Sample file: 6._Máº«u_sá»‘_6C_E-TVCN_tÆ°_váº¥n_cÃ¡_nhÃ¢n.json\n",
      "ðŸ“‹ Current structure:\n",
      "{\n",
      "  \"source_file\": \"data/raw/Ho so moi thau/6. TÆ° váº¥n/6. Máº«u sá»‘ 6C_E-TVCN tÆ° váº¥n cÃ¡ nhÃ¢n.docx\",\n",
      "  \"document_type\": \"bidding\",\n",
      "  \"category\": \"6. TÆ° váº¥n\",\n",
      "  \"chunk_count\": 15,\n",
      "  \"output_file\": \"data/processed_enriched/chunks/6._Máº«u_sá»‘_6C_E-TVCN_tÆ°_váº¥n_cÃ¡_nhÃ¢n.jsonl\",\n",
      "  \"processed_at\": \"2025-11-04T12:13:35.519761\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Scan all metadata files\n",
    "metadata_files = list(METADATA_DIR.glob(\"*.json\"))\n",
    "\n",
    "print(f\"âœ… Found {len(metadata_files)} metadata files\\n\")\n",
    "\n",
    "# Sample one file to check current structure\n",
    "if metadata_files:\n",
    "    sample = metadata_files[0]\n",
    "    with open(sample, 'r', encoding='utf-8') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    print(f\"ðŸ“„ Sample file: {sample.name}\")\n",
    "    print(f\"ðŸ“‹ Current structure:\")\n",
    "    print(json.dumps(sample_data, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45eb50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… Fixed 63 metadata files\n",
      "ðŸ’¾ Backups saved to: ../data/processed/metadata_backup\n"
     ]
    }
   ],
   "source": [
    "# Fix all metadata output_file paths\n",
    "fixed_count = 0\n",
    "errors = []\n",
    "\n",
    "for meta_file in sorted(METADATA_DIR.glob(\"*.json\")):\n",
    "    try:\n",
    "        # Load metadata\n",
    "        with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Backup original\n",
    "        backup_path = BACKUP_DIR / meta_file.name\n",
    "        with open(backup_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Fix output_file path if it contains processed_enriched\n",
    "        if \"output_file\" in metadata:\n",
    "            old_path = metadata[\"output_file\"]\n",
    "            if \"processed_enriched\" in old_path:\n",
    "                new_path = old_path.replace(\n",
    "                    \"data/processed_enriched/chunks/\",\n",
    "                    \"data/processed/chunks/\"\n",
    "                )\n",
    "                metadata[\"output_file\"] = new_path\n",
    "                \n",
    "                # Verify chunk file exists\n",
    "                chunk_path = Path(\"..\") / new_path\n",
    "                if not chunk_path.exists():\n",
    "                    print(f\"âš ï¸  {meta_file.name}: Chunk file not found at {new_path}\")\n",
    "                \n",
    "                # Save updated metadata\n",
    "                with open(meta_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                fixed_count += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        errors.append((meta_file.name, str(e)))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… Fixed {fixed_count} metadata files\")\n",
    "print(f\"ðŸ’¾ Backups saved to: {BACKUP_DIR}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nâŒ Errors encountered: {len(errors)}\")\n",
    "    for name, err in errors[:5]:  # Show first 5 errors\n",
    "        print(f\"  - {name}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e721c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Verifying updated metadata files...\n",
      "\n",
      "ðŸ“„ 0._Lá»i_vÄƒn_thÃ´ng_tÆ°.json\n",
      "   source_file: data/raw/Thong tu/0. Lá»i vÄƒn thÃ´ng tÆ°.docx\n",
      "   output_file: data/processed/chunks/0._Lá»i_vÄƒn_thÃ´ng_tÆ°.jsonl\n",
      "   âœ… Chunk file exists: True\n",
      "\n",
      "ðŸ“„ 00._Quyáº¿t_Ä‘á»‹nh_ThÃ´ng_tÆ°.json\n",
      "   source_file: data/raw/Thong tu/00. Quyáº¿t Ä‘á»‹nh ThÃ´ng tÆ°.docx\n",
      "   output_file: data/processed/chunks/00._Quyáº¿t_Ä‘á»‹nh_ThÃ´ng_tÆ°.jsonl\n",
      "   âœ… Chunk file exists: True\n",
      "\n",
      "ðŸ“„ 01._Phá»¥_lá»¥c.json\n",
      "   source_file: data/raw/Ho so moi thau/01. Phá»¥ lá»¥c.doc\n",
      "   output_file: data/processed/chunks/01._Phá»¥_lá»¥c.jsonl\n",
      "   âœ… Chunk file exists: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify a few updated files\n",
    "print(\"ðŸ“‹ Verifying updated metadata files...\\n\")\n",
    "\n",
    "for meta_file in sorted(METADATA_DIR.glob(\"*.json\"))[:3]:  # Check first 3\n",
    "    with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"ðŸ“„ {meta_file.name}\")\n",
    "    print(f\"   source_file: {metadata.get('source_file')}\")\n",
    "    print(f\"   output_file: {metadata.get('output_file')}\")\n",
    "    \n",
    "    # Check if chunk file exists\n",
    "    chunk_path = Path(\"..\") / metadata.get('output_file', '')\n",
    "    status = \"âœ…\" if chunk_path.exists() else \"âŒ\"\n",
    "    print(f\"   {status} Chunk file exists: {chunk_path.exists()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9845015",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: Build chunk_id â†’ source_file Mapping\n",
    "\n",
    "Äá»c táº¥t cáº£ chunk files vÃ  táº¡o mapping tá»« `chunk_id` â†’ `source_file` Ä‘á»ƒ backfill vÃ o database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d7ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¨ Building chunk_id â†’ source_file mapping...\n",
      "\n",
      "  Processed 10/63 files, 443 chunks so far...\n",
      "  Processed 20/63 files, 516 chunks so far...\n",
      "  Processed 30/63 files, 1214 chunks so far...\n",
      "  Processed 40/63 files, 1738 chunks so far...\n",
      "  Processed 50/63 files, 2390 chunks so far...\n",
      "  Processed 60/63 files, 3657 chunks so far...\n",
      "\n",
      "============================================================\n",
      "âœ… Processed 63 metadata files\n",
      "âœ… Built mapping for 4512 chunks\n",
      "ðŸ“Š Average: 71 chunks per file\n"
     ]
    }
   ],
   "source": [
    "# Build chunk_id â†’ source_file mapping\n",
    "print(\"ðŸ”¨ Building chunk_id â†’ source_file mapping...\\n\")\n",
    "\n",
    "mapping = {}  # {chunk_id: source_file}\n",
    "processed_files = 0\n",
    "total_chunks = 0\n",
    "errors_phase2 = []\n",
    "\n",
    "for meta_file in sorted(METADATA_DIR.glob(\"*.json\")):\n",
    "    try:\n",
    "        # Load metadata\n",
    "        with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        source_file = metadata.get(\"source_file\")\n",
    "        chunk_file_path = Path(\"..\") / metadata.get(\"output_file\", \"\")\n",
    "        \n",
    "        if not chunk_file_path.exists():\n",
    "            errors_phase2.append((meta_file.name, \"Chunk file not found\"))\n",
    "            continue\n",
    "        \n",
    "        # Read all chunks from JSONL file\n",
    "        with open(chunk_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    chunk = json.loads(line)\n",
    "                    chunk_id = chunk.get(\"chunk_id\")\n",
    "                    \n",
    "                    if chunk_id:\n",
    "                        mapping[chunk_id] = source_file\n",
    "                        total_chunks += 1\n",
    "                except json.JSONDecodeError as e:\n",
    "                    errors_phase2.append((meta_file.name, f\"Line {line_num}: Invalid JSON\"))\n",
    "        \n",
    "        processed_files += 1\n",
    "        if processed_files % 10 == 0:\n",
    "            print(f\"  Processed {processed_files}/63 files, {total_chunks} chunks so far...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        errors_phase2.append((meta_file.name, str(e)))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… Processed {processed_files} metadata files\")\n",
    "print(f\"âœ… Built mapping for {total_chunks} chunks\")\n",
    "print(f\"ðŸ“Š Average: {total_chunks // processed_files if processed_files > 0 else 0} chunks per file\")\n",
    "\n",
    "if errors_phase2:\n",
    "    print(f\"\\nâš ï¸  Errors: {len(errors_phase2)}\")\n",
    "    for name, err in errors_phase2[:5]:\n",
    "        print(f\"  - {name}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f160861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving mapping to ../data/processed/chunk_to_source_mapping.json...\n",
      "âœ… Saved 1722 mappings\n",
      "ðŸ“Š File size: 0.17 MB\n",
      "\n",
      "ðŸ“‹ Sample mappings (first 5):\n",
      "  1. circular_untitled_dieu_0000...\n",
      "     â†’ data/raw/Thong tu/00. Quyáº¿t Ä‘á»‹nh ThÃ´ng tÆ°.docx\n",
      "  2. circular_untitled_dieu_0001...\n",
      "     â†’ data/raw/Thong tu/0. Lá»i vÄƒn thÃ´ng tÆ°.docx\n",
      "  3. circular_untitled_khoan_0002_merged...\n",
      "     â†’ data/raw/Thong tu/0. Lá»i vÄƒn thÃ´ng tÆ°.docx\n",
      "  4. circular_untitled_khoan_0004...\n",
      "     â†’ data/raw/Thong tu/00. Quyáº¿t Ä‘á»‹nh ThÃ´ng tÆ°.docx\n",
      "  5. circular_untitled_khoan_0005...\n",
      "     â†’ data/raw/Thong tu/0. Lá»i vÄƒn thÃ´ng tÆ°.docx\n"
     ]
    }
   ],
   "source": [
    "# Save mapping to file\n",
    "MAPPING_OUTPUT = Path(\"../data/processed/chunk_to_source_mapping.json\")\n",
    "\n",
    "print(f\"ðŸ’¾ Saving mapping to {MAPPING_OUTPUT}...\")\n",
    "\n",
    "with open(MAPPING_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "    json.dump(mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Show file size\n",
    "file_size = MAPPING_OUTPUT.stat().st_size / (1024 * 1024)  # MB\n",
    "print(f\"âœ… Saved {len(mapping)} mappings\")\n",
    "print(f\"ðŸ“Š File size: {file_size:.2f} MB\")\n",
    "\n",
    "# Show sample mappings\n",
    "print(f\"\\nðŸ“‹ Sample mappings (first 5):\")\n",
    "for i, (chunk_id, source_file) in enumerate(list(mapping.items())[:5]):\n",
    "    print(f\"  {i+1}. {chunk_id[:50]}...\")\n",
    "    print(f\"     â†’ {source_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700a8f7",
   "metadata": {},
   "source": [
    "### âš ï¸ PHÃT HIá»†N Váº¤N Äá»€: Duplicate chunk_ids!\n",
    "\n",
    "**Váº¥n Ä‘á»:** \n",
    "- Xá»­ lÃ½ 4512 chunks nhÆ°ng chá»‰ lÆ°u Ä‘Æ°á»£c 1722 mappings (máº¥t 62%)\n",
    "- NguyÃªn nhÃ¢n: Nhiá»u files cÃ³ cÃ¹ng chunk_id â†’ Dictionary ghi Ä‘Ã¨\n",
    "\n",
    "**VÃ­ dá»¥:**\n",
    "```\n",
    "circular_untitled_khoan_0005 xuáº¥t hiá»‡n á»Ÿ:\n",
    "- \"0. Lá»i vÄƒn thÃ´ng tÆ°.docx\"\n",
    "- \"00. Quyáº¿t Ä‘á»‹nh ThÃ´ng tÆ°.docx\"\n",
    "â†’ Dictionary chá»‰ giá»¯ entry cuá»‘i!\n",
    "```\n",
    "\n",
    "**Giáº£i phÃ¡p:** Cáº§n approach khÃ¡c - query database trá»±c tiáº¿p hoáº·c dÃ¹ng composite key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450215b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2B: Fix - Batch Update vá»›i Document Context\n",
    "\n",
    "Thay vÃ¬ dÃ¹ng chunk_id mapping (bá»‹ duplicate), ta sáº½:\n",
    "1. Group chunks theo document_id + document_type\n",
    "2. Update tá»«ng batch vá»›i WHERE clause chÃ­nh xÃ¡c\n",
    "3. DÃ¹ng thÃ´ng tin tá»« metadata Ä‘á»ƒ match chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "900b6a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Analyzing chunk patterns to find distinguishing features...\n",
      "\n",
      "ðŸ“„ 6._Máº«u_sá»‘_6C_E-TVCN_tÆ°_váº¥n_cÃ¡_nhÃ¢n.json\n",
      "   source_file: data/raw/Ho so moi thau/6. TÆ° váº¥n/6. Máº«u sá»‘ 6C_E-TVCN tÆ° váº¥n cÃ¡ nhÃ¢n.docx\n",
      "   document_id: bidding_untitled\n",
      "   chunk_id: bidding_untitled_form_0000\n",
      "   chunk_index: 0\n",
      "   content preview: [Section: Máº«u sá»‘ 01 (webform trÃªn Há»‡ thá»‘ng)]\n",
      "\n",
      "Máº«u sá»‘ 01 (webform trÃªn Há»‡ thá»‘ng)\n",
      "...\n",
      "\n",
      "ðŸ“„ 05._Máº«u_BÃ¡o_cÃ¡o_Ä‘áº¥u_tháº§u.json\n",
      "   source_file: data/raw/Ho so moi thau/5. Máº«u bÃ¡o cÃ¡o tÃ¬nh hÃ¬nh thá»±c hiá»‡n hoáº¡t Ä‘á»™ng Ä‘áº¥u tháº§u/05. Máº«u BÃ¡o cÃ¡o Ä‘áº¥u tháº§u.docx\n",
      "   document_id: bidding_untitled\n",
      "   chunk_id: bidding_untitled_section_0000\n",
      "   chunk_index: 0\n",
      "   content preview: [Section: I. Tá»•ng há»£p sá»‘ liá»‡u vá» káº¿t quáº£ thá»±c hiá»‡n hoáº¡t Ä‘á»™ng Ä‘áº¥u tháº§u lá»±a chá»n n...\n",
      "\n",
      "ðŸ“„ 14A._Máº«u_BCÄG_PTV_HH_XL_hop_hop_TBYT_CGTT._quy_trÃ¬nh_1_1_tui.json\n",
      "   source_file: data/raw/Mau bao cao/14. BÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡/14A. Máº«u BCÄG PTV HH XL hop hop TBYT CGTT. quy trÃ¬nh 1_1 tui.docx\n",
      "   document_id: bidding_untitled\n",
      "   chunk_id: bidding_untitled_section_0000\n",
      "   chunk_index: 0\n",
      "   content preview: [Section: I. THÃ”NG TIN CÆ  Báº¢N]\n",
      "\n",
      "1. Giá»›i thiá»‡u chung vá» dá»± Ã¡n/dá»± toÃ¡n mua sáº¯m, gÃ³...\n",
      "\n",
      "ðŸ“„ 3._Máº«u_sá»‘_3C_E-HSMST_XÃ¢y_láº¯p_sÆ¡_tuyá»ƒn.json\n",
      "   source_file: data/raw/Ho so moi thau/3. XÃ¢y láº¯p/3. Máº«u sá»‘ 3C_E-HSMST XÃ¢y láº¯p sÆ¡ tuyá»ƒn.docx\n",
      "   document_id: bidding_untitled\n",
      "   chunk_id: bidding_untitled_form_0000\n",
      "   chunk_index: 0\n",
      "   content preview: [Section: Máº«u sá»‘ 01 (Webform trÃªn Há»‡ thá»‘ng)]\n",
      "\n",
      "Máº«u sá»‘ 01 (Webform trÃªn Há»‡ thá»‘ng)\n",
      "...\n",
      "\n",
      "ðŸ“„ 4._Máº«u_sá»‘_4B_E-HSMT_hÃ ng_hÃ³a_2_tÃºi.json\n",
      "   source_file: data/raw/Ho so moi thau/4. HÃ ng hÃ³a/4. Máº«u sá»‘ 4B E-HSMT hÃ ng hÃ³a 2 tÃºi.docx\n",
      "   document_id: bidding_untitled\n",
      "   chunk_id: bidding_untitled_form_0000\n",
      "   chunk_index: 0\n",
      "   content preview: [Section: Máº«u sá»‘ 01A (webform trÃªn Há»‡ thá»‘ng)]\n",
      "\n",
      "Máº«u sá»‘ 01A (webform trÃªn Há»‡ thá»‘ng...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach má»›i: TÃ¬m pattern trong chunk content Ä‘á»ƒ match vá»›i source_file\n",
    "# Database cÃ³ cáº£ content, ta cÃ³ thá»ƒ dÃ¹ng unique patterns Ä‘á»ƒ identify chunks\n",
    "\n",
    "# First, phÃ¢n tÃ­ch xem chunks tá»« files khÃ¡c nhau cÃ³ gÃ¬ khÃ¡c biá»‡t\n",
    "print(\"ðŸ” Analyzing chunk patterns to find distinguishing features...\\n\")\n",
    "\n",
    "# Load a few chunks from different files to see structure\n",
    "sample_files = list(METADATA_DIR.glob(\"*.json\"))[:5]\n",
    "\n",
    "for meta_file in sample_files:\n",
    "    with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    chunk_file = Path(\"..\") / metadata.get(\"output_file\", \"\")\n",
    "    if chunk_file.exists():\n",
    "        # Read first chunk\n",
    "        with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            if first_line:\n",
    "                chunk = json.loads(first_line)\n",
    "                print(f\"ðŸ“„ {meta_file.name}\")\n",
    "                print(f\"   source_file: {metadata.get('source_file')}\")\n",
    "                print(f\"   document_id: {chunk.get('document_id')}\")\n",
    "                print(f\"   chunk_id: {chunk.get('chunk_id')}\")\n",
    "                print(f\"   chunk_index: {chunk.get('chunk_index')}\")\n",
    "                print(f\"   content preview: {chunk.get('content', '')[:80]}...\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d63b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giáº£i phÃ¡p tá»‘t nháº¥t: DÃ¹ng content hash hoáº·c unique content snippet\n",
    "# VÃ¬ database cÃ³ cáº£ embedding vector, ta cÃ³ thá»ƒ dÃ¹ng content Ä‘á»ƒ match\n",
    "\n",
    "# Build mapping: (document_type, content_hash) â†’ source_file\n",
    "print(\"ðŸ”¨ Building content-based mapping...\\n\")\n",
    "\n",
    "import hashlib\n",
    "\n",
    "content_mapping = []  # [(document_type, content_snippet, source_file, chunk_index)]\n",
    "\n",
    "for meta_file in sorted(METADATA_DIR.glob(\"*.json\")):\n",
    "    metadata = json.load(open(meta_file))\n",
    "    source_file = metadata.get(\"source_file\")\n",
    "    chunk_file = Path(\"..\") / metadata.get(\"output_file\", \"\")\n",
    "    \n",
    "    if not chunk_file.exists():\n",
    "        continue\n",
    "    \n",
    "    # Read chunks and create unique identifiers\n",
    "    with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            chunk = json.loads(line)\n",
    "            document_type = chunk.get(\"document_type\")\n",
    "            content = chunk.get(\"content\", \"\")\n",
    "            chunk_index = chunk.get(\"chunk_index\")\n",
    "            \n",
    "            # Use first 200 chars of content as identifier (more reliable than chunk_id)\n",
    "            content_snippet = content[:200] if content else \"\"\n",
    "            \n",
    "            content_mapping.append({\n",
    "                \"document_type\": document_type,\n",
    "                \"content_snippet\": content_snippet,\n",
    "                \"source_file\": source_file,\n",
    "                \"chunk_index\": chunk_index\n",
    "            })\n",
    "\n",
    "print(f\"âœ… Built content mapping for {len(content_mapping)} chunks\")\n",
    "print(f\"ðŸ“Š This covers all {len(content_mapping)} chunks uniquely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5db72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 3: Backfill source_file vÃ o Database\n",
    "\n",
    "Update database chunks vá»›i source_file tá»« mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup database connection\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import asyncio\n",
    "\n",
    "# Database connection string\n",
    "DATABASE_URL = \"postgresql+asyncpg://sakana@localhost/rag_bidding_v2\"\n",
    "\n",
    "# Create async engine\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "\n",
    "print(\"âœ… Database connection setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backfill source_file vÃ o database\n",
    "async def backfill_source_file():\n",
    "    \"\"\"Update database chunks vá»›i source_file tá»« mapping\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Starting database backfill...\\n\")\n",
    "    \n",
    "    # Load mapping\n",
    "    with open(MAPPING_OUTPUT, 'r', encoding='utf-8') as f:\n",
    "        mapping = json.load(f)\n",
    "    \n",
    "    print(f\"ðŸ“Š Loaded {len(mapping)} chunk_id â†’ source_file mappings\")\n",
    "    \n",
    "    # Update database\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        updated = 0\n",
    "        skipped = 0\n",
    "        errors_db = []\n",
    "        \n",
    "        # Batch update for performance\n",
    "        batch_size = 100\n",
    "        batch = []\n",
    "        \n",
    "        for chunk_id, source_file in mapping.items():\n",
    "            batch.append((chunk_id, source_file))\n",
    "            \n",
    "            if len(batch) >= batch_size:\n",
    "                try:\n",
    "                    # Update batch\n",
    "                    for cid, sf in batch:\n",
    "                        await session.execute(\n",
    "                            text(\"\"\"\n",
    "                                UPDATE langchain_pg_embedding\n",
    "                                SET cmetadata = jsonb_set(\n",
    "                                    cmetadata, \n",
    "                                    '{source_file}', \n",
    "                                    to_jsonb(:source_file::text)\n",
    "                                )\n",
    "                                WHERE cmetadata->>'chunk_id' = :chunk_id\n",
    "                            \"\"\"),\n",
    "                            {\"chunk_id\": cid, \"source_file\": sf}\n",
    "                        )\n",
    "                    \n",
    "                    await session.commit()\n",
    "                    updated += len(batch)\n",
    "                    print(f\"  âœ… Updated {updated}/{len(mapping)} chunks...\")\n",
    "                    batch = []\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    errors_db.append(str(e))\n",
    "                    await session.rollback()\n",
    "                    batch = []\n",
    "        \n",
    "        # Update remaining batch\n",
    "        if batch:\n",
    "            try:\n",
    "                for cid, sf in batch:\n",
    "                    await session.execute(\n",
    "                        text(\"\"\"\n",
    "                            UPDATE langchain_pg_embedding\n",
    "                            SET cmetadata = jsonb_set(\n",
    "                                cmetadata, \n",
    "                                '{source_file}', \n",
    "                                to_jsonb(:source_file::text)\n",
    "                            )\n",
    "                            WHERE cmetadata->>'chunk_id' = :chunk_id\n",
    "                        \"\"\"),\n",
    "                        {\"chunk_id\": cid, \"source_file\": sf}\n",
    "                    )\n",
    "                \n",
    "                await session.commit()\n",
    "                updated += len(batch)\n",
    "            except Exception as e:\n",
    "                errors_db.append(str(e))\n",
    "                await session.rollback()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"âœ… Backfill complete!\")\n",
    "        print(f\"ðŸ“Š Updated: {updated} chunks\")\n",
    "        \n",
    "        if errors_db:\n",
    "            print(f\"âš ï¸  Errors: {len(errors_db)}\")\n",
    "            for err in errors_db[:3]:\n",
    "                print(f\"  - {err}\")\n",
    "\n",
    "# Run backfill\n",
    "await backfill_source_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a21bbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 4: Verify Backfill\n",
    "\n",
    "Kiá»ƒm tra xem source_file Ä‘Ã£ Ä‘Æ°á»£c backfill vÃ o database chÆ°a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify source_file in database\n",
    "async def verify_backfill():\n",
    "    \"\"\"Kiá»ƒm tra source_file Ä‘Ã£ Ä‘Æ°á»£c populate vÃ o database\"\"\"\n",
    "    \n",
    "    print(\"ðŸ” Verifying database backfill...\\n\")\n",
    "    \n",
    "    async with AsyncSessionLocal() as session:\n",
    "        # Count chunks with source_file\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) FILTER (WHERE cmetadata->>'source_file' IS NOT NULL) as with_source,\n",
    "                COUNT(*) FILTER (WHERE cmetadata->>'source_file' IS NULL) as without_source,\n",
    "                COUNT(*) as total\n",
    "            FROM langchain_pg_embedding\n",
    "        \"\"\"))\n",
    "        \n",
    "        row = result.fetchone()\n",
    "        print(f\"ðŸ“Š Chunks with source_file: {row[0]}\")\n",
    "        print(f\"ðŸ“Š Chunks without source_file: {row[1]}\")\n",
    "        print(f\"ðŸ“Š Total chunks: {row[2]}\")\n",
    "        print(f\"ðŸ“Š Coverage: {row[0]/row[2]*100:.1f}%\\n\")\n",
    "        \n",
    "        # Show breakdown by document_id\n",
    "        result = await session.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                cmetadata->>'document_id' as doc_id,\n",
    "                cmetadata->>'source_file' as source,\n",
    "                COUNT(*) as chunks\n",
    "            FROM langchain_pg_embedding\n",
    "            GROUP BY cmetadata->>'document_id', cmetadata->>'source_file'\n",
    "            ORDER BY doc_id, source\n",
    "            LIMIT 20\n",
    "        \"\"\"))\n",
    "        \n",
    "        print(\"ðŸ“‹ Sample breakdown by document_id and source_file:\\n\")\n",
    "        for row in result.fetchall():\n",
    "            doc_id = row[0][:30] + \"...\" if len(row[0]) > 30 else row[0]\n",
    "            source = row[1]\n",
    "            if source:\n",
    "                source_name = source.split('/')[-1][:40]\n",
    "            else:\n",
    "                source_name = \"(NULL)\"\n",
    "            print(f\"  {doc_id:35} | {source_name:45} | {row[2]:4} chunks\")\n",
    "\n",
    "# Run verification\n",
    "await verify_backfill()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
