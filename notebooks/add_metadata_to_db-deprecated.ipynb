{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43311a0",
   "metadata": {},
   "source": [
    "# Th√™m Metadata v√†o Documents ƒë√£ Embedded\n",
    "\n",
    "## M·ª•c ti√™u\n",
    "Th√™m `status` (active/expired) v√† `valid_until` v√†o metadata c·ªßa documents **KH√îNG C·∫¶N re-embedding**.\n",
    "\n",
    "## ‚ö†Ô∏è L∆ØU √ù QUAN TR·ªåNG\n",
    "- **Ch·∫°y t·ª´ng cell m·ªôt** v√† ki·ªÉm tra k·∫øt qu·∫£\n",
    "- **KH√îNG ch·∫°y cell bulk update** cho ƒë·∫øn khi verify xong\n",
    "- Backup database tr∆∞·ªõc khi update (optional nh∆∞ng recommended)\n",
    "\n",
    "## Workflow\n",
    "1. ‚úÖ K·∫øt n·ªëi database v√† ki·ªÉm tra schema\n",
    "2. ‚úÖ Xem sample metadata hi·ªán t·∫°i\n",
    "3. ‚úÖ Test logic x√°c ƒë·ªãnh status tr√™n 1 document\n",
    "4. ‚úÖ Dry-run: Xem s·∫Ω update nh·ªØng g√¨ (KH√îNG th·ª±c s·ª± update)\n",
    "5. ‚è∏Ô∏è **PAUSE** - Review k·∫øt qu·∫£ dry-run\n",
    "6. ‚úÖ Bulk update (sau khi confirm)\n",
    "7. ‚úÖ Verify k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8401c7",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 1: Import Libraries v√† Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae4543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "üìä Database: localhost:5432/rag_bidding_v2\n",
      "üì¶ Collection: docs\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# Load config\n",
    "import sys\n",
    "sys.path.append('/home/sakana/Code/RAG-bidding')\n",
    "from src.config.models import settings\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìä Database: {settings.database_url.split('@')[1] if '@' in settings.database_url else 'hidden'}\")\n",
    "print(f\"üì¶ Collection: {settings.collection}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ae606",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 2: K·∫øt n·ªëi Database v√† Ki·ªÉm tra Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4ad37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connected successfully!\n",
      "\n",
      "üìä PGVector Tables:\n",
      "   - langchain_pg_collection\n",
      "   - langchain_pg_embedding\n",
      "\n",
      "üìã Columns in langchain_pg_embedding:\n",
      "   - id: character varying\n",
      "   - collection_id: uuid\n",
      "   - embedding: USER-DEFINED\n",
      "   - document: character varying\n",
      "   - cmetadata: jsonb\n"
     ]
    }
   ],
   "source": [
    "# K·∫øt n·ªëi database\n",
    "dsn = settings.database_url.replace(\"postgresql+psycopg\", \"postgresql\")\n",
    "\n",
    "try:\n",
    "    conn = psycopg.connect(dsn)\n",
    "    print(\"‚úÖ Database connected successfully!\")\n",
    "    \n",
    "    # Ki·ªÉm tra schema\n",
    "    with conn.cursor() as cur:\n",
    "        # 1. Ki·ªÉm tra tables\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_name LIKE '%langchain%'\n",
    "        \"\"\")\n",
    "        tables = cur.fetchall()\n",
    "        print(f\"\\nüìä PGVector Tables:\")\n",
    "        for table in tables:\n",
    "            print(f\"   - {table[0]}\")\n",
    "        \n",
    "        # 2. Ki·ªÉm tra columns trong langchain_pg_embedding\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT column_name, data_type \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = 'langchain_pg_embedding'\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\")\n",
    "        columns = cur.fetchall()\n",
    "        print(f\"\\nüìã Columns in langchain_pg_embedding:\")\n",
    "        for col_name, col_type in columns:\n",
    "            print(f\"   - {col_name}: {col_type}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection error: {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfece6",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 3: L·∫•y Collection UUID v√† ƒê·∫øm Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4963005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collection found: docs\n",
      "   UUID: b625f353-708f-41a4-8580-6e7523325aba\n",
      "\n",
      "üìä Total documents: 845\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    # Get collection UUID\n",
    "    cur.execute(\n",
    "        \"SELECT uuid FROM langchain_pg_collection WHERE name = %s\",\n",
    "        (settings.collection,)\n",
    "    )\n",
    "    result = cur.fetchone()\n",
    "    \n",
    "    if not result:\n",
    "        print(f\"‚ùå Collection '{settings.collection}' not found!\")\n",
    "        collection_uuid = None\n",
    "    else:\n",
    "        collection_uuid = result[0]\n",
    "        print(f\"‚úÖ Collection found: {settings.collection}\")\n",
    "        print(f\"   UUID: {collection_uuid}\")\n",
    "        \n",
    "        # Count documents\n",
    "        cur.execute(\n",
    "            \"SELECT COUNT(*) FROM langchain_pg_embedding WHERE collection_id = %s\",\n",
    "            (collection_uuid,)\n",
    "        )\n",
    "        doc_count = cur.fetchone()[0]\n",
    "        print(f\"\\nüìä Total documents: {doc_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940d675",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 4: Xem Sample Metadata Hi·ªán T·∫°i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d456701",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 4a: Ph√¢n t√≠ch chi ti·∫øt Metadata Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794a45a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ANALYZING METADATA STRUCTURE\n",
      "\n",
      "üìã All metadata fields found:\n",
      "   - char_count\n",
      "   - chunk_id\n",
      "   - chunk_level\n",
      "   - chunking_strategy\n",
      "   - chuong\n",
      "   - crawled_at\n",
      "   - dieu\n",
      "   - has_diem\n",
      "   - has_khoan\n",
      "   - hierarchy\n",
      "   - is_within_token_limit\n",
      "   - khoan\n",
      "   - parent_dieu\n",
      "   - quality_flags\n",
      "   - readability_score\n",
      "   - section\n",
      "   - semantic_tags\n",
      "   - source\n",
      "   - source_file\n",
      "   - status\n",
      "   - structure_score\n",
      "   - title\n",
      "   - token_count\n",
      "   - token_ratio\n",
      "   - url\n",
      "   - valid_until\n",
      "\n",
      "üìÇ Sample 'source' field (first 10):\n",
      "   1. thuvienphapluat.vn\n",
      "   2. thuvienphapluat.vn\n",
      "   3. thuvienphapluat.vn\n",
      "   4. thuvienphapluat.vn\n",
      "   5. thuvienphapluat.vn\n",
      "   6. thuvienphapluat.vn\n",
      "   7. thuvienphapluat.vn\n",
      "   8. thuvienphapluat.vn\n",
      "   9. thuvienphapluat.vn\n",
      "   10. thuvienphapluat.vn\n",
      "\n",
      "üìÑ Sample 'title' field (first 10):\n",
      "   1. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "   2. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "   3. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "   4. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "   5. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "   6. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "   7. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "   8. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "   9. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "   10. N·ªôi dung t·ª´ thuvienphapluat.vn\n",
      "\n",
      "============================================================\n",
      "üí° OBSERVATION:\n",
      "============================================================\n",
      "- Data comes from PDF files, NOT web-crawled documents\n",
      "- 'source' contains file paths\n",
      "- 'title' contains document titles\n",
      "- NO 'url' field available\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch chi ti·∫øt metadata structure\n",
    "print(\"üîç ANALYZING METADATA STRUCTURE\\n\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT cmetadata\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        LIMIT 20\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    \n",
    "    # Collect all metadata fields\n",
    "    all_fields = set()\n",
    "    source_examples = []\n",
    "    title_examples = []\n",
    "    \n",
    "    for (metadata,) in samples:\n",
    "        if metadata:\n",
    "            all_fields.update(metadata.keys())\n",
    "            if 'source' in metadata:\n",
    "                source_examples.append(metadata['source'])\n",
    "            if 'title' in metadata:\n",
    "                title_examples.append(metadata['title'])\n",
    "    \n",
    "    print(\"üìã All metadata fields found:\")\n",
    "    for field in sorted(all_fields):\n",
    "        print(f\"   - {field}\")\n",
    "    \n",
    "    print(\"\\nüìÇ Sample 'source' field (first 10):\")\n",
    "    for i, src in enumerate(source_examples[:10], 1):\n",
    "        print(f\"   {i}. {src}\")\n",
    "    \n",
    "    print(\"\\nüìÑ Sample 'title' field (first 10):\")\n",
    "    for i, title in enumerate(title_examples[:10], 1):\n",
    "        print(f\"   {i}. {title}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üí° OBSERVATION:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"- Data comes from PDF files, NOT web-crawled documents\")\n",
    "    print(\"- 'source' contains file paths\")\n",
    "    print(\"- 'title' contains document titles\")\n",
    "    print(\"- NO 'url' field available\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42190676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Sample Documents v√† Metadata:\n",
      "\n",
      "--- Document 1 ---\n",
      "ID: 8b4824ec-a6fa-4535-8a57-3f5f20b8650d\n",
      "Preview: H·ªåC VI·ªÜN C√îNG NGH·ªÜ B∆ØU CH√çNH VI·ªÑN TH√îNG \n",
      "---------ÔÇñÔÄ¶ÔÇó---------- \n",
      " \n",
      "KHOA C∆† B·∫¢N \n",
      " \n",
      " \n",
      "B√ÄI GI·∫¢NG \n",
      "T∆Ø T∆Ø...\n",
      "Metadata keys: ['page', 'title', 'author', 'source', 'cleaned', 'creator', 'moddate', 'producer', 'file_size', 'file_type', 'page_label', 'total_pages', 'creationdate']\n",
      "‚úÖ No status field (need to add)\n",
      "\n",
      "--- Document 2 ---\n",
      "ID: 664eb9bc-a2d6-4bd1-b2c8-441c679af8e0\n",
      "Preview: 1 \n",
      "M·ª§C L·ª§C \n",
      " \n",
      "L·ªúI N√ìI ƒê·∫¶U... \n",
      "Ch∆£∆°ng m·ªü ƒë·∫ßu: ƒê·ªêI T∆¢·ª¢NG, PH∆¢∆†NG PH√ÅP NGHI√äN C·ª®U V√Ä √ù NGHƒ®A H·ªåC \n",
      "T·∫¨P M...\n",
      "Metadata keys: ['page', 'title', 'author', 'source', 'cleaned', 'creator', 'moddate', 'producer', 'file_size', 'file_type', 'page_label', 'total_pages', 'creationdate']\n",
      "‚úÖ No status field (need to add)\n",
      "\n",
      "--- Document 3 ---\n",
      "ID: 4b94d8ee-2cca-4a5d-ab4c-308a7f026380\n",
      "Preview: TRI·ªÇN T∆¢ T∆¢·ªûNG H·ªí CH√ç MINH \n",
      " I. C∆° s·ªü h√¨nh th√†nh t∆£ t∆£·ªüng H·ªì Ch√≠ Minh ... \n",
      " 1.C∆° s·ªü kh√°ch quan \n",
      " 2.N...\n",
      "Metadata keys: ['page', 'title', 'author', 'source', 'cleaned', 'creator', 'moddate', 'producer', 'file_size', 'file_type', 'page_label', 'total_pages', 'creationdate']\n",
      "‚úÖ No status field (need to add)\n",
      "\n",
      "--- Document 4 ---\n",
      "ID: 9a8f84df-e257-4c7b-b261-a5360175bf85\n",
      "Preview: 2 \n",
      "5. Th·ªùi k·ª≥ t·ª´ 1945 - 1969: T∆£ t∆£·ªüng H·ªì Ch√≠ Minh ti·∫øp t·ª•c ph√°t tri·ªÉn, ho√†n \n",
      "thi·ªán... \n",
      "III. GI√Å TR·ªä...\n",
      "Metadata keys: ['page', 'title', 'author', 'source', 'cleaned', 'creator', 'moddate', 'producer', 'file_size', 'file_type', 'page_label', 'total_pages', 'creationdate']\n",
      "‚úÖ No status field (need to add)\n",
      "\n",
      "--- Document 5 ---\n",
      "ID: 64fc5dc5-7443-4873-8142-bb81b82e78d4\n",
      "Preview: 5. C√°ch m·∫°ng gi·∫£i ph√≥ng d√¢n t·ªôc c·∫ßn ƒë∆£·ª£c ti·∫øn h√†nh ch·ªß ƒë·ªông s√°ng t·∫°o v√† c√≥ kh·∫£ \n",
      "nƒÉng gi√†nh th·∫Øng l·ª£i...\n",
      "Metadata keys: ['page', 'title', 'author', 'source', 'cleaned', 'creator', 'moddate', 'producer', 'file_size', 'file_type', 'page_label', 'total_pages', 'creationdate']\n",
      "‚úÖ No status field (need to add)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# L·∫•y 5 sample documents\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            id, \n",
    "            LEFT(document, 100) as doc_preview,\n",
    "            cmetadata\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        LIMIT 5\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    \n",
    "    print(\"üìÑ Sample Documents v√† Metadata:\\n\")\n",
    "    for i, (doc_id, doc_preview, metadata) in enumerate(samples, 1):\n",
    "        print(f\"--- Document {i} ---\")\n",
    "        print(f\"ID: {doc_id}\")\n",
    "        print(f\"Preview: {doc_preview}...\")\n",
    "        print(f\"Metadata keys: {list(metadata.keys()) if metadata else 'None'}\")\n",
    "        \n",
    "        # Check if already has status\n",
    "        if metadata and 'status' in metadata:\n",
    "            print(f\"‚ö†Ô∏è  ALREADY HAS STATUS: {metadata['status']}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ No status field (need to add)\")\n",
    "        \n",
    "        # Show URL for verification\n",
    "        if metadata and 'url' in metadata:\n",
    "            print(f\"URL: {metadata['url']}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211338b0",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 5: Define Logic X√°c ƒë·ªãnh Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a81c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Functions defined successfully\n",
      "üìö Supports both:\n",
      "   - Legal documents (Lu·∫≠t: 5yr, Ngh·ªã ƒë·ªãnh/Th√¥ng t∆∞: 2yr)\n",
      "   - Educational materials (5yr)\n",
      "üîç Parses year from URL, filename, title, or creation date\n"
     ]
    }
   ],
   "source": [
    "def parse_year_from_source_or_title(metadata: dict) -> int | None:\n",
    "    \"\"\"\n",
    "    Extract year from various sources:\n",
    "    1. URL (for web-scraped legal docs): 'Luat-Dau-thau-2023-22-2023-QH15'\n",
    "    2. Source filename (for PDFs): 'T∆∞-t∆∞·ªüng-2016.pdf' or 'CNPM 2020 final.pdf'\n",
    "    3. Title field\n",
    "    4. Creation date\n",
    "    \"\"\"\n",
    "    # Priority 1: URL field (for legal documents)\n",
    "    url = metadata.get(\"url\", \"\")\n",
    "    if url:\n",
    "        # Pattern: Luat-Dau-thau-2023-22-2023-QH15\n",
    "        match = re.search(r'Luat.*-(\\d{4})-', url)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            if 1900 <= year <= 2030:\n",
    "                return year\n",
    "        \n",
    "        # Pattern: Nghi-dinh-214-2025-ND-CP\n",
    "        match = re.search(r'-(\\d{4})-', url)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            if 1900 <= year <= 2030:\n",
    "                return year\n",
    "    \n",
    "    # Priority 2: Source field (filename for PDFs)\n",
    "    source = metadata.get(\"source\", \"\")\n",
    "    if source:\n",
    "        # Pattern 1: -YYYY.pdf or _YYYY.pdf\n",
    "        match = re.search(r'[-_](\\d{4})\\.pdf', source)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            if 1900 <= year <= 2030:\n",
    "                return year\n",
    "        \n",
    "        # Pattern 2: space+YYYY+space/end (e.g., \"CNPM 2020 final.pdf\")\n",
    "        match = re.search(r'\\s(\\d{4})[\\s\\.]', source)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            if 1900 <= year <= 2030:\n",
    "                return year\n",
    "        \n",
    "        # Pattern 3: any 4-digit year in filename\n",
    "        match = re.search(r'\\b(19\\d{2}|20[0-3]\\d)\\b', source)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            return year\n",
    "    \n",
    "    # Priority 3: Title field\n",
    "    title = metadata.get(\"title\", \"\")\n",
    "    if title:\n",
    "        match = re.search(r'\\b(19\\d{2}|20[0-3]\\d)\\b', title)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    \n",
    "    # Priority 4: Creation date (D:20160101120000)\n",
    "    creationdate = metadata.get(\"creationdate\", \"\")\n",
    "    if creationdate:\n",
    "        match = re.search(r'D:(\\d{4})', creationdate)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def determine_status_and_validity(metadata: dict) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Determine document status and valid_until date.\n",
    "    \n",
    "    Rules:\n",
    "    - Legal documents (has 'url' field):\n",
    "      * Lu·∫≠t (Law): 5 years validity\n",
    "      * Ngh·ªã ƒë·ªãnh (Decree): 2 years validity\n",
    "      * Th√¥ng t∆∞ (Circular): 2 years validity\n",
    "    - Educational materials (PDF files): 5 years validity\n",
    "    \n",
    "    Returns:\n",
    "        (status, valid_until_iso_string)\n",
    "    \"\"\"\n",
    "    year = parse_year_from_source_or_title(metadata)\n",
    "    \n",
    "    if not year:\n",
    "        # No year found: mark as expired\n",
    "        return \"expired\", \"2024-01-01\"\n",
    "    \n",
    "    current_year = datetime.now().year\n",
    "    url = metadata.get(\"url\", \"\")\n",
    "    title = metadata.get(\"title\", \"\")\n",
    "    \n",
    "    # Determine document type and validity period\n",
    "    if url and (\"Luat\" in url or \"Lu·∫≠t\" in title):\n",
    "        # Law: 5 years validity\n",
    "        valid_until = datetime(year + 5, 12, 31)\n",
    "    elif url and (\"Nghi-dinh\" in url or \"Ngh·ªã ƒë·ªãnh\" in title):\n",
    "        # Decree: 2 years validity\n",
    "        valid_until = datetime(year + 2, 12, 31)\n",
    "    elif url and (\"Thong-tu\" in url or \"Th√¥ng t∆∞\" in title):\n",
    "        # Circular: 2 years validity\n",
    "        valid_until = datetime(year + 2, 12, 31)\n",
    "    else:\n",
    "        # Educational materials or unknown: 5 years default\n",
    "        valid_until = datetime(year + 5, 12, 31)\n",
    "    \n",
    "    # Check if still valid\n",
    "    status = \"active\" if valid_until.year >= current_year else \"expired\"\n",
    "    \n",
    "    return status, valid_until.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(\"‚úÖ Functions defined successfully\")\n",
    "print(\"üìö Supports both:\")\n",
    "print(\"   - Legal documents (Lu·∫≠t: 5yr, Ngh·ªã ƒë·ªãnh/Th√¥ng t∆∞: 2yr)\")\n",
    "print(\"   - Educational materials (5yr)\")\n",
    "print(\"üîç Parses year from URL, filename, title, or creation date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3cfd56",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 6: Test Logic tr√™n Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21297d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing status detection logic:\n",
      "\n",
      "1. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "2. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "3. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "4. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "5. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "6. Year: None ‚Üí Status: expired, Valid until: 2024-01-01\n",
      "   File: BG HP TTTN 2 CNPM 2020 final.pdf\n",
      "\n",
      "7. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "8. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "9. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "10. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "11. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "12. Year: None ‚Üí Status: expired, Valid until: 2024-01-01\n",
      "   File: BG HP TTTN 2 CNPM 2020 final.pdf\n",
      "\n",
      "13. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "14. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "15. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "16. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "17. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "18. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "19. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "20. Year: 2016 ‚Üí Status: expired, Valid until: 2021-12-31\n",
      "   File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test tr√™n 20 sample documents ƒë·ªÉ cover different files\n",
    "print(\"üß™ Testing status detection logic:\\n\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, cmetadata\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        LIMIT 20\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    \n",
    "    for i, (doc_id, metadata) in enumerate(samples, 1):\n",
    "        if not metadata:\n",
    "            print(f\"{i}. No metadata\")\n",
    "            continue\n",
    "        \n",
    "        source = metadata.get('source', 'N/A')\n",
    "        title = metadata.get('title', 'N/A')\n",
    "        \n",
    "        # Determine status\n",
    "        status, valid_until = determine_status_and_validity(metadata)\n",
    "        \n",
    "        # Extract year\n",
    "        year = parse_year_from_source_or_title(metadata)\n",
    "        \n",
    "        # Extract filename from source\n",
    "        filename = source.split('/')[-1] if source != 'N/A' else 'N/A'\n",
    "        \n",
    "        print(f\"{i}. Year: {year} ‚Üí Status: {status}, Valid until: {valid_until}\")\n",
    "        print(f\"   File: {filename}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848fcef",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 7: DRY RUN - Xem s·∫Ω update nh∆∞ th·∫ø n√†o (KH√îNG th·ª±c s·ª± update)\n",
    "\n",
    "‚ö†Ô∏è **Cell n√†y KH√îNG update database**, ch·ªâ SHOW k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc8d58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DRY RUN - Analyzing 2103 documents...\n",
      "\n",
      "============================================================\n",
      "üìä DRY RUN RESULTS:\n",
      "============================================================\n",
      "Total documents: 2103\n",
      "‚úÖ Will mark as ACTIVE: 1327\n",
      "‚ùå Will mark as EXPIRED: 776\n",
      "‚è≠Ô∏è  Already has status (skip): 0\n",
      "‚ö†Ô∏è  No metadata (skip): 0\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  Documents to update: 2103\n",
      "üí° No changes made yet. Review results before proceeding to B∆∞·ªõc 8.\n"
     ]
    }
   ],
   "source": [
    "# DRY RUN: Simulate update without actually updating\n",
    "active_count = 0\n",
    "expired_count = 0\n",
    "already_has_status_count = 0\n",
    "no_metadata_count = 0\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, cmetadata \n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    all_docs = cur.fetchall()\n",
    "    \n",
    "    print(f\"üìä DRY RUN - Analyzing {len(all_docs)} documents...\\n\")\n",
    "    \n",
    "    for doc_id, metadata in all_docs:\n",
    "        if not metadata:\n",
    "            no_metadata_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Skip if already has status\n",
    "        if 'status' in metadata:\n",
    "            already_has_status_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Determine status\n",
    "        status, valid_until = determine_status_and_validity(metadata)\n",
    "        \n",
    "        if status == \"active\":\n",
    "            active_count += 1\n",
    "        else:\n",
    "            expired_count += 1\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä DRY RUN RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total documents: {len(all_docs)}\")\n",
    "print(f\"‚úÖ Will mark as ACTIVE: {active_count}\")\n",
    "print(f\"‚ùå Will mark as EXPIRED: {expired_count}\")\n",
    "print(f\"‚è≠Ô∏è  Already has status (skip): {already_has_status_count}\")\n",
    "print(f\"‚ö†Ô∏è  No metadata (skip): {no_metadata_count}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚ö†Ô∏è  Documents to update: {active_count + expired_count}\")\n",
    "print(f\"üí° No changes made yet. Review results before proceeding to B∆∞·ªõc 8.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8a581",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 7a: Ph√¢n t√≠ch Year Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6781847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä YEAR DISTRIBUTION:\n",
      "============================================================\n",
      "2016: 776 docs (1 files) - Valid until 2021 - ‚ùå EXPIRED\n",
      "2020: 482 docs (1 files) - Valid until 2025 - ‚úÖ ACTIVE\n",
      "2023: 272 docs (1 files) - Valid until 2028 - ‚úÖ ACTIVE\n",
      "2024: 88 docs (1 files) - Valid until 2029 - ‚úÖ ACTIVE\n",
      "2025: 485 docs (1 files) - Valid until 2030 - ‚úÖ ACTIVE\n",
      "No year: 0 docs (0 files) - Valid until None - ‚ùå EXPIRED\n",
      "============================================================\n",
      "\n",
      "üí° Current year: 2025\n",
      "üìö Materials published in 2020 or later are ACTIVE (5-year validity)\n",
      "üìö Materials published before 2020 are EXPIRED\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch distribution of years\n",
    "from collections import Counter\n",
    "\n",
    "year_distribution = Counter()\n",
    "files_by_year = {}\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT cmetadata\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    all_docs = cur.fetchall()\n",
    "    \n",
    "    for (metadata,) in all_docs:\n",
    "        if not metadata:\n",
    "            continue\n",
    "        \n",
    "        year = parse_year_from_source_or_title(metadata)\n",
    "        year_distribution[year] += 1\n",
    "        \n",
    "        # Track unique filenames per year\n",
    "        source = metadata.get('source', '')\n",
    "        if source:\n",
    "            filename = source.split('/')[-1]\n",
    "            if year not in files_by_year:\n",
    "                files_by_year[year] = set()\n",
    "            files_by_year[year].add(filename)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä YEAR DISTRIBUTION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sort with None at the end\n",
    "sorted_years = sorted([y for y in year_distribution.keys() if y is not None]) + [None]\n",
    "\n",
    "for year in sorted_years:\n",
    "    count = year_distribution[year]\n",
    "    year_label = year if year else \"No year\"\n",
    "    files = len(files_by_year.get(year, set()))\n",
    "    validity_end = year + 5 if year else None\n",
    "    status = \"‚úÖ ACTIVE\" if validity_end and validity_end >= 2025 else \"‚ùå EXPIRED\"\n",
    "    \n",
    "    print(f\"{year_label}: {count} docs ({files} files) - Valid until {validity_end} - {status}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüí° Current year: 2025\")\n",
    "print(f\"üìö Materials published in 2020 or later are ACTIVE (5-year validity)\")\n",
    "print(f\"üìö Materials published before 2020 are EXPIRED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4b4740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç FILES WITHOUT DETECTED YEAR:\n",
      "============================================================\n",
      "   - thuvienphapluat.vn\n",
      "\n",
      "üí° Observation:\n",
      "File 'BG HP TTTN 2 CNPM 2020 final.pdf' has '2020' in name\n",
      "but our regex pattern requires '-YYYY.pdf' format\n",
      "\n",
      "Let's improve the regex pattern...\n"
     ]
    }
   ],
   "source": [
    "# Inspect files with \"No year\"\n",
    "print(\"\\nüîç FILES WITHOUT DETECTED YEAR:\")\n",
    "print(\"=\" * 60)\n",
    "for filename in sorted(files_by_year.get(None, set())):\n",
    "    print(f\"   - {filename}\")\n",
    "\n",
    "print(\"\\nüí° Observation:\")\n",
    "print(\"File 'BG HP TTTN 2 CNPM 2020 final.pdf' has '2020' in name\")\n",
    "print(\"but our regex pattern requires '-YYYY.pdf' format\")\n",
    "print(\"\\nLet's improve the regex pattern...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b71bde9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç INSPECTING 'thuvienphapluat.vn' DOCUMENTS:\n",
      "============================================================\n",
      "\n",
      "--- Document 1 ---\n",
      "Source: thuvienphapluat.vn\n",
      "Title: N·ªôi dung t·ª´ thuvienphapluat.vn...\n",
      "All fields: ['url', 'dieu', 'title', 'chuong', 'source', 'section', 'chunk_id', 'has_diem', 'has_khoan', 'hierarchy', 'char_count', 'crawled_at', 'chunk_level', 'source_file', 'token_count', 'token_ratio', 'quality_flags', 'semantic_tags', 'structure_score', 'chunking_strategy', 'readability_score', 'is_within_token_limit']\n",
      "\n",
      "--- Document 2 ---\n",
      "Source: thuvienphapluat.vn\n",
      "Title: N·ªôi dung t·ª´ thuvienphapluat.vn...\n",
      "All fields: ['url', 'dieu', 'khoan', 'title', 'chuong', 'source', 'section', 'chunk_id', 'has_diem', 'has_khoan', 'hierarchy', 'char_count', 'crawled_at', 'chunk_level', 'parent_dieu', 'source_file', 'token_count', 'token_ratio', 'quality_flags', 'semantic_tags', 'structure_score', 'chunking_strategy', 'readability_score', 'is_within_token_limit']\n",
      "\n",
      "--- Document 3 ---\n",
      "Source: thuvienphapluat.vn\n",
      "Title: N·ªôi dung t·ª´ thuvienphapluat.vn...\n",
      "All fields: ['url', 'dieu', 'title', 'chuong', 'source', 'section', 'chunk_id', 'has_diem', 'has_khoan', 'hierarchy', 'char_count', 'crawled_at', 'chunk_level', 'source_file', 'token_count', 'token_ratio', 'quality_flags', 'semantic_tags', 'structure_score', 'chunking_strategy', 'readability_score', 'is_within_token_limit']\n",
      "\n",
      "--- Document 4 ---\n",
      "Source: thuvienphapluat.vn\n",
      "Title: N·ªôi dung t·ª´ thuvienphapluat.vn...\n",
      "All fields: ['url', 'dieu', 'title', 'chuong', 'source', 'section', 'chunk_id', 'has_diem', 'has_khoan', 'hierarchy', 'char_count', 'crawled_at', 'chunk_level', 'source_file', 'token_count', 'token_ratio', 'quality_flags', 'semantic_tags', 'structure_score', 'chunking_strategy', 'readability_score', 'is_within_token_limit']\n",
      "\n",
      "--- Document 5 ---\n",
      "Source: thuvienphapluat.vn\n",
      "Title: N·ªôi dung t·ª´ thuvienphapluat.vn...\n",
      "All fields: ['url', 'dieu', 'khoan', 'title', 'chuong', 'source', 'section', 'chunk_id', 'has_diem', 'has_khoan', 'hierarchy', 'char_count', 'crawled_at', 'chunk_level', 'parent_dieu', 'source_file', 'token_count', 'token_ratio', 'quality_flags', 'semantic_tags', 'structure_score', 'chunking_strategy', 'readability_score', 'is_within_token_limit']\n"
     ]
    }
   ],
   "source": [
    "# Inspect thuvienphapluat.vn documents\n",
    "print(\"\\nüîç INSPECTING 'thuvienphapluat.vn' DOCUMENTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT cmetadata\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' LIKE %s\n",
    "        LIMIT 5\n",
    "    \"\"\", (collection_uuid, '%thuvienphapluat.vn%'))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    \n",
    "    for i, (metadata,) in enumerate(samples, 1):\n",
    "        print(f\"\\n--- Document {i} ---\")\n",
    "        print(f\"Source: {metadata.get('source', 'N/A')}\")\n",
    "        print(f\"Title: {metadata.get('title', 'N/A')[:100]}...\")\n",
    "        \n",
    "        # Check all fields\n",
    "        print(f\"All fields: {list(metadata.keys())}\")\n",
    "        \n",
    "        # Check for any date-related fields\n",
    "        for key, value in metadata.items():\n",
    "            if any(date_keyword in key.lower() for date_keyword in ['date', 'time', 'year', 'created', 'modified']):\n",
    "                print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "548c364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SAMPLE URLs:\n",
      "============================================================\n",
      "1. https://thuvienphapluat.vn/van-ban/Dau-tu/Luat-Dau-thau-2023-22-2023-QH15-518805.aspx\n",
      "2. https://thuvienphapluat.vn/van-ban/Dau-tu/Nghi-dinh-214-2025-ND-CP-huong-dan-Luat-Dau-thau-ve-lua-chon-nha-thau-668157.aspx\n",
      "3. https://thuvienphapluat.vn/van-ban/Dau-tu/Thong-tu-22-2024-TT-BKHDT-cung-cap-thong-tin-ve-lua-chon-nha-thau-tren-He-thong-mang-dau-thau-quoc-gia-619403.aspx\n"
     ]
    }
   ],
   "source": [
    "# Check URL examples for thuvienphapluat.vn docs\n",
    "print(\"\\nüìã SAMPLE URLs:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT DISTINCT cmetadata->>'url' as url\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' = 'thuvienphapluat.vn'\n",
    "        LIMIT 10\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    urls = cur.fetchall()\n",
    "    \n",
    "    for i, (url,) in enumerate(urls, 1):\n",
    "        print(f\"{i}. {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81fe1b",
   "metadata": {},
   "source": [
    "## ‚è∏Ô∏è PAUSE - Review Dry Run Results\n",
    "\n",
    "**Tr∆∞·ªõc khi ch·∫°y B∆∞·ªõc 8:**\n",
    "1. ‚úÖ Check dry run numbers c√≥ h·ª£p l√Ω kh√¥ng\n",
    "2. ‚úÖ Verify active/expired ratio\n",
    "3. ‚úÖ Confirm kh√¥ng c√≥ documents n√†o b·ªã miss\n",
    "\n",
    "**N·∫øu OK ‚Üí Ch·∫°y B∆∞·ªõc 8**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef9fbf8",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 8: üö® BULK UPDATE - Th·ª±c s·ª± update database\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL: Cell n√†y s·∫Ω UPDATE database!**\n",
    "\n",
    "Ch·ªâ ch·∫°y sau khi:\n",
    "- ‚úÖ Review dry run results\n",
    "- ‚úÖ Confirm active/expired numbers\n",
    "- ‚úÖ Ready to commit changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "257ca34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting bulk update of 2103 documents...\n",
      "‚è≥ Updated 100/2103 documents...\n",
      "‚è≥ Updated 200/2103 documents...\n",
      "‚è≥ Updated 300/2103 documents...\n",
      "‚è≥ Updated 400/2103 documents...\n",
      "‚è≥ Updated 500/2103 documents...\n",
      "‚è≥ Updated 600/2103 documents...\n",
      "‚è≥ Updated 700/2103 documents...\n",
      "‚è≥ Updated 800/2103 documents...\n",
      "‚è≥ Updated 900/2103 documents...\n",
      "‚è≥ Updated 1000/2103 documents...\n",
      "‚è≥ Updated 1100/2103 documents...\n",
      "‚è≥ Updated 1200/2103 documents...\n",
      "‚è≥ Updated 1300/2103 documents...\n",
      "‚è≥ Updated 1400/2103 documents...\n",
      "‚è≥ Updated 1500/2103 documents...\n",
      "‚è≥ Updated 1600/2103 documents...\n",
      "‚è≥ Updated 1700/2103 documents...\n",
      "‚è≥ Updated 1800/2103 documents...\n",
      "‚è≥ Updated 1900/2103 documents...\n",
      "‚è≥ Updated 2000/2103 documents...\n",
      "‚è≥ Updated 2100/2103 documents...\n",
      "\n",
      "============================================================\n",
      "‚úÖ BULK UPDATE COMPLETE!\n",
      "============================================================\n",
      "üìù Total updated: 2103\n",
      "‚úÖ Active documents: 1327\n",
      "‚ùå Expired documents: 776\n",
      "============================================================\n",
      "‚ö†Ô∏è  Cell is commented out for safety.\n",
      "üí° Remove triple quotes to enable update.\n",
      "\n",
      "============================================================\n",
      "‚úÖ BULK UPDATE COMPLETE!\n",
      "============================================================\n",
      "üìù Total updated: 2103\n",
      "‚úÖ Active documents: 1327\n",
      "‚ùå Expired documents: 776\n",
      "============================================================\n",
      "‚ö†Ô∏è  Cell is commented out for safety.\n",
      "üí° Remove triple quotes to enable update.\n"
     ]
    }
   ],
   "source": [
    "# üö® BULK UPDATE - Uncomment and run to execute\n",
    "# ‚ö†Ô∏è  Remove the triple quotes to enable this cell\n",
    "\n",
    "from psycopg.types.json import Json\n",
    "\n",
    "updated_count = 0\n",
    "active_count = 0\n",
    "expired_count = 0\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    # Get all documents\n",
    "    cur.execute(\n",
    "        \"SELECT id, cmetadata FROM langchain_pg_embedding WHERE collection_id = %s\",\n",
    "        (collection_uuid,)\n",
    "    )\n",
    "    \n",
    "    all_docs = cur.fetchall()\n",
    "    total_docs = len(all_docs)\n",
    "    \n",
    "    print(f\"üöÄ Starting bulk update of {total_docs} documents...\")\n",
    "    \n",
    "    for i, (doc_id, metadata) in enumerate(all_docs, 1):\n",
    "        # Skip if no metadata or already has status\n",
    "        if not metadata or 'status' in metadata:\n",
    "            continue\n",
    "        \n",
    "        # Determine status\n",
    "        status, valid_until = determine_status_and_validity(metadata)\n",
    "        \n",
    "        # Update metadata\n",
    "        metadata[\"status\"] = status\n",
    "        metadata[\"valid_until\"] = valid_until\n",
    "        \n",
    "        # Update database (wrap dict in Json())\n",
    "        cur.execute(\n",
    "            \"UPDATE langchain_pg_embedding SET cmetadata = %s WHERE id = %s\",\n",
    "            (Json(metadata), doc_id)\n",
    "        )\n",
    "        \n",
    "        updated_count += 1\n",
    "        if status == \"active\":\n",
    "            active_count += 1\n",
    "        else:\n",
    "            expired_count += 1\n",
    "        \n",
    "        # Progress\n",
    "        if updated_count % 100 == 0:\n",
    "            print(f\"‚è≥ Updated {updated_count}/{total_docs} documents...\")\n",
    "    \n",
    "    # COMMIT changes\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ BULK UPDATE COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìù Total updated: {updated_count}\")\n",
    "    print(f\"‚úÖ Active documents: {active_count}\")\n",
    "    print(f\"‚ùå Expired documents: {expired_count}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"‚ö†Ô∏è  Cell is commented out for safety.\")\n",
    "print(\"üí° Remove triple quotes to enable update.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1571d6",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 9: Verify Update - Ki·ªÉm tra k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e874b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä STATUS BREAKDOWN:\n",
      "============================================================\n",
      "‚ùå expired: 776 documents\n",
      "‚úÖ active: 1327 documents\n",
      "============================================================\n",
      "\n",
      "üìÑ Sample documents with metadata:\n",
      "\n",
      "1. H·ªåC VI·ªÜN C√îNG NGH·ªÜ B∆ØU CH√çNH VI·ªÑN TH√îNG \n",
      "---------ÔÇñÔÄ¶ÔÇó-------...\n",
      "   Status: expired, Valid until: 2021-12-31\n",
      "   Source: /home/sakana/Code/RAG-bidding/app/data/raw/T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "2. Ch∆∞∆°ng 4. Thi·∫øt k·∫ø\n",
      "‚Ä¢ C√°c l·ªõp th·ª±c th·ªÉ li√™n quan.\n",
      "102\n",
      "H√¨nh 4....\n",
      "   Status: active, Valid until: 2025-12-31\n",
      "   Source: /home/sakana/Code/RAG-bidding/app/data/raw/BG HP TTTN 2 CNPM 2020 final.pdf\n",
      "\n",
      "3. v·ªã cao nh·∫•t l√† d√¢n, v√¨ d√¢n l√† ch·ªß‚Äù74. \n",
      " \n",
      " \n",
      "86 H·ªì C h√≠ Minh, ...\n",
      "   Status: expired, Valid until: 2021-12-31\n",
      "   Source: /home/sakana/Code/RAG-bidding/app/data/raw/T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "\n",
      "4. required /></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>M·∫≠t kh·∫©u:</td>\n",
      "<td><input ty...\n",
      "   Status: active, Valid until: 2025-12-31\n",
      "   Source: /home/sakana/Code/RAG-bidding/app/data/raw/BG HP TTTN 2 CNPM 2020 final.pdf\n",
      "\n",
      "5. String sqlDiem = \"{call DiemcuaDK(?)}\";// su dung stored pro...\n",
      "   Status: active, Valid until: 2025-12-31\n",
      "   Source: /home/sakana/Code/RAG-bidding/app/data/raw/BG HP TTTN 2 CNPM 2020 final.pdf\n"
     ]
    }
   ],
   "source": [
    "# Verify update results\n",
    "with conn.cursor() as cur:\n",
    "    # Count by status\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            cmetadata->>'status' as status,\n",
    "            COUNT(*) as count\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        GROUP BY cmetadata->>'status'\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä STATUS BREAKDOWN:\")\n",
    "    print(\"=\" * 60)\n",
    "    for status, count in results:\n",
    "        emoji = \"‚úÖ\" if status == \"active\" else \"‚ùå\" if status == \"expired\" else \"‚ùì\"\n",
    "        print(f\"{emoji} {status or 'NULL'}: {count} documents\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sample documents with new metadata\n",
    "    print(\"\\nüìÑ Sample documents with metadata:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            LEFT(document, 60) as doc_preview,\n",
    "            cmetadata->>'status' as status,\n",
    "            cmetadata->>'valid_until' as valid_until,\n",
    "            cmetadata->>'url' as url,\n",
    "            cmetadata->>'source' as source\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        LIMIT 5\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    for i, (doc, status, valid_until, url, source) in enumerate(samples, 1):\n",
    "        print(f\"\\n{i}. {doc}...\")\n",
    "        print(f\"   Status: {status}, Valid until: {valid_until}\")\n",
    "        if url:\n",
    "            print(f\"   URL: {url[:60]}...\")\n",
    "        else:\n",
    "            print(f\"   Source: {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78dc05",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 9a: Verify Legal Documents (thuvienphapluat.vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f288379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç LEGAL DOCUMENTS STATUS:\n",
      "============================================================\n",
      "Legal documents status:\n",
      "  ‚úÖ active: 845 documents\n",
      "\n",
      "üìÑ Sample legal documents:\n",
      "\n",
      "1. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "   Status: active, Valid until: 2028-12-31\n",
      "   URL: https://thuvienphapluat.vn/van-ban/Dau-tu/Luat-Dau-thau-2023-22-2023-QH15-518805...\n",
      "\n",
      "2. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "   Status: active, Valid until: 2028-12-31\n",
      "   URL: https://thuvienphapluat.vn/van-ban/Dau-tu/Luat-Dau-thau-2023-22-2023-QH15-518805...\n",
      "\n",
      "3. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "   Status: active, Valid until: 2028-12-31\n",
      "   URL: https://thuvienphapluat.vn/van-ban/Dau-tu/Luat-Dau-thau-2023-22-2023-QH15-518805...\n",
      "\n",
      "4. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "   Status: active, Valid until: 2028-12-31\n",
      "   URL: https://thuvienphapluat.vn/van-ban/Dau-tu/Luat-Dau-thau-2023-22-2023-QH15-518805...\n",
      "\n",
      "5. ‚úÖ Nghi-dinh-214-2025-ND-CP-huong-dan-Luat-Dau-thau-ve-lua-chon-nha-thau-668157\n",
      "   Status: active, Valid until: 2030-12-31\n",
      "   URL: https://thuvienphapluat.vn/van-ban/Dau-tu/Nghi-dinh-214-2025-ND-CP-huong-dan-Lua...\n"
     ]
    }
   ],
   "source": [
    "# Check legal documents status\n",
    "print(\"üîç LEGAL DOCUMENTS STATUS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    # Get status breakdown for legal docs\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            cmetadata->>'status' as status,\n",
    "            COUNT(*) as count\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' = 'thuvienphapluat.vn'\n",
    "        GROUP BY cmetadata->>'status'\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    print(\"Legal documents status:\")\n",
    "    for status, count in results:\n",
    "        emoji = \"‚úÖ\" if status == \"active\" else \"‚ùå\"\n",
    "        print(f\"  {emoji} {status}: {count} documents\")\n",
    "    \n",
    "    # Sample legal documents\n",
    "    print(\"\\nüìÑ Sample legal documents:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            cmetadata->>'url' as url,\n",
    "            cmetadata->>'status' as status,\n",
    "            cmetadata->>'valid_until' as valid_until\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' = 'thuvienphapluat.vn'\n",
    "        LIMIT 5\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    for i, (url, status, valid_until) in enumerate(samples, 1):\n",
    "        # Extract doc type and year from URL\n",
    "        doc_name = url.split('/')[-1].split('.')[0] if url else 'N/A'\n",
    "        emoji = \"‚úÖ\" if status == \"active\" else \"‚ùå\"\n",
    "        print(f\"\\n{i}. {emoji} {doc_name}\")\n",
    "        print(f\"   Status: {status}, Valid until: {valid_until}\")\n",
    "        print(f\"   URL: {url[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db8b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eabbd662",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 10: üóëÔ∏è X√ìA T√ÄI LI·ªÜU KH√îNG LI√äN QUAN ƒê·∫æN ƒê·∫§U TH·∫¶U\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL**: C√°c cells d∆∞·ªõi ƒë√¢y s·∫Ω X√ìA Vƒ®NH VI·ªÑN t√†i li·ªáu t·ª´ database!\n",
    "\n",
    "**T√†i li·ªáu s·∫Ω b·ªã x√≥a:**\n",
    "- Educational PDFs (textbooks, course materials) - 1,258 documents\n",
    "- V√≠ d·ª•: \"T∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh 2016.pdf\", \"BG HP TTTN 2 CNPM 2020.pdf\"\n",
    "\n",
    "**T√†i li·ªáu ƒë∆∞·ª£c GI·ªÆ L·∫†I:**\n",
    "- Legal documents t·ª´ thuvienphapluat.vn - 845 documents\n",
    "- V√≠ d·ª•: Lu·∫≠t ƒê·∫•u th·∫ßu 2023, Ngh·ªã ƒë·ªãnh 214-2025, Th√¥ng t∆∞\n",
    "\n",
    "**Workflow:**\n",
    "1. ‚úÖ Ph√¢n t√≠ch documents ƒë·ªÉ x√≥a\n",
    "2. ‚úÖ DRY RUN - Xem s·∫Ω x√≥a nh·ªØng g√¨ (KH√îNG th·ª±c s·ª± x√≥a)\n",
    "3. ‚è∏Ô∏è **PAUSE** - Review tr∆∞·ªõc khi x√≥a\n",
    "4. üö® BULK DELETE - Th·ª±c s·ª± x√≥a (c·∫ßn confirm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee514b",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 10a: Ph√¢n t√≠ch Documents theo Lo·∫°i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23a9325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç PH√ÇN T√çCH DOCUMENTS THEO NGU·ªíN:\n",
      "============================================================\n",
      "üìú Legal documents (thuvienphapluat.vn): 845 documents\n",
      "üìö Educational PDFs: 1258 documents from 2 files\n",
      "\n",
      "============================================================\n",
      "Total: 2103 documents\n",
      "============================================================\n",
      "\n",
      "üìÇ Sample PDF files to be deleted:\n",
      "   1. T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "   2. BG HP TTTN 2 CNPM 2020 final.pdf\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch documents theo ngu·ªìn\n",
    "print(\"üîç PH√ÇN T√çCH DOCUMENTS THEO NGU·ªíN:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    # Count by source type\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            cmetadata->>'source' as source,\n",
    "            COUNT(*) as count\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        GROUP BY cmetadata->>'source'\n",
    "        ORDER BY count DESC\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    legal_count = 0\n",
    "    pdf_count = 0\n",
    "    pdf_files = set()\n",
    "    \n",
    "    for source, count in results:\n",
    "        if source == 'thuvienphapluat.vn':\n",
    "            legal_count = count\n",
    "            print(f\"üìú Legal documents (thuvienphapluat.vn): {count} documents\")\n",
    "        else:\n",
    "            pdf_count += count\n",
    "            pdf_files.add(source)\n",
    "    \n",
    "    print(f\"üìö Educational PDFs: {pdf_count} documents from {len(pdf_files)} files\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Total: {legal_count + pdf_count} documents\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Show sample PDF filenames\n",
    "    print(\"\\nüìÇ Sample PDF files to be deleted:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT DISTINCT cmetadata->>'source' as source\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' != 'thuvienphapluat.vn'\n",
    "        LIMIT 10\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    for i, (source,) in enumerate(samples, 1):\n",
    "        filename = source.split('/')[-1]\n",
    "        print(f\"   {i}. {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf050f",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 10b: DRY RUN - Xem s·∫Ω x√≥a g√¨ (KH√îNG th·ª±c s·ª± x√≥a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df2130b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DRY RUN - Analyzing documents to delete...\n",
      "============================================================\n",
      "üìä DELETION PLAN:\n",
      "   Total documents: 2103\n",
      "   üóëÔ∏è  Will DELETE (Educational PDFs): 1258\n",
      "   ‚úÖ Will KEEP (Legal documents): 845\n",
      "============================================================\n",
      "\n",
      "üìÑ Sample documents that will be DELETED:\n",
      "\n",
      "1. File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "   Title: ¬ß√í thi ch√ùnh tr√û cu√®i kh√£a kh√®i ¬Æ¬πi h√§c n¬®m h√§c 2006 ‚Äì 2007...\n",
      "   Preview: H·ªåC VI·ªÜN C√îNG NGH·ªÜ B∆ØU CH√çNH VI·ªÑN TH√îNG \n",
      "---------ÔÇñÔÄ¶ÔÇó---------- \n",
      " \n",
      "KHOA C∆† B·∫¢N \n",
      "...\n",
      "\n",
      "2. File: BG HP TTTN 2 CNPM 2020 final.pdf\n",
      "   Title: N/A...\n",
      "   Preview: Ch∆∞∆°ng 4. Thi·∫øt k·∫ø\n",
      "‚Ä¢ C√°c l·ªõp th·ª±c th·ªÉ li√™n quan.\n",
      "102\n",
      "H√¨nh 4.7: Thi·∫øt k·∫ø giao di·ªá...\n",
      "\n",
      "3. File: T∆∞-t∆∞·ªüng-H·ªì-Ch√≠-Minh-2016.pdf\n",
      "   Title: ¬ß√í thi ch√ùnh tr√û cu√®i kh√£a kh√®i ¬Æ¬πi h√§c n¬®m h√§c 2006 ‚Äì 2007...\n",
      "   Preview: v·ªã cao nh·∫•t l√† d√¢n, v√¨ d√¢n l√† ch·ªß‚Äù74. \n",
      " \n",
      " \n",
      "86 H·ªì C h√≠ Minh, to√†n t·∫≠p, nxb Ch√≠nh ...\n",
      "\n",
      "4. File: BG HP TTTN 2 CNPM 2020 final.pdf\n",
      "   Title: N/A...\n",
      "   Preview: required /></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>M·∫≠t kh·∫©u:</td>\n",
      "<td><input type=\"password\" name=\"...\n",
      "\n",
      "5. File: BG HP TTTN 2 CNPM 2020 final.pdf\n",
      "   Title: N/A...\n",
      "   Preview: String sqlDiem = \"{call DiemcuaDK(?)}\";// su dung stored procedure\n",
      "LichhocDAO ld...\n",
      "\n",
      "============================================================\n",
      "‚úÖ Sample documents that will be KEPT:\n",
      "\n",
      "1. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "   Preview: [Ph·∫ßn: CƒÇN C·ª® HI·∫æN PH√ÅP N∆Ø·ªöC C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM;] [CH∆Ø∆†NG II]\n",
      "\n",
      "ƒê...\n",
      "\n",
      "2. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "   Preview: [Ph·∫ßn: CƒÇN C·ª® HI·∫æN PH√ÅP N∆Ø·ªöC C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM;]\n",
      "ƒêi·ªÅu 11.\n",
      "\n",
      "Kho·∫£...\n",
      "\n",
      "3. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "   Preview: [Ph·∫ßn: CƒÇN C·ª® HI·∫æN PH√ÅP N∆Ø·ªöC C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM;] [CH∆Ø∆†NG II]\n",
      "\n",
      "ƒê...\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è  No changes made yet. Review carefully before proceeding!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# DRY RUN: Xem documents s·∫Ω b·ªã x√≥a\n",
    "print(\"üîç DRY RUN - Analyzing documents to delete...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    # Count documents to delete (PDFs)\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' != 'thuvienphapluat.vn'\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    to_delete_count = cur.fetchone()[0]\n",
    "    \n",
    "    # Count documents to keep (legal)\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' = 'thuvienphapluat.vn'\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    to_keep_count = cur.fetchone()[0]\n",
    "    \n",
    "    # Get total\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    total_count = cur.fetchone()[0]\n",
    "    \n",
    "    print(f\"üìä DELETION PLAN:\")\n",
    "    print(f\"   Total documents: {total_count}\")\n",
    "    print(f\"   üóëÔ∏è  Will DELETE (Educational PDFs): {to_delete_count}\")\n",
    "    print(f\"   ‚úÖ Will KEEP (Legal documents): {to_keep_count}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Show sample documents to delete\n",
    "    print(\"\\nüìÑ Sample documents that will be DELETED:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            LEFT(document, 80) as doc_preview,\n",
    "            cmetadata->>'source' as source,\n",
    "            cmetadata->>'title' as title\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' != 'thuvienphapluat.vn'\n",
    "        LIMIT 5\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    for i, (doc, source, title) in enumerate(samples, 1):\n",
    "        filename = source.split('/')[-1] if source else 'N/A'\n",
    "        print(f\"\\n{i}. File: {filename}\")\n",
    "        print(f\"   Title: {title[:60] if title else 'N/A'}...\")\n",
    "        print(f\"   Preview: {doc}...\")\n",
    "    \n",
    "    # Show sample documents to keep\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Sample documents that will be KEPT:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            LEFT(document, 80) as doc_preview,\n",
    "            cmetadata->>'url' as url,\n",
    "            cmetadata->>'status' as status\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' = 'thuvienphapluat.vn'\n",
    "        LIMIT 3\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    for i, (doc, url, status) in enumerate(samples, 1):\n",
    "        doc_name = url.split('/')[-1].split('.')[0][:40] if url else 'N/A'\n",
    "        emoji = \"‚úÖ\" if status == \"active\" else \"‚ùå\"\n",
    "        print(f\"\\n{i}. {emoji} {doc_name}\")\n",
    "        print(f\"   Preview: {doc}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  No changes made yet. Review carefully before proceeding!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96819c",
   "metadata": {},
   "source": [
    "## ‚è∏Ô∏è PAUSE - Review Deletion Plan\n",
    "\n",
    "**Tr∆∞·ªõc khi ch·∫°y cell x√≥a:**\n",
    "1. ‚úÖ Verify s·ªë l∆∞·ª£ng documents to delete/keep\n",
    "2. ‚úÖ Check sample documents to ensure kh√¥ng x√≥a nh·∫ßm legal docs\n",
    "3. ‚úÖ Backup database n·∫øu c·∫ßn (recommended)\n",
    "4. ‚úÖ Confirm s·∫µn s√†ng x√≥a vƒ©nh vi·ªÖn\n",
    "\n",
    "**‚ö†Ô∏è L∆ØU √ù:**\n",
    "- Documents b·ªã x√≥a KH√îNG TH·ªÇ KH√îI PH·ª§C (tr·ª´ khi c√≥ backup)\n",
    "- Embeddings c≈©ng s·∫Ω b·ªã x√≥a theo\n",
    "- N√™n backup tr∆∞·ªõc: `pg_dump ragdb > backup_before_delete.sql`\n",
    "\n",
    "**N·∫øu OK ‚Üí Ch·∫°y B∆∞·ªõc 10c**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb4d26",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 10c: üö® BULK DELETE - X√≥a t√†i li·ªáu kh√¥ng li√™n quan\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL: Cell n√†y s·∫Ω X√ìA Vƒ®NH VI·ªÑN documents t·ª´ database!**\n",
    "\n",
    "Ch·ªâ ch·∫°y sau khi:\n",
    "- ‚úÖ Review dry run results\n",
    "- ‚úÖ Confirm numbers are correct\n",
    "- ‚úÖ Backup database (recommended)\n",
    "- ‚úÖ Ready to permanently delete educational PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60f72e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Starting deletion process...\n",
      "üìä Documents before: 2103\n",
      "\n",
      "============================================================\n",
      "‚úÖ BULK DELETE COMPLETE!\n",
      "============================================================\n",
      "üìä Documents before: 2103\n",
      "üóëÔ∏è  Documents deleted: 1258\n",
      "‚úÖ Documents remaining: 845\n",
      "============================================================\n",
      "\n",
      "üí° Kept only legal documents from thuvienphapluat.vn\n",
      "üéØ Database now contains ONLY bidding law-related documents\n",
      "‚ö†Ô∏è  Cell is commented out for safety.\n",
      "üí° Remove triple quotes to enable deletion.\n",
      "‚ö†Ô∏è  Make sure you've reviewed the dry-run results first!\n"
     ]
    }
   ],
   "source": [
    "# üö® BULK DELETE - Uncomment and run to execute\n",
    "# ‚ö†Ô∏è  Remove the triple quotes to enable this cell\n",
    "\n",
    "\n",
    "deleted_count = 0\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    # Count before deletion\n",
    "    cur.execute(\n",
    "        \"SELECT COUNT(*) FROM langchain_pg_embedding WHERE collection_id = %s\",\n",
    "        (collection_uuid,)\n",
    "    )\n",
    "    before_count = cur.fetchone()[0]\n",
    "    \n",
    "    print(f\"üóëÔ∏è  Starting deletion process...\")\n",
    "    print(f\"üìä Documents before: {before_count}\")\n",
    "    \n",
    "    # DELETE all educational PDFs (source != 'thuvienphapluat.vn')\n",
    "    cur.execute(\"\"\"\n",
    "        DELETE FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        AND cmetadata->>'source' != 'thuvienphapluat.vn'\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    deleted_count = cur.rowcount\n",
    "    \n",
    "    # Count after deletion\n",
    "    cur.execute(\n",
    "        \"SELECT COUNT(*) FROM langchain_pg_embedding WHERE collection_id = %s\",\n",
    "        (collection_uuid,)\n",
    "    )\n",
    "    after_count = cur.fetchone()[0]\n",
    "    \n",
    "    # COMMIT changes\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ BULK DELETE COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìä Documents before: {before_count}\")\n",
    "    print(f\"üóëÔ∏è  Documents deleted: {deleted_count}\")\n",
    "    print(f\"‚úÖ Documents remaining: {after_count}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüí° Kept only legal documents from thuvienphapluat.vn\")\n",
    "    print(f\"üéØ Database now contains ONLY bidding law-related documents\")\n",
    "\n",
    "\n",
    "print(\"‚ö†Ô∏è  Cell is commented out for safety.\")\n",
    "print(\"üí° Remove triple quotes to enable deletion.\")\n",
    "print(\"‚ö†Ô∏è  Make sure you've reviewed the dry-run results first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd906fd",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 10d: Verify Deletion - Ki·ªÉm tra sau khi x√≥a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099513a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFICATION AFTER DELETION:\n",
      "============================================================\n",
      "üìä Total documents remaining: 845\n",
      "\n",
      "üìã Documents by source:\n",
      "   - thuvienphapluat.vn: 845 documents\n",
      "\n",
      "üìä Status breakdown:\n",
      "   ‚úÖ active: 845 documents\n",
      "\n",
      "üìÑ Sample remaining documents:\n",
      "\n",
      "   1. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "      Status: active, Valid until: 2028-12-31\n",
      "\n",
      "   2. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "      Status: active, Valid until: 2028-12-31\n",
      "\n",
      "   3. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "      Status: active, Valid until: 2028-12-31\n",
      "\n",
      "   4. ‚úÖ Luat-Dau-thau-2023-22-2023-QH15-518805\n",
      "      Status: active, Valid until: 2028-12-31\n",
      "\n",
      "   5. ‚úÖ Nghi-dinh-214-2025-ND-CP-huong-dan-Luat-Dau-thau-v\n",
      "      Status: active, Valid until: 2030-12-31\n",
      "\n",
      "============================================================\n",
      "‚úÖ Verification complete!\n",
      "üéØ Database now contains ONLY legal documents from thuvienphapluat.vn\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify deletion results\n",
    "print(\"üîç VERIFICATION AFTER DELETION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    # Total count\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    total_count = cur.fetchone()[0]\n",
    "    print(f\"üìä Total documents remaining: {total_count}\")\n",
    "    \n",
    "    # Count by source (should only have thuvienphapluat.vn)\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            cmetadata->>'source' as source,\n",
    "            COUNT(*) as count\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        GROUP BY cmetadata->>'source'\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    print(\"\\nüìã Documents by source:\")\n",
    "    for source, count in results:\n",
    "        print(f\"   - {source}: {count} documents\")\n",
    "    \n",
    "    # Status breakdown\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            cmetadata->>'status' as status,\n",
    "            COUNT(*) as count\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        GROUP BY cmetadata->>'status'\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    print(\"\\nüìä Status breakdown:\")\n",
    "    for status, count in results:\n",
    "        emoji = \"‚úÖ\" if status == \"active\" else \"‚ùå\"\n",
    "        print(f\"   {emoji} {status}: {count} documents\")\n",
    "    \n",
    "    # Sample remaining documents\n",
    "    print(\"\\nüìÑ Sample remaining documents:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            cmetadata->>'url' as url,\n",
    "            cmetadata->>'status' as status,\n",
    "            cmetadata->>'valid_until' as valid_until\n",
    "        FROM langchain_pg_embedding \n",
    "        WHERE collection_id = %s\n",
    "        LIMIT 5\n",
    "    \"\"\", (collection_uuid,))\n",
    "    \n",
    "    samples = cur.fetchall()\n",
    "    for i, (url, status, valid_until) in enumerate(samples, 1):\n",
    "        doc_name = url.split('/')[-1].split('.')[0][:50] if url else 'N/A'\n",
    "        emoji = \"‚úÖ\" if status == \"active\" else \"‚ùå\"\n",
    "        print(f\"\\n   {i}. {emoji} {doc_name}\")\n",
    "        print(f\"      Status: {status}, Valid until: {valid_until}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Verification complete!\")\n",
    "print(\"üéØ Database now contains ONLY legal documents from thuvienphapluat.vn\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
