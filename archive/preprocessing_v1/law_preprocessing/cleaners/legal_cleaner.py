"""
Legal Document Cleaner cho vÄƒn báº£n Ä‘áº¥u tháº§u

Module nÃ y clean vÃ  normalize text extracted tá»« DOCX
vá»›i focus vÃ o vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam vá» Ä‘áº¥u tháº§u
"""

import re
from typing import Dict, List


class LegalDocumentCleaner:
    """Clean vÃ  normalize vÄƒn báº£n phÃ¡p luáº­t"""

    def __init__(self):
        # Bidding-specific terms cáº§n standardize
        self.bidding_terms = {
            # Standardize cÃ¡c thuáº­t ngá»¯ Ä‘áº¥u tháº§u
            r"Ä‘áº¥u\s*tháº§u": "Ä‘áº¥u tháº§u",
            r"nhÃ \s*tháº§u": "nhÃ  tháº§u",
            r"gÃ³i\s*tháº§u": "gÃ³i tháº§u",
            r"há»“\s*sÆ¡\s*má»i\s*tháº§u": "há»“ sÆ¡ má»i tháº§u",
            r"há»“\s*sÆ¡\s*dá»±\s*tháº§u": "há»“ sÆ¡ dá»± tháº§u",
            r"káº¿\s*hoáº¡ch\s*lá»±a\s*chá»n\s*nhÃ \s*tháº§u": "káº¿ hoáº¡ch lá»±a chá»n nhÃ  tháº§u",
            r"e[\s-]?procurement": "e-procurement",
        }

    def clean(self, text: str, aggressive: bool = False) -> str:
        """
        Main cleaning method

        Args:
            text: Raw text to clean
            aggressive: Náº¿u True, apply cleaning máº¡nh hÆ¡n

        Returns:
            Cleaned text
        """
        # Step 1: Basic cleaning
        text = self._basic_clean(text)

        # Step 2: Legal document specific cleaning
        text = self._legal_clean(text)

        # Step 3: Normalize bidding terms
        text = self._normalize_bidding_terms(text)

        # Step 4: Structure cleaning
        text = self._clean_structure(text)

        # Step 5: Aggressive cleaning (optional)
        if aggressive:
            text = self._aggressive_clean(text)

        return text

    def _basic_clean(self, text: str) -> str:
        """Basic text cleaning"""

        # Remove zero-width characters
        text = re.sub(r"[\u200b\u200c\u200d\ufeff]", "", text)

        # Normalize whitespace
        text = re.sub(r"[ \t]+", " ", text)  # Multiple spaces to single

        # Normalize newlines
        text = re.sub(r"\n\s*\n\s*\n+", "\n\n", text)  # Max 2 newlines

        # Remove trailing whitespace
        lines = [line.rstrip() for line in text.split("\n")]
        text = "\n".join(lines)

        # Remove leading/trailing whitespace
        text = text.strip()

        return text

    def _legal_clean(self, text: str) -> str:
        """Cleaning Ä‘áº·c biá»‡t cho vÄƒn báº£n phÃ¡p luáº­t"""

        # Chuáº©n hÃ³a cáº¥u trÃºc Äiá»u
        text = re.sub(r"Äiá»u\s+(\d+[a-z]?)\s*[:\.]", r"Äiá»u \1.", text)

        # Chuáº©n hÃ³a CHÆ¯Æ NG
        text = re.sub(
            r"(CHÆ¯Æ NG|ChÆ°Æ¡ng)\s+([IVXLCDM]+)\s*[:\.]",
            r"\1 \2.",
            text,
            flags=re.I,
        )

        # Chuáº©n hÃ³a Má»¤C
        text = re.sub(r"(Má»¤C|Má»¥c)\s+(\d+)\s*[:\.]", r"\1 \2.", text, flags=re.I)

        # Chuáº©n hÃ³a khoáº£n (numbered lists)
        text = re.sub(r"^(\d+)\s*[\.ã€‚]\s+", r"\1. ", text, flags=re.MULTILINE)

        # Chuáº©n hÃ³a Ä‘iá»ƒm (lettered lists)
        text = re.sub(r"^([a-zÄ‘])\s*\)\s+", r"\1) ", text, flags=re.MULTILINE)

        return text

    def _normalize_bidding_terms(self, text: str) -> str:
        """Chuáº©n hÃ³a cÃ¡c thuáº­t ngá»¯ Ä‘áº¥u tháº§u"""

        for pattern, replacement in self.bidding_terms.items():
            text = re.sub(pattern, replacement, text, flags=re.I)

        return text

    def _clean_structure(self, text: str) -> str:
        """Clean structural issues"""

        # Remove page numbers (isolated numbers)
        text = re.sub(r"^\s*\d+\s*$", "", text, flags=re.MULTILINE)

        # Remove header/footer artifacts
        artifacts = [
            r"^Trang\s+\d+\s*(/\s*\d+)?$",  # Page numbers
            r"^Báº£n\s+sao\s+lÆ°u\s+trá»¯$",  # Archive copy markers
        ]

        for pattern in artifacts:
            text = re.sub(pattern, "", text, flags=re.MULTILINE | re.I)

        # Remove table markers
        text = re.sub(r"\[TABLE\]\s*\n", "", text)

        return text

    def _aggressive_clean(self, text: str) -> str:
        """Aggressive cleaning (cÃ³ thá»ƒ remove useful content)"""

        # Remove very short lines (likely artifacts)
        lines = text.split("\n")
        lines = [line for line in lines if len(line.strip()) > 5 or not line.strip()]
        text = "\n".join(lines)

        # Remove duplicate lines
        lines = text.split("\n")
        unique_lines = []
        prev_line = None

        for line in lines:
            if line != prev_line:
                unique_lines.append(line)
            prev_line = line

        text = "\n".join(unique_lines)

        return text

    def validate_cleaned_text(self, text: str) -> Dict:
        """Validate cleaned text"""

        validation = {
            "is_valid": True,
            "warnings": [],
            "errors": [],
            "statistics": {},
        }

        # Check for key legal structures
        has_dieu = bool(re.search(r"Äiá»u\s+\d+", text))
        has_chuong = bool(re.search(r"(CHÆ¯Æ NG|ChÆ°Æ¡ng)\s+[IVXLCDM]+", text, re.I))

        if not has_dieu:
            validation["warnings"].append("No 'Äiá»u' structure found")

        if not has_chuong:
            validation["warnings"].append("No 'ChÆ°Æ¡ng' structure found")

        # Check length
        if len(text) < 1000:
            validation["warnings"].append("Document seems too short")

        # Statistics
        validation["statistics"] = {
            "char_count": len(text),
            "line_count": len(text.split("\n")),
            "word_count": len(text.split()),
            "has_dieu": has_dieu,
            "has_chuong": has_chuong,
        }

        return validation

    def extract_metadata_from_text(self, text: str) -> Dict:
        """
        Extract metadata tá»« ná»™i dung vÄƒn báº£n
        (tÃ¬m thÃ´ng tin trong pháº§n header/title)
        """
        metadata = {
            "effective_date": None,
            "issuing_agency": None,
            "signers": [],
        }

        # Extract effective date
        date_patterns = [
            r"(?:cÃ³\s+)?hiá»‡u\s+lá»±c\s+(?:tá»«\s+)?(?:ngÃ y\s+)?(\d{1,2}[/-]\d{1,2}[/-]\d{4})",
            r"(?:cÃ³\s+)?hiá»‡u\s+lá»±c\s+(?:tá»«\s+)?(?:ngÃ y\s+)?(\d{1,2}\s+thÃ¡ng\s+\d{1,2}\s+nÄƒm\s+\d{4})",
        ]

        for pattern in date_patterns:
            match = re.search(pattern, text, re.I)
            if match:
                metadata["effective_date"] = match.group(1)
                break

        # Extract issuing agency
        agency_patterns = [
            r"(QUá»C Há»˜I|Quá»‘c há»™i)",
            r"(CHÃNH PHá»¦|ChÃ­nh phá»§)",
            r"(Bá»˜ TÃ€I CHÃNH|Bá»™ TÃ i chÃ­nh)",
            r"(Bá»˜ Káº¾ HOáº CH VÃ€ Äáº¦U TÆ¯|Bá»™ Káº¿ hoáº¡ch vÃ  Äáº§u tÆ°)",
        ]

        for pattern in agency_patterns:
            match = re.search(pattern, text[:2000], re.I)  # Check first 2000 chars
            if match:
                metadata["issuing_agency"] = match.group(1)
                break

        # Extract signers
        signer_patterns = [
            r"(?:CHá»¦ Tá»ŠCH|Chá»§ tá»‹ch)\s+(.+?)(?:\n|$)",
            r"(?:THá»¦ TÆ¯á»šNG|Thá»§ tÆ°á»›ng)\s+(.+?)(?:\n|$)",
            r"(?:Bá»˜ TRÆ¯á»NG|Bá»™ trÆ°á»Ÿng)\s+(.+?)(?:\n|$)",
        ]

        for pattern in signer_patterns:
            match = re.search(pattern, text[-3000:], re.I)  # Check last 3000 chars
            if match:
                metadata["signers"].append(match.group(1).strip())

        return metadata


# ============ USAGE EXAMPLE ============

if __name__ == "__main__":
    sample_text = """
    CHÆ¯Æ NG   I:  QUY   Äá»ŠNH  CHUNG
    
    Äiá»u  1  .  Pháº¡m  vi  Ä‘iá»u  chá»‰nh
    
    1.  Luáº­t nÃ y quy Ä‘á»‹nh vá»   Ä‘áº¥u   tháº§u   vÃ  lá»±a chá»n   nhÃ    tháº§u .
    
    2.  Quy Ä‘á»‹nh vá»   há»“   sÆ¡   má»i   tháº§u  vÃ    há»“   sÆ¡   dá»±   tháº§u  .
    
    a)  YÃªu cáº§u vá» nÄƒng lá»±c cá»§a   nhÃ    tháº§u
    
    b)  TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡
    
    
    
    
    Trang 1
    
    Äiá»u  2  .  Äá»‘i tÆ°á»£ng Ã¡p dá»¥ng
    """

    cleaner = LegalDocumentCleaner()

    print("=" * 80)
    print("LEGAL DOCUMENT CLEANER")
    print("=" * 80)

    print("\nğŸ“„ Original text:")
    print(sample_text)

    cleaned = cleaner.clean(sample_text, aggressive=True)

    print("\nâœ¨ Cleaned text:")
    print(cleaned)

    validation = cleaner.validate_cleaned_text(cleaned)
    print(f"\nâœ… Validation:")
    print(f"   Valid: {validation['is_valid']}")
    print(f"   Warnings: {validation['warnings']}")
    print(f"   Stats: {validation['statistics']}")

    metadata = cleaner.extract_metadata_from_text(cleaned)
    print(f"\nğŸ“‹ Extracted metadata:")
    for key, value in metadata.items():
        print(f"   {key}: {value}")
