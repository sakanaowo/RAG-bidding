#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Thuvienphapluat.vn Web Crawler Module

Module nÃ y cung cáº¥p cÃ¡c chá»©c nÄƒng Ä‘á»ƒ crawl vÃ  trÃ­ch xuáº¥t ná»™i dung tá»« website
thuvienphapluat.vn, sau Ä‘Ã³ export sang Ä‘á»‹nh dáº¡ng Markdown.

Author: Generated from Jupyter Notebook
Date: 2025-09-29
"""

import os
import re
import time
import requests
from datetime import datetime
from urllib.parse import urlparse
from bs4 import BeautifulSoup, NavigableString, Comment
from typing import Optional, List, Dict, Any


class ThuvienphapluatCrawler:
    """
    Class chÃ­nh Ä‘á»ƒ crawl dá»¯ liá»‡u tá»« thuvienphapluat.vn
    """

    def __init__(
        self,
        user_agent: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        timeout: int = 30,
        default_output_dir: str = "../processed/",
    ):
        """
        Khá»Ÿi táº¡o crawler

        Args:
            user_agent (str): User agent string Ä‘á»ƒ trÃ¡nh bá»‹ block
            timeout (int): Timeout cho requests (seconds)
            default_output_dir (str): ThÆ° má»¥c máº·c Ä‘á»‹nh Ä‘á»ƒ lÆ°u file
        """
        self.headers = {"User-Agent": user_agent}
        self.timeout = timeout
        self.default_output_dir = default_output_dir

    def extract_content_from_div(self, content_div) -> str:
        """
        TrÃ­ch xuáº¥t ná»™i dung tá»« div vÃ  chuyá»ƒn Ä‘á»•i sang markdown (phiÃªn báº£n cÆ¡ báº£n)

        Args:
            content_div: BeautifulSoup element chá»©a ná»™i dung

        Returns:
            str: Ná»™i dung Ä‘Ã£ chuyá»ƒn Ä‘á»•i sang markdown
        """
        if not content_div:
            return "KhÃ´ng tÃ¬m tháº¥y ná»™i dung"

        markdown_content = []

        # Duyá»‡t qua táº¥t cáº£ cÃ¡c tháº» con
        for element in content_div.find_all(recursive=True):
            # Bá» qua cÃ¡c comment vÃ  navigable string
            if isinstance(element, (Comment, NavigableString)):
                continue

            # Xá»­ lÃ½ cÃ¡c tháº» tiÃªu Ä‘á»
            if element.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
                level = int(element.name[1])
                text = element.get_text(strip=True)
                if text:
                    markdown_content.append("#" * level + " " + text + "\n")

            # Xá»­ lÃ½ tháº» paragraph
            elif element.name == "p":
                text = element.get_text(strip=True)
                if text:
                    markdown_content.append(text + "\n\n")

            # Xá»­ lÃ½ danh sÃ¡ch cÃ³ thá»© tá»±
            elif element.name == "ol":
                for i, li in enumerate(element.find_all("li"), 1):
                    text = li.get_text(strip=True)
                    if text:
                        markdown_content.append(f"{i}. {text}\n")
                markdown_content.append("\n")

            # Xá»­ lÃ½ danh sÃ¡ch khÃ´ng thá»© tá»±
            elif element.name == "ul":
                for li in element.find_all("li"):
                    text = li.get_text(strip=True)
                    if text:
                        markdown_content.append(f"- {text}\n")
                markdown_content.append("\n")

            # Xá»­ lÃ½ cÃ¡c tháº» strong/b
            elif element.name in ["strong", "b"]:
                text = element.get_text(strip=True)
                if text:
                    markdown_content.append(f"**{text}**")

            # Xá»­ lÃ½ cÃ¡c tháº» em/i
            elif element.name in ["em", "i"]:
                text = element.get_text(strip=True)
                if text:
                    markdown_content.append(f"*{text}*")

            # Xá»­ lÃ½ tháº» div vá»›i class Ä‘áº·c biá»‡t
            elif element.name == "div":
                text = element.get_text(strip=True)
                # Chá»‰ thÃªm ná»™i dung náº¿u div khÃ´ng cÃ³ tháº» con phá»©c táº¡p
                if text and not element.find_all(
                    ["div", "p", "h1", "h2", "h3", "h4", "h5", "h6"]
                ):
                    markdown_content.append(text + "\n\n")

        return "".join(markdown_content)

    def extract_clean_content(self, content_div) -> str:
        """
        TrÃ­ch xuáº¥t ná»™i dung sáº¡ch vÃ  chuyá»ƒn Ä‘á»•i sang markdown (phiÃªn báº£n cáº£i tiáº¿n)

        Args:
            content_div: BeautifulSoup element chá»©a ná»™i dung

        Returns:
            str: Ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch vÃ  chuyá»ƒn Ä‘á»•i sang markdown
        """
        if not content_div:
            return "KhÃ´ng tÃ¬m tháº¥y ná»™i dung"

        markdown_lines = []
        processed_elements = set()

        def clean_text(text):
            """LÃ m sáº¡ch text, loáº¡i bá» khoáº£ng tráº¯ng thá»«a"""
            return " ".join(text.split())

        def process_element(element, level=0):
            """Xá»­ lÃ½ tá»«ng element má»™t cÃ¡ch Ä‘á»‡ quy"""
            if element in processed_elements:
                return

            processed_elements.add(element)

            # Xá»­ lÃ½ tiÃªu Ä‘á»
            if element.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
                level_num = int(element.name[1])
                text = clean_text(element.get_text())
                if text and len(text) > 2:  # Chá»‰ láº¥y tiÃªu Ä‘á» cÃ³ Ã½ nghÄ©a
                    markdown_lines.append("#" * level_num + " " + text + "\n\n")

            # Xá»­ lÃ½ paragraph
            elif element.name == "p":
                text = clean_text(element.get_text())
                if text and len(text) > 10:  # Chá»‰ láº¥y Ä‘oáº¡n vÄƒn cÃ³ Ã½ nghÄ©a
                    markdown_lines.append(text + "\n\n")

            # Xá»­ lÃ½ div cÃ³ ná»™i dung vÄƒn báº£n
            elif element.name == "div":
                # Náº¿u div khÃ´ng chá»©a cÃ¡c tháº» block khÃ¡c, láº¥y text
                if not element.find_all(
                    ["div", "p", "h1", "h2", "h3", "h4", "h5", "h6", "ul", "ol"]
                ):
                    text = clean_text(element.get_text())
                    if text and len(text) > 10:
                        markdown_lines.append(text + "\n\n")
                else:
                    # Náº¿u cÃ³ tháº» con, xá»­ lÃ½ Ä‘á»‡ quy
                    for child in element.children:
                        if hasattr(child, "name") and child.name:
                            process_element(child, level + 1)

            # Xá»­ lÃ½ danh sÃ¡ch
            elif element.name == "ul":
                for li in element.find_all("li"):
                    text = clean_text(li.get_text())
                    if text:
                        markdown_lines.append(f"- {text}\n")
                markdown_lines.append("\n")

            elif element.name == "ol":
                for i, li in enumerate(element.find_all("li"), 1):
                    text = clean_text(li.get_text())
                    if text:
                        markdown_lines.append(f"{i}. {text}\n")
                markdown_lines.append("\n")

        # Báº¯t Ä‘áº§u xá»­ lÃ½ tá»« root element
        process_element(content_div)

        # LÃ m sáº¡ch káº¿t quáº£
        content = "".join(markdown_lines)
        # Loáº¡i bá» nhiá»u dÃ²ng trá»‘ng liÃªn tiáº¿p
        content = re.sub(r"\n{3,}", "\n\n", content)

        return content.strip()

    def export_to_markdown(
        self, content: str, url: str, output_dir: Optional[str] = None
    ) -> Optional[str]:
        """
        Export ná»™i dung ra file markdown

        Args:
            content (str): Ná»™i dung Ä‘á»ƒ export
            url (str): URL gá»‘c
            output_dir (str, optional): ThÆ° má»¥c output. Máº·c Ä‘á»‹nh sá»­ dá»¥ng default_output_dir

        Returns:
            str: ÄÆ°á»ng dáº«n file Ä‘Ã£ táº¡o, hoáº·c None náº¿u cÃ³ lá»—i
        """
        if output_dir is None:
            output_dir = self.default_output_dir

        # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
        os.makedirs(output_dir, exist_ok=True)

        # Táº¡o tÃªn file tá»« URL
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.split("/")

        # Láº¥y pháº§n cuá»‘i cá»§a URL lÃ m tÃªn file, loáº¡i bá» extension
        filename_base = path_parts[-1].replace(".aspx", "").replace(".html", "")
        if not filename_base:
            filename_base = "crawled_content"

        # Táº¡o timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_base}_{timestamp}.md"
        filepath = os.path.join(output_dir, filename)

        # Táº¡o header cho file markdown
        header = f"""---
title: "Ná»™i dung tá»« {parsed_url.netloc}"
url: {url}
crawled_at: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
source: thuvienphapluat.vn
---

"""

        # Ghi file
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(header)
                f.write(content)

            print(f"âœ… ÄÃ£ export thÃ nh cÃ´ng vÃ o file: {filepath}")
            print(f"ğŸ“„ KÃ­ch thÆ°á»›c file: {os.path.getsize(filepath):,} bytes")
            return filepath

        except Exception as e:
            print(f"âŒ Lá»—i khi export file: {str(e)}")
            return None

    def crawl_single_url(
        self, url: str, output_dir: Optional[str] = None
    ) -> Optional[str]:
        """
        Crawl má»™t URL Ä‘Æ¡n láº» vÃ  export sang markdown

        Args:
            url (str): URL cáº§n crawl
            output_dir (str, optional): ThÆ° má»¥c output

        Returns:
            str: ÄÆ°á»ng dáº«n file Ä‘Ã£ táº¡o, hoáº·c None náº¿u cÃ³ lá»—i
        """
        print(f"ğŸ”„ Báº¯t Ä‘áº§u crawl: {url}")

        try:
            # 1. Gá»­i request
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            print(f"ğŸ“¡ Status code: {response.status_code}")

            if response.status_code != 200:
                print(
                    f"âŒ KhÃ´ng thá»ƒ truy cáº­p trang web. Status: {response.status_code}"
                )
                return None

            # 2. Parse HTML
            soup = BeautifulSoup(response.content, "html.parser")
            content_div = soup.find("div", class_="content1")

            if not content_div:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y div vá»›i class 'content1'")
                return None

            print("âœ… TÃ¬m tháº¥y ná»™i dung")

            # 3. TrÃ­ch xuáº¥t ná»™i dung
            clean_content = self.extract_clean_content(content_div)
            print(f"ğŸ“ ÄÃ£ trÃ­ch xuáº¥t {len(clean_content)} kÃ½ tá»±")

            # 4. Export ra markdown
            output_file = self.export_to_markdown(clean_content, url, output_dir)

            return output_file

        except requests.RequestException as e:
            print(f"âŒ Lá»—i khi request: {str(e)}")
            return None
        except Exception as e:
            print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {str(e)}")
            return None

    def crawl_multiple_urls(
        self, urls: List[str], output_dir: Optional[str] = None, delay: int = 2
    ) -> List[Dict[str, Any]]:
        """
        Crawl nhiá»u URL vá»›i delay giá»¯a cÃ¡c request

        Args:
            urls (List[str]): Danh sÃ¡ch cÃ¡c URL cáº§n crawl
            output_dir (str, optional): ThÆ° má»¥c output
            delay (int): Delay giá»¯a cÃ¡c request (seconds)

        Returns:
            List[Dict]: Danh sÃ¡ch káº¿t quáº£ crawling
        """
        results = []
        total_urls = len(urls)

        print(f"ğŸš€ Báº¯t Ä‘áº§u crawl {total_urls} URL...")
        print(f"â±ï¸  Delay giá»¯a cÃ¡c request: {delay} giÃ¢y")
        print("=" * 50)

        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{total_urls}] Processing: {url}")

            result = self.crawl_single_url(url, output_dir)
            results.append(
                {
                    "url": url,
                    "success": result is not None,
                    "output_file": result,
                    "index": i,
                }
            )

            # Delay giá»¯a cÃ¡c request Ä‘á»ƒ trÃ¡nh bá»‹ block
            if i < total_urls:
                print(f"â³ Chá» {delay} giÃ¢y...")
                time.sleep(delay)

        # TÃ³m táº¯t káº¿t quáº£
        print("\n" + "=" * 50)
        print("ğŸ“Š Tá»”NG Káº¾T:")
        successful = sum(1 for r in results if r["success"])
        failed = total_urls - successful

        print(f"âœ… ThÃ nh cÃ´ng: {successful}/{total_urls}")
        print(f"âŒ Tháº¥t báº¡i: {failed}/{total_urls}")

        if successful > 0:
            print(f"\nğŸ“ CÃ¡c file Ä‘Ã£ táº¡o:")
            for result in results:
                if result["success"]:
                    print(f"  - {result['output_file']}")

        return results


# Convenience functions Ä‘á»ƒ sá»­ dá»¥ng dá»… dÃ ng hÆ¡n
def crawl_and_export_to_markdown(
    url: str, output_dir: str = "../processed/"
) -> Optional[str]:
    """
    Function tiá»‡n Ã­ch Ä‘á»ƒ crawl má»™t URL vÃ  export sang markdown

    Args:
        url (str): URL cáº§n crawl
        output_dir (str): ThÆ° má»¥c output

    Returns:
        str: ÄÆ°á»ng dáº«n file Ä‘Ã£ táº¡o, hoáº·c None náº¿u cÃ³ lá»—i
    """
    crawler = ThuvienphapluatCrawler(default_output_dir=output_dir)
    return crawler.crawl_single_url(url, output_dir)


def batch_crawl_urls(
    urls: List[str], output_dir: str = "../processed/", delay: int = 2
) -> List[Dict[str, Any]]:
    """
    Function tiá»‡n Ã­ch Ä‘á»ƒ crawl nhiá»u URL

    Args:
        urls (List[str]): Danh sÃ¡ch URL
        output_dir (str): ThÆ° má»¥c output
        delay (int): Delay giá»¯a requests

    Returns:
        List[Dict]: Káº¿t quáº£ crawling
    """
    crawler = ThuvienphapluatCrawler(default_output_dir=output_dir)
    return crawler.crawl_multiple_urls(urls, output_dir, delay)


# Example usage vÃ  testing
if __name__ == "__main__":
    # Test vá»›i má»™t URL máº«u
    test_url = "https://thuvienphapluat.vn/van-ban/Dau-tu/Nghi-dinh-214-2025-ND-CP-huong-dan-Luat-Dau-thau-ve-lua-chon-nha-thau-668157.aspx"

    print("ğŸ§ª Testing Thuvienphapluat Crawler...")
    print("=" * 50)

    # Test crawl single URL
    result = crawl_and_export_to_markdown(test_url, output_dir="./crawled_data/")

    if result:
        print(f"\nğŸ‰ Test thÃ nh cÃ´ng! File Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {result}")
    else:
        print("\nâŒ Test tháº¥t báº¡i")

    # Example usage vá»›i class
    print("\n" + "=" * 50)
    print("ğŸ“ Example usage vá»›i ThuvienphapluatCrawler class:")
    print(
        """
# Khá»Ÿi táº¡o crawler
crawler = ThuvienphapluatCrawler(default_output_dir='./my_data/')

# Crawl má»™t URL
result = crawler.crawl_single_url('https://thuvienphapluat.vn/van-ban/...')

# Crawl nhiá»u URL
urls = ['url1', 'url2', 'url3']
results = crawler.crawl_multiple_urls(urls, delay=3)
"""
    )
