# ğŸ“„ MD-Preprocess Module

Module tiá»n xá»­ lÃ½ vÄƒn báº£n phÃ¡p luáº­t tá»« file `.md` vá»›i YAML frontmatter thÃ nh chunks tá»‘i Æ°u cho há»‡ thá»‘ng RAG.

## ğŸ¯ TÃ­nh nÄƒng chÃ­nh

- **Parsing YAML frontmatter**: Tá»± Ä‘á»™ng trÃ­ch xuáº¥t metadata tá»« header
- **Optimal chunking**: Káº¿t há»£p by_dieu vÃ  hierarchical smart chunking  
- **Token validation**: Kiá»ƒm tra compatibility vá»›i embedding models
- **Quality scoring**: Tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng chunks
- **Export ready**: JSONL format cho vector databases

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Xá»­ lÃ½ má»™t file Ä‘Æ¡n láº»

```python
from md_processor import MarkdownDocumentProcessor

# Khá»Ÿi táº¡o processor
processor = MarkdownDocumentProcessor(
    max_chunk_size=2000,
    min_chunk_size=300,
    token_limit=6500
)

# Xá»­ lÃ½ file
chunks, report, output_path = processor.process_single_file(
    "path/to/document.md", 
    "output/directory"
)

print(f"Created {len(chunks)} chunks")
print(f"Quality score: {report['summary']['avg_quality_score']:.2f}")
```

### 2. Xá»­ lÃ½ batch nhiá»u files

```python
from utils import process_md_documents_pipeline

chunks, report = process_md_documents_pipeline(
    input_dir="data/processed/",
    output_dir="data/chunks/"
)
```

### 3. PhÃ¢n tÃ­ch document trÆ°á»›c khi chunking

```python
# Parse vÃ  analyze
document = processor.parse_md_file("document.md")
stats = processor.get_document_stats(document)

print(f"Äiá»u count: {stats['dieu_count']}")
print(f"Estimated tokens: {stats['estimated_tokens']}")
```

## ğŸ“Š Káº¿t quáº£ Demo

**Document Analysis:**
- Content: 170,414 chars, 2,215 lines
- Structure: 2 ChÆ°Æ¡ng, 95 Äiá»u, 420 Khoáº£n, 531 Äiá»ƒm  
- Estimated tokens: 60,862

**Chunking Results:**
- Total chunks: 272 (66 dieu + 206 khoan level)
- Average quality score: 0.84/1.0
- Size range: Optimized cho embedding models
- Token efficiency: ~75% utilization

## ğŸ”§ Configuration

### Chunker Parameters

```python
OptimalLegalChunker(
    max_chunk_size=2000,    # Max chars per chunk
    min_chunk_size=300,     # Min chars per chunk  
    token_limit=6500,       # 80% cá»§a model limit
    overlap_size=150        # Context overlap
)
```

### Token Models Supported

- `text-embedding-3-small` (8191 tokens) 
- `text-embedding-3-large` (8191 tokens)
- `text-embedding-ada-002` (8191 tokens)
- Custom models via TokenChecker

## ğŸ“ Cáº¥u trÃºc Output

### JSONL Format
```json
{
  "id": "optimal_dieu_1",
  "text": "[Pháº§n: QUY Äá»ŠNH CHUNG]\\n\\nÄiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh\\n\\n...",
  "metadata": {
    "title": "Luáº­t Äáº¥u tháº§u 2023",
    "source": "thuvienphapluat.vn",
    "dieu": "1",
    "chuong": "CHÆ¯Æ NG I", 
    "chunk_level": "dieu",
    "hierarchy": "QUY Äá»ŠNH CHUNG â†’ CHÆ¯Æ NG I â†’ Äiá»u 1",
    "char_count": 360,
    "token_count": 205,
    "semantic_tags": ["management"],
    "quality_flags": {
      "good_size": true,
      "good_tokens": true, 
      "good_structure": true
    }
  }
}
```

### Quality Metrics

- **High quality** (â‰¥0.8): Structured, proper size, good tokens
- **Medium quality** (0.5-0.8): Minor issues but usable  
- **Low quality** (<0.5): May need manual review

## ğŸ¯ Integration vá»›i RAG System

### 1. Replace existing chunker trong vectorstore.py

```python
from app.data.core.md_preprocess import MarkdownDocumentProcessor

processor = MarkdownDocumentProcessor()
chunks, _ = processor.process_single_file("document.md")
```

### 2. Vector database integration

```python
# Chunks Ä‘Ã£ ready cho embedding
for chunk in chunks:
    vector_db.add(
        text=chunk['text'],
        metadata=chunk['metadata'],
        id=chunk['id']
    )
```

## ğŸ“ˆ Performance

- **Processing speed**: ~1 second cho document 170K chars
- **Memory efficiency**: Stream processing cho large files
- **Token optimization**: ~75% utilization rate
- **Quality consistency**: 84% average quality score

## ğŸ” Semantic Features

### Auto-tagging
- `registration`: Ä‘Äƒng kÃ½, há»‡ thá»‘ng máº¡ng
- `timeline`: thá»i gian, thá»i háº¡n
- `procedures`: thá»§ tá»¥c, quy trÃ¬nh  
- `documentation`: há»“ sÆ¡, tÃ i liá»‡u
- `management`: quáº£n lÃ½, giÃ¡m sÃ¡t
- `penalties`: xá»­ pháº¡t, vi pháº¡m
- `requirements`: yÃªu cáº§u, Ä‘iá»u kiá»‡n

### Context Enhancement  
- Hierarchical headers: `[Pháº§n: QUY Äá»ŠNH CHUNG] [CHÆ¯Æ NG I]`
- Breadcrumb navigation: `"hierarchy": "Section â†’ Chapter â†’ Article"`
- Parent-child relationships: Äiá»u â†’ Khoáº£n â†’ Äiá»ƒm

## ğŸš€ Roadmap

- **Phase 1**: âœ… Core chunking implementation  
- **Phase 2**: ğŸ”„ Integration vá»›i RAG system
- **Phase 3**: â³ ML-enhanced semantic chunking
- **Phase 4**: â³ Performance optimization
- **Phase 5**: â³ Multi-document processing

---

ğŸ“ **Support**: Module Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam vá»›i cáº¥u trÃºc hierarchical rÃµ rÃ ng.