"""
Hybrid Semantic Cache V2 - Threshold and Performance Test

Tests:
1. Threshold analysis with BGE reranker
2. Speed comparison: V1 (Cosine only) vs V2 (Hybrid)
3. Accuracy comparison with different cache sizes

Usage:
    cd RAG-bidding
    PYTHONPATH=$(pwd) python scripts/analysis/test_semantic_cache_v2.py
"""

import numpy as np
import time
from typing import List, Tuple, Dict
from dataclasses import dataclass
import sys

# Add project path
sys.path.insert(0, "/home/sakana/Code/RAG-project/RAG-bidding")

from src.embedding.embedders.openai_embedder import OpenAIEmbedder
from src.retrieval.ranking.bge_reranker import get_singleton_reranker


@dataclass
class TestPair:
    """Test case for similarity comparison."""

    q1: str
    q2: str
    category: str  # "similar", "different", "identical"
    description: str = ""


def get_test_pairs() -> List[TestPair]:
    """Define comprehensive test pairs."""
    return [
        # =====================================================================
        # SIMILAR PAIRS (should match)
        # =====================================================================
        TestPair(
            "YÃªu cáº§u vá» nÄƒng lá»±c tÃ i chÃ­nh cá»§a nhÃ  tháº§u gá»“m nhá»¯ng gÃ¬",
            "Äiá»u kiá»‡n vá» nÄƒng lá»±c tÃ i chÃ­nh khi tham gia Ä‘áº¥u tháº§u yÃªu cáº§u nhá»¯ng gÃ¬",
            "similar",
            "NÄƒng lá»±c tÃ i chÃ­nh",
        ),
        TestPair(
            "Há»“ sÆ¡ má»i tháº§u bao gá»“m nhá»¯ng ná»™i dung gÃ¬",
            "ThÃ nh pháº§n cá»§a há»“ sÆ¡ má»i tháº§u gá»“m nhá»¯ng gÃ¬",
            "similar",
            "Há»“ sÆ¡ má»i tháº§u",
        ),
        TestPair(
            "Thá»i gian ná»™p há»“ sÆ¡ dá»± tháº§u lÃ  bao lÃ¢u",
            "NhÃ  tháº§u cÃ³ bao nhiÃªu ngÃ y Ä‘á»ƒ ná»™p há»“ sÆ¡",
            "similar",
            "Thá»i gian ná»™p HSDT",
        ),
        TestPair(
            "Quy Ä‘á»‹nh vá» báº£o lÃ£nh dá»± tháº§u nhÆ° tháº¿ nÃ o",
            "YÃªu cáº§u báº£o Ä‘áº£m dá»± tháº§u theo quy Ä‘á»‹nh lÃ  gÃ¬",
            "similar",
            "Báº£o lÃ£nh dá»± tháº§u",
        ),
        TestPair(
            "TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡ há»“ sÆ¡ dá»± tháº§u gá»“m nhá»¯ng gÃ¬",
            "CÃ¡c tiÃªu chuáº©n Ä‘á»ƒ cháº¥m Ä‘iá»ƒm há»“ sÆ¡ tháº§u",
            "similar",
            "TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡",
        ),
        TestPair(
            "CÃ¡c loáº¡i há»£p Ä‘á»“ng trong Ä‘áº¥u tháº§u",
            "PhÃ¢n loáº¡i há»£p Ä‘á»“ng Ä‘áº¥u tháº§u theo quy Ä‘á»‹nh",
            "similar",
            "Loáº¡i há»£p Ä‘á»“ng",
        ),
        TestPair(
            "Quy trÃ¬nh má»Ÿ tháº§u diá»…n ra nhÆ° tháº¿ nÃ o",
            "CÃ¡c bÆ°á»›c trong buá»•i má»Ÿ há»“ sÆ¡ dá»± tháº§u",
            "similar",
            "Quy trÃ¬nh má»Ÿ tháº§u",
        ),
        TestPair(
            "TrÆ°á»ng há»£p nÃ o Ä‘Æ°á»£c phÃ©p Ã¡p dá»¥ng chá»‰ Ä‘á»‹nh tháº§u",
            "Äiá»u kiá»‡n Ä‘á»ƒ thá»±c hiá»‡n chá»‰ Ä‘á»‹nh tháº§u trá»±c tiáº¿p lÃ  gÃ¬",
            "similar",
            "Chá»‰ Ä‘á»‹nh tháº§u",
        ),
        TestPair(
            "Nhá»¯ng hÃ nh vi nÃ o bá»‹ nghiÃªm cáº¥m trong Ä‘áº¥u tháº§u",
            "CÃ¡c lá»—i vi pháº¡m dáº«n Ä‘áº¿n bá»‹ cáº¥m tham gia hoáº¡t Ä‘á»™ng Ä‘áº¥u tháº§u",
            "similar",
            "HÃ nh vi cáº¥m",
        ),
        TestPair(
            "Ai lÃ  ngÆ°á»i cÃ³ tháº©m quyá»n quyáº¿t Ä‘á»‹nh há»§y tháº§u",
            "Viá»‡c há»§y tháº§u do cáº¥p nÃ o cÃ³ tháº©m quyá»n phÃª duyá»‡t",
            "similar",
            "Tháº©m quyá»n há»§y tháº§u",
        ),
        # =====================================================================
        # DIFFERENT TOPIC PAIRS (should NOT match - tricky cases)
        # =====================================================================
        TestPair(
            "Quy Ä‘á»‹nh vá» báº£o Ä‘áº£m dá»± tháº§u",
            "Quy Ä‘á»‹nh vá» báº£o Ä‘áº£m thá»±c hiá»‡n há»£p Ä‘á»“ng",
            "different",
            "Báº£o Ä‘áº£m dá»± tháº§u vs báº£o Ä‘áº£m HÄ",
        ),
        TestPair(
            "Ná»™i dung chÃ­nh cá»§a há»“ sÆ¡ má»i tháº§u (HSMT)",
            "Ná»™i dung chÃ­nh cá»§a há»“ sÆ¡ dá»± tháº§u (HSDT)",
            "different",
            "HSMT vs HSDT",
        ),
        TestPair(
            "CÃ¡c trÆ°á»ng há»£p Ä‘Æ°á»£c phÃ©p chá»‰ Ä‘á»‹nh tháº§u",
            "CÃ¡c trÆ°á»ng há»£p khÃ´ng Ä‘Æ°á»£c phÃ©p chá»‰ Ä‘á»‹nh tháº§u",
            "different",
            "ÄÆ°á»£c vs khÃ´ng Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh tháº§u",
        ),
        TestPair(
            "Thá»i gian cÃ³ hiá»‡u lá»±c lÃ  90 ngÃ y",
            "Thá»i gian cÃ³ hiá»‡u lá»±c lÃ  120 ngÃ y",
            "different",
            "90 ngÃ y vs 120 ngÃ y",
        ),
        TestPair(
            "PhÆ°Æ¡ng phÃ¡p giÃ¡ tháº¥p nháº¥t",
            "PhÆ°Æ¡ng phÃ¡p giÃ¡ Ä‘Ã¡nh giÃ¡",
            "different",
            "PhÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ khÃ¡c",
        ),
        TestPair(
            "TrÃ¡ch nhiá»‡m cá»§a nhÃ  tháº§u chÃ­nh trong gÃ³i tháº§u",
            "Quy Ä‘á»‹nh vá» pháº§n viá»‡c cá»§a nhÃ  tháº§u phá»¥",
            "different",
            "NhÃ  tháº§u chÃ­nh vs phá»¥",
        ),
        TestPair(
            "YÃªu cáº§u vá» nÄƒng lá»±c tÃ i chÃ­nh cá»§a nhÃ  tháº§u",
            "Quy trÃ¬nh má»Ÿ tháº§u diá»…n ra nhÆ° tháº¿ nÃ o",
            "different",
            "Topics hoÃ n toÃ n khÃ¡c",
        ),
        # =====================================================================
        # IDENTICAL (baseline)
        # =====================================================================
        TestPair(
            "YÃªu cáº§u vá» nÄƒng lá»±c tÃ i chÃ­nh cá»§a nhÃ  tháº§u gá»“m nhá»¯ng gÃ¬",
            "YÃªu cáº§u vá» nÄƒng lá»±c tÃ i chÃ­nh cá»§a nhÃ  tháº§u gá»“m nhá»¯ng gÃ¬",
            "identical",
            "Identical query",
        ),
    ]


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """Compute cosine similarity."""
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


def test_bge_threshold(test_pairs: List[TestPair], reranker) -> Dict:
    """Test BGE scores and find optimal threshold."""

    print("\n" + "=" * 80)
    print("BGE CROSS-ENCODER THRESHOLD ANALYSIS")
    print("=" * 80)

    similar_scores = []
    different_scores = []
    identical_scores = []

    # Prepare all pairs
    all_pairs = [[p.q1, p.q2] for p in test_pairs]

    # Get BGE scores in batch
    start_time = time.time()
    scores = reranker.model.predict(all_pairs, show_progress_bar=False)
    bge_time_ms = (time.time() - start_time) * 1000

    print(f"\nâ±ï¸  BGE batch inference: {bge_time_ms:.1f}ms for {len(all_pairs)} pairs")
    print(f"   Per pair: {bge_time_ms/len(all_pairs):.1f}ms")

    print(f"\nðŸ“Š Scores by category:")

    print(f"\n   SIMILAR pairs (should match):")
    for pair, score in zip(test_pairs, scores):
        score = float(score)
        if pair.category == "similar":
            similar_scores.append(score)
            print(f"      {score:.4f} | {pair.description}")
        elif pair.category == "different":
            different_scores.append(score)
        else:
            identical_scores.append(score)

    print(f"\n   DIFFERENT topic pairs (should NOT match):")
    for pair, score in zip(test_pairs, scores):
        score = float(score)
        if pair.category == "different":
            print(f"      {score:.4f} | {pair.description}")

    print(f"\n   IDENTICAL pairs:")
    for pair, score in zip(test_pairs, scores):
        score = float(score)
        if pair.category == "identical":
            print(f"      {score:.4f} | {pair.description}")

    # Statistics
    print(f"\nðŸ“Š Statistics:")
    print(
        f"   Similar:   min={min(similar_scores):.4f}, max={max(similar_scores):.4f}, mean={np.mean(similar_scores):.4f}"
    )
    print(
        f"   Different: min={min(different_scores):.4f}, max={max(different_scores):.4f}, mean={np.mean(different_scores):.4f}"
    )

    # Check separation
    min_similar = min(similar_scores)
    max_different = max(different_scores)
    separation = min_similar - max_different

    print(f"\nðŸ“Š Score Separation:")
    print(f"   Min similar:  {min_similar:.4f}")
    print(f"   Max different: {max_different:.4f}")
    print(
        f"   Separation:   {separation:.4f} {'âœ… Clear separation!' if separation > 0 else 'âŒ Overlap detected'}"
    )

    # Threshold analysis
    print(f"\nðŸ“Š Threshold Analysis:")
    print(f"   Threshold | Similar Hit | Different FP | Accuracy")
    print(f"   " + "-" * 50)

    best_threshold = 0
    best_accuracy = 0

    for t in np.arange(0.0, 1.01, 0.05):
        similar_hit = sum(1 for s in similar_scores if s >= t)
        different_fp = sum(1 for s in different_scores if s >= t)
        total = len(similar_scores) + len(different_scores)
        accuracy = (similar_hit + (len(different_scores) - different_fp)) / total

        marker = ""
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_threshold = t
            marker = " â† best"

        print(
            f"      {t:.2f}   |   {similar_hit:2d}/{len(similar_scores)}     |     {different_fp}/{len(different_scores)}      |  {accuracy:.1%}{marker}"
        )

    print(f"\nðŸŽ¯ RECOMMENDED BGE THRESHOLD: {best_threshold:.2f}")
    print(f"   Accuracy: {best_accuracy:.1%}")
    print(
        f"   Similar hit rate: {sum(1 for s in similar_scores if s >= best_threshold)}/{len(similar_scores)}"
    )
    print(
        f"   False positive rate: {sum(1 for s in different_scores if s >= best_threshold)}/{len(different_scores)}"
    )

    return {
        "similar_scores": similar_scores,
        "different_scores": different_scores,
        "best_threshold": best_threshold,
        "best_accuracy": best_accuracy,
        "separation": separation,
    }


def test_speed_comparison(embedder, reranker, num_cached_queries: int = 100) -> Dict:
    """Compare speed of Cosine-only vs Hybrid approach."""

    print("\n" + "=" * 80)
    print(f"SPEED COMPARISON: COSINE vs HYBRID (cache size = {num_cached_queries})")
    print("=" * 80)

    # Generate fake cached queries and embeddings
    print(f"\nðŸ”§ Generating {num_cached_queries} cached query embeddings...")

    # Use a base query and variations
    base_queries = [
        "YÃªu cáº§u vá» nÄƒng lá»±c tÃ i chÃ­nh cá»§a nhÃ  tháº§u",
        "Quy trÃ¬nh Ä‘áº¥u tháº§u Ä‘iá»‡n tá»­",
        "Äiá»u kiá»‡n tham gia Ä‘áº¥u tháº§u",
        "Há»“ sÆ¡ má»i tháº§u cáº§n nhá»¯ng gÃ¬",
        "Thá»i gian ná»™p há»“ sÆ¡ dá»± tháº§u",
    ]

    # Generate embeddings for cached queries
    cached_embeddings = []
    cached_queries = []

    for i in range(num_cached_queries):
        query = f"{base_queries[i % len(base_queries)]} - variation {i}"
        cached_queries.append(query)

    # Batch embed (this is just for setup, not timed)
    print("   Computing embeddings for cached queries...")
    start = time.time()
    for q in cached_queries[:20]:  # Only compute 20 for demo
        emb = np.array(embedder.embed_query(q), dtype=np.float32)
        cached_embeddings.append(emb)

    # Fill rest with random embeddings (to save API calls)
    dim = cached_embeddings[0].shape[0]
    for _ in range(num_cached_queries - 20):
        cached_embeddings.append(np.random.randn(dim).astype(np.float32))

    embed_time = time.time() - start
    print(f"   Done in {embed_time:.1f}s")

    # Test query
    test_query = (
        "Äiá»u kiá»‡n vá» nÄƒng lá»±c tÃ i chÃ­nh khi tham gia Ä‘áº¥u tháº§u yÃªu cáº§u nhá»¯ng gÃ¬"
    )

    print(f"\nðŸ“ Test query: {test_query}")

    # Method 1: Cosine-only (current V1)
    print(f"\nðŸ” Method 1: Cosine-only (V1)")

    start = time.time()
    query_embedding = np.array(embedder.embed_query(test_query), dtype=np.float32)
    embed_time_ms = (time.time() - start) * 1000

    start = time.time()
    cosine_scores = []
    for i, cached_emb in enumerate(cached_embeddings):
        sim = cosine_similarity(query_embedding, cached_emb)
        cosine_scores.append((i, sim))
    cosine_scores.sort(key=lambda x: x[1], reverse=True)
    cosine_time_ms = (time.time() - start) * 1000

    top_cosine = cosine_scores[:5]

    print(f"   Embedding time: {embed_time_ms:.1f}ms")
    print(
        f"   Cosine scan time: {cosine_time_ms:.2f}ms for {num_cached_queries} queries"
    )
    print(f"   Total V1: {embed_time_ms + cosine_time_ms:.1f}ms")
    print(f"   Top 5 cosine scores: {[round(s, 4) for _, s in top_cosine]}")

    # Method 2: Hybrid (Cosine + BGE)
    print(f"\nðŸ” Method 2: Hybrid Cosine + BGE (V2)")

    # Step 1: Cosine pre-filter (top 20)
    cosine_threshold = 0.3
    top_k = 20

    start = time.time()
    candidates = [(i, s) for i, s in cosine_scores if s >= cosine_threshold][:top_k]
    cosine_filter_time_ms = (time.time() - start) * 1000

    print(
        f"   Cosine pre-filter: {len(candidates)} candidates in {cosine_filter_time_ms:.2f}ms"
    )

    # Step 2: BGE rerank
    if candidates:
        pairs = [
            [test_query, cached_queries[i] if i < len(cached_queries) else f"Query {i}"]
            for i, _ in candidates
        ]

        start = time.time()
        bge_scores = reranker.model.predict(pairs, show_progress_bar=False)
        bge_time_ms = (time.time() - start) * 1000

        # Find best
        best_idx = np.argmax(bge_scores)
        best_bge_score = float(bge_scores[best_idx])

        print(
            f"   BGE rerank time: {bge_time_ms:.1f}ms for {len(candidates)} candidates"
        )
        print(f"   Best BGE score: {best_bge_score:.4f}")
    else:
        bge_time_ms = 0
        print(f"   No candidates to rerank")

    total_v2 = embed_time_ms + cosine_time_ms + bge_time_ms
    print(f"   Total V2: {total_v2:.1f}ms")

    # Comparison
    print(f"\nðŸ“Š Speed Comparison Summary:")
    print(f"   V1 (Cosine-only):  {embed_time_ms + cosine_time_ms:.1f}ms")
    print(f"   V2 (Hybrid):       {total_v2:.1f}ms")
    print(
        f"   Overhead:          {total_v2 - (embed_time_ms + cosine_time_ms):.1f}ms (BGE rerank)"
    )

    # Simulate larger cache
    print(f"\nðŸ“Š Projected Speed at Different Cache Sizes:")
    print(f"   Cache Size | V1 (Cosine) | V2 (Hybrid) | V2 Overhead")
    print(f"   " + "-" * 55)

    for size in [100, 500, 1000, 5000, 10000]:
        # V1: Embedding + Cosine scan (linear)
        v1_cosine = cosine_time_ms * (size / num_cached_queries)
        v1_total = embed_time_ms + v1_cosine

        # V2: Embedding + Cosine scan + BGE (fixed 20 candidates)
        v2_cosine = v1_cosine
        v2_bge = bge_time_ms  # Fixed - always 20 candidates
        v2_total = embed_time_ms + v2_cosine + v2_bge

        overhead = v2_total - v1_total
        overhead_pct = overhead / v1_total * 100 if v1_total > 0 else 0

        print(
            f"   {size:6d}    |  {v1_total:8.1f}ms |  {v2_total:8.1f}ms |  +{overhead:.1f}ms ({overhead_pct:.1f}%)"
        )

    return {
        "embed_time_ms": embed_time_ms,
        "cosine_time_ms": cosine_time_ms,
        "bge_time_ms": bge_time_ms,
        "total_v1": embed_time_ms + cosine_time_ms,
        "total_v2": total_v2,
    }


def test_accuracy_simulation(reranker, test_pairs: List[TestPair]) -> Dict:
    """Simulate cache and test accuracy."""

    print("\n" + "=" * 80)
    print("ACCURACY SIMULATION: V1 (Cosine) vs V2 (Hybrid)")
    print("=" * 80)

    embedder = OpenAIEmbedder()

    # Build cache from q1 of each similar pair
    similar_pairs = [p for p in test_pairs if p.category == "similar"]

    print(f"\nðŸ”§ Building cache from {len(similar_pairs)} queries...")

    cache = {}  # query -> embedding
    for pair in similar_pairs:
        emb = np.array(embedder.embed_query(pair.q1), dtype=np.float32)
        cache[pair.q1] = emb

    # Test lookup with q2 of each similar pair
    print(f"\nðŸ” Testing lookup with paraphrased queries...")

    cosine_threshold_v1 = 0.70  # V1 threshold (if we use cosine-only)
    bge_threshold_v2 = 0.85  # V2 threshold

    v1_hits = 0
    v2_hits = 0

    print(
        f"\n   Query | V1 (Cosineâ‰¥{cosine_threshold_v1}) | V2 (BGEâ‰¥{bge_threshold_v2})"
    )
    print(f"   " + "-" * 70)

    for pair in similar_pairs:
        query = pair.q2  # Paraphrased query
        query_emb = np.array(embedder.embed_query(query), dtype=np.float32)

        # V1: Cosine lookup
        best_cosine = 0
        best_match = None
        for cached_q, cached_emb in cache.items():
            sim = cosine_similarity(query_emb, cached_emb)
            if sim > best_cosine:
                best_cosine = sim
                best_match = cached_q

        v1_hit = best_cosine >= cosine_threshold_v1
        if v1_hit:
            v1_hits += 1

        # V2: BGE on best cosine match
        if best_match:
            bge_pairs = [[query, best_match]]
            bge_score = float(
                reranker.model.predict(bge_pairs, show_progress_bar=False)[0]
            )
            v2_hit = bge_score >= bge_threshold_v2
        else:
            bge_score = 0
            v2_hit = False

        if v2_hit:
            v2_hits += 1

        v1_marker = "âœ…" if v1_hit else "âŒ"
        v2_marker = "âœ…" if v2_hit else "âŒ"

        print(
            f"   {pair.description[:25]:25s} | {v1_marker} {best_cosine:.4f}         | {v2_marker} {bge_score:.4f}"
        )

    print(f"\nðŸ“Š Accuracy Results:")
    print(
        f"   V1 (Cosineâ‰¥{cosine_threshold_v1}): {v1_hits}/{len(similar_pairs)} = {v1_hits/len(similar_pairs):.1%}"
    )
    print(
        f"   V2 (BGEâ‰¥{bge_threshold_v2}):    {v2_hits}/{len(similar_pairs)} = {v2_hits/len(similar_pairs):.1%}"
    )

    return {
        "v1_hits": v1_hits,
        "v2_hits": v2_hits,
        "total": len(similar_pairs),
        "v1_accuracy": v1_hits / len(similar_pairs),
        "v2_accuracy": v2_hits / len(similar_pairs),
    }


def main():
    print("=" * 80)
    print("HYBRID SEMANTIC CACHE V2 - THRESHOLD AND PERFORMANCE TEST")
    print("=" * 80)

    # Initialize
    print("\nðŸ”§ Initializing models...")
    embedder = OpenAIEmbedder()
    print("   âœ… OpenAI Embedder loaded")

    reranker = get_singleton_reranker()
    print(f"   âœ… BGE Reranker loaded (device: {reranker.device})")

    # Get test pairs
    test_pairs = get_test_pairs()

    # Test 1: BGE Threshold Analysis
    threshold_results = test_bge_threshold(test_pairs, reranker)

    # Test 2: Speed Comparison
    speed_results = test_speed_comparison(embedder, reranker, num_cached_queries=100)

    # Test 3: Accuracy Simulation
    accuracy_results = test_accuracy_simulation(reranker, test_pairs)

    # Final Summary
    print("\n" + "=" * 80)
    print("FINAL SUMMARY & RECOMMENDATIONS")
    print("=" * 80)

    print(
        f"""
    ðŸŽ¯ BGE Threshold:
       Recommended: {threshold_results['best_threshold']:.2f}
       Accuracy: {threshold_results['best_accuracy']:.1%}
       Score separation: {threshold_results['separation']:.4f}
    
    â±ï¸  Speed (100 cached queries):
       V1 (Cosine-only): {speed_results['total_v1']:.1f}ms
       V2 (Hybrid):      {speed_results['total_v2']:.1f}ms
       BGE overhead:     {speed_results['bge_time_ms']:.1f}ms
    
    ðŸŽ¯ Accuracy:
       V1 (Cosine): {accuracy_results['v1_accuracy']:.1%}
       V2 (Hybrid): {accuracy_results['v2_accuracy']:.1%}
    
    âœ… CONCLUSION:
       - V2 Hybrid is {'BETTER' if accuracy_results['v2_accuracy'] > accuracy_results['v1_accuracy'] else 'SIMILAR'} in accuracy
       - V2 adds ~{speed_results['bge_time_ms']:.0f}ms overhead for BGE reranking
       - BGE provides better separation for tricky cases (opposite meanings, similar wording)
       - Recommended: Use V2 with BGE threshold = {threshold_results['best_threshold']:.2f}
    """
    )


if __name__ == "__main__":
    main()
