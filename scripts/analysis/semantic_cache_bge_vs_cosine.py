"""
BGE Cross-Encoder vs OpenAI Embeddings + Cosine Similarity

So s√°nh 2 ph∆∞∆°ng ph√°p cho Semantic Cache:
1. OpenAI Embeddings + Cosine Similarity (hi·ªán t·∫°i)
2. BGE Cross-Encoder Reranker (ƒë·ªÅ xu·∫•t)

ƒê√°nh gi√°:
- ƒê·ªô ch√≠nh x√°c ph√¢n bi·ªát similar vs different topic
- Th·ªùi gian x·ª≠ l√Ω
- Trade-offs

Usage:
    cd RAG-bidding
    PYTHONPATH=/home/sakana/Code/RAG-project/RAG-bidding python scripts/analysis/semantic_cache_bge_vs_cosine.py
"""

import numpy as np
import time
from typing import List, Tuple
from dataclasses import dataclass

# Import project modules
from src.embedding.embedders.openai_embedder import OpenAIEmbedder
from src.retrieval.ranking.bge_reranker import get_singleton_reranker


@dataclass
class TestPair:
    """Test case for similarity comparison."""
    q1: str
    q2: str
    category: str  # "similar", "different", "identical"
    description: str = ""


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """Compute cosine similarity between two vectors."""
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


def get_test_pairs() -> List[TestPair]:
    """Define test pairs for evaluation."""
    return [
        # =====================================================================
        # SIMILAR PAIRS (should match with semantic cache)
        # =====================================================================
        TestPair("Y√™u c·∫ßu v·ªÅ nƒÉng l·ª±c t√†i ch√≠nh c·ªßa nh√† th·∫ßu g·ªìm nh·ªØng g√¨",
                 "ƒêi·ªÅu ki·ªán v·ªÅ nƒÉng l·ª±c t√†i ch√≠nh khi tham gia ƒë·∫•u th·∫ßu y√™u c·∫ßu nh·ªØng g√¨",
                 "similar", "NƒÉng l·ª±c t√†i ch√≠nh - original case"),
        
        TestPair("H·ªì s∆° m·ªùi th·∫ßu bao g·ªìm nh·ªØng n·ªôi dung g√¨",
                 "Th√†nh ph·∫ßn c·ªßa h·ªì s∆° m·ªùi th·∫ßu g·ªìm nh·ªØng g√¨",
                 "similar", "H·ªì s∆° m·ªùi th·∫ßu"),
        
        TestPair("Th·ªùi gian n·ªôp h·ªì s∆° d·ª± th·∫ßu l√† bao l√¢u",
                 "Nh√† th·∫ßu c√≥ bao nhi√™u ng√†y ƒë·ªÉ n·ªôp h·ªì s∆°",
                 "similar", "Th·ªùi gian n·ªôp HSDT"),
        
        TestPair("Quy ƒë·ªãnh v·ªÅ b·∫£o l√£nh d·ª± th·∫ßu nh∆∞ th·∫ø n√†o",
                 "Y√™u c·∫ßu b·∫£o ƒë·∫£m d·ª± th·∫ßu theo quy ƒë·ªãnh l√† g√¨",
                 "similar", "B·∫£o l√£nh d·ª± th·∫ßu"),
        
        TestPair("Ti√™u ch√≠ ƒë√°nh gi√° h·ªì s∆° d·ª± th·∫ßu g·ªìm nh·ªØng g√¨",
                 "C√°c ti√™u chu·∫©n ƒë·ªÉ ch·∫•m ƒëi·ªÉm h·ªì s∆° th·∫ßu",
                 "similar", "Ti√™u ch√≠ ƒë√°nh gi√° HSDT"),
        
        TestPair("C√°c lo·∫°i h·ª£p ƒë·ªìng trong ƒë·∫•u th·∫ßu",
                 "Ph√¢n lo·∫°i h·ª£p ƒë·ªìng ƒë·∫•u th·∫ßu theo quy ƒë·ªãnh",
                 "similar", "Lo·∫°i h·ª£p ƒë·ªìng"),
        
        TestPair("Quy tr√¨nh m·ªü th·∫ßu di·ªÖn ra nh∆∞ th·∫ø n√†o",
                 "C√°c b∆∞·ªõc trong bu·ªïi m·ªü h·ªì s∆° d·ª± th·∫ßu",
                 "similar", "Quy tr√¨nh m·ªü th·∫ßu"),
        
        TestPair("Tr∆∞·ªùng h·ª£p n√†o ƒë∆∞·ª£c ph√©p √°p d·ª•ng ch·ªâ ƒë·ªãnh th·∫ßu",
                 "ƒêi·ªÅu ki·ªán ƒë·ªÉ th·ª±c hi·ªán ch·ªâ ƒë·ªãnh th·∫ßu tr·ª±c ti·∫øp l√† g√¨",
                 "similar", "Ch·ªâ ƒë·ªãnh th·∫ßu"),
        
        TestPair("Nh·ªØng h√†nh vi n√†o b·ªã nghi√™m c·∫•m trong ƒë·∫•u th·∫ßu",
                 "C√°c l·ªói vi ph·∫°m d·∫´n ƒë·∫øn b·ªã c·∫•m tham gia ho·∫°t ƒë·ªông ƒë·∫•u th·∫ßu",
                 "similar", "H√†nh vi c·∫•m"),
        
        TestPair("Ai l√† ng∆∞·ªùi c√≥ th·∫©m quy·ªÅn quy·∫øt ƒë·ªãnh h·ªßy th·∫ßu",
                 "Vi·ªác h·ªßy th·∫ßu do c·∫•p n√†o c√≥ th·∫©m quy·ªÅn ph√™ duy·ªát",
                 "similar", "Th·∫©m quy·ªÅn h·ªßy th·∫ßu"),
        
        # =====================================================================
        # DIFFERENT TOPIC PAIRS (should NOT match - tricky cases)
        # =====================================================================
        TestPair("Quy ƒë·ªãnh v·ªÅ b·∫£o ƒë·∫£m d·ª± th·∫ßu",
                 "Quy ƒë·ªãnh v·ªÅ b·∫£o ƒë·∫£m th·ª±c hi·ªán h·ª£p ƒë·ªìng",
                 "different", "B·∫£o ƒë·∫£m d·ª± th·∫ßu vs b·∫£o ƒë·∫£m Hƒê - kh√°c nhau!"),
        
        TestPair("N·ªôi dung ch√≠nh c·ªßa h·ªì s∆° m·ªùi th·∫ßu (HSMT)",
                 "N·ªôi dung ch√≠nh c·ªßa h·ªì s∆° d·ª± th·∫ßu (HSDT)",
                 "different", "HSMT vs HSDT - kh√°c nhau!"),
        
        TestPair("C√°c tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c ph√©p ch·ªâ ƒë·ªãnh th·∫ßu",
                 "C√°c tr∆∞·ªùng h·ª£p kh√¥ng ƒë∆∞·ª£c ph√©p ch·ªâ ƒë·ªãnh th·∫ßu",
                 "different", "ƒê∆∞·ª£c ph√©p vs kh√¥ng ƒë∆∞·ª£c ph√©p - ng∆∞·ª£c nhau!"),
        
        TestPair("Th·ªùi gian c√≥ hi·ªáu l·ª±c l√† 90 ng√†y",
                 "Th·ªùi gian c√≥ hi·ªáu l·ª±c l√† 120 ng√†y",
                 "different", "90 ng√†y vs 120 ng√†y - s·ªë kh√°c nhau!"),
        
        TestPair("Ph∆∞∆°ng ph√°p gi√° th·∫•p nh·∫•t",
                 "Ph∆∞∆°ng ph√°p gi√° ƒë√°nh gi√°",
                 "different", "Ph∆∞∆°ng ph√°p ƒë√°nh gi√° kh√°c nhau!"),
        
        TestPair("Tr√°ch nhi·ªám c·ªßa nh√† th·∫ßu ch√≠nh trong g√≥i th·∫ßu",
                 "Quy ƒë·ªãnh v·ªÅ ph·∫ßn vi·ªác c·ªßa nh√† th·∫ßu ph·ª•",
                 "different", "Nh√† th·∫ßu ch√≠nh vs nh√† th·∫ßu ph·ª•"),
        
        TestPair("Y√™u c·∫ßu v·ªÅ nƒÉng l·ª±c t√†i ch√≠nh c·ªßa nh√† th·∫ßu",
                 "Quy tr√¨nh m·ªü th·∫ßu di·ªÖn ra nh∆∞ th·∫ø n√†o",
                 "different", "Topics ho√†n to√†n kh√°c nhau"),
        
        # =====================================================================
        # IDENTICAL PAIRS (baseline - should always match)
        # =====================================================================
        TestPair("Y√™u c·∫ßu v·ªÅ nƒÉng l·ª±c t√†i ch√≠nh c·ªßa nh√† th·∫ßu g·ªìm nh·ªØng g√¨",
                 "Y√™u c·∫ßu v·ªÅ nƒÉng l·ª±c t√†i ch√≠nh c·ªßa nh√† th·∫ßu g·ªìm nh·ªØng g√¨",
                 "identical", "Identical query"),
    ]


def evaluate_openai_cosine(
    test_pairs: List[TestPair],
    embedder: OpenAIEmbedder
) -> Tuple[List[float], List[float], List[float], float]:
    """
    Evaluate OpenAI embeddings + cosine similarity.
    
    Returns:
        (similar_scores, different_scores, identical_scores, total_time_ms)
    """
    similar_scores = []
    different_scores = []
    identical_scores = []
    
    start_time = time.time()
    
    for pair in test_pairs:
        e1 = np.array(embedder.embed_query(pair.q1))
        e2 = np.array(embedder.embed_query(pair.q2))
        sim = cosine_similarity(e1, e2)
        
        if pair.category == "similar":
            similar_scores.append(sim)
        elif pair.category == "different":
            different_scores.append(sim)
        else:
            identical_scores.append(sim)
    
    total_time_ms = (time.time() - start_time) * 1000
    
    return similar_scores, different_scores, identical_scores, total_time_ms


def evaluate_bge_crossencoder(
    test_pairs: List[TestPair],
    reranker
) -> Tuple[List[float], List[float], List[float], float]:
    """
    Evaluate BGE cross-encoder scores.
    
    Returns:
        (similar_scores, different_scores, identical_scores, total_time_ms)
    """
    similar_scores = []
    different_scores = []
    identical_scores = []
    
    start_time = time.time()
    
    # BGE CrossEncoder.predict() takes list of [query, document] pairs
    pairs = [[pair.q1, pair.q2] for pair in test_pairs]
    
    # Get scores in batch
    scores = reranker.model.predict(pairs, show_progress_bar=False)
    
    for pair, score in zip(test_pairs, scores):
        score = float(score)
        if pair.category == "similar":
            similar_scores.append(score)
        elif pair.category == "different":
            different_scores.append(score)
        else:
            identical_scores.append(score)
    
    total_time_ms = (time.time() - start_time) * 1000
    
    return similar_scores, different_scores, identical_scores, total_time_ms


def analyze_threshold(similar: List[float], different: List[float], method_name: str):
    """Analyze optimal threshold for a method."""
    
    max_different = max(different)
    min_similar = min(similar)
    
    # Check if there's a clear separation
    has_clear_separation = min_similar > max_different
    
    print(f"\n  üìä {method_name} Threshold Analysis:")
    print(f"     Similar range:   [{min(similar):.4f}, {max(similar):.4f}]")
    print(f"     Different range: [{min(different):.4f}, {max(different):.4f}]")
    
    if has_clear_separation:
        optimal_threshold = (max_different + min_similar) / 2
        print(f"     ‚úÖ Clear separation! Optimal threshold: {optimal_threshold:.4f}")
        print(f"        ‚Üí Would catch 100% similar, 0% false positives")
    else:
        overlap = max_different - min_similar
        print(f"     ‚ùå Overlap detected: {overlap:.4f}")
        
        # Find threshold that minimizes errors
        all_scores = [(s, 'similar') for s in similar] + [(s, 'different') for s in different]
        all_scores.sort(key=lambda x: x[0])
        
        best_threshold = 0
        best_accuracy = 0
        
        for threshold in np.arange(min(similar) - 0.1, max(different) + 0.1, 0.01):
            similar_caught = sum(1 for s in similar if s >= threshold)
            different_rejected = sum(1 for s in different if s < threshold)
            accuracy = (similar_caught + different_rejected) / (len(similar) + len(different))
            
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_threshold = threshold
        
        print(f"     Best threshold: {best_threshold:.4f} (accuracy: {best_accuracy:.1%})")
        
        # Calculate recall at best threshold
        similar_caught = sum(1 for s in similar if s >= best_threshold)
        false_positives = sum(1 for s in different if s >= best_threshold)
        print(f"     At {best_threshold:.4f}: {similar_caught}/{len(similar)} similar caught, {false_positives}/{len(different)} false positives")


def main():
    print("=" * 80)
    print("BGE CROSS-ENCODER vs OPENAI EMBEDDINGS + COSINE SIMILARITY")
    print("=" * 80)
    
    # Initialize
    print("\nüîß Initializing models...")
    embedder = OpenAIEmbedder()
    print("   ‚úÖ OpenAI Embedder loaded")
    
    reranker = get_singleton_reranker()
    print("   ‚úÖ BGE Reranker loaded")
    
    # Get test pairs
    test_pairs = get_test_pairs()
    num_similar = sum(1 for p in test_pairs if p.category == "similar")
    num_different = sum(1 for p in test_pairs if p.category == "different")
    num_identical = sum(1 for p in test_pairs if p.category == "identical")
    
    print(f"\nüìã Test pairs: {len(test_pairs)} total")
    print(f"   - Similar (should match): {num_similar}")
    print(f"   - Different (should NOT match): {num_different}")
    print(f"   - Identical (baseline): {num_identical}")
    
    # =========================================================================
    # Evaluate OpenAI + Cosine
    # =========================================================================
    print("\n" + "=" * 80)
    print("METHOD 1: OpenAI Embeddings + Cosine Similarity (CURRENT)")
    print("=" * 80)
    
    cosine_similar, cosine_different, cosine_identical, cosine_time = evaluate_openai_cosine(
        test_pairs, embedder
    )
    
    print(f"\n‚è±Ô∏è  Time: {cosine_time:.1f}ms ({len(test_pairs)} pairs)")
    print(f"   Per pair: {cosine_time/len(test_pairs):.1f}ms")
    
    print(f"\nüìä Scores by category:")
    print(f"   Similar pairs:")
    for pair, score in zip([p for p in test_pairs if p.category == "similar"], cosine_similar):
        print(f"      {score:.4f} | {pair.description}")
    
    print(f"\n   Different topic pairs:")
    for pair, score in zip([p for p in test_pairs if p.category == "different"], cosine_different):
        print(f"      {score:.4f} | {pair.description}")
    
    print(f"\n   Identical pairs:")
    for pair, score in zip([p for p in test_pairs if p.category == "identical"], cosine_identical):
        print(f"      {score:.4f} | {pair.description}")
    
    analyze_threshold(cosine_similar, cosine_different, "Cosine")
    
    # =========================================================================
    # Evaluate BGE Cross-Encoder
    # =========================================================================
    print("\n" + "=" * 80)
    print("METHOD 2: BGE Cross-Encoder (PROPOSED)")
    print("=" * 80)
    
    bge_similar, bge_different, bge_identical, bge_time = evaluate_bge_crossencoder(
        test_pairs, reranker
    )
    
    print(f"\n‚è±Ô∏è  Time: {bge_time:.1f}ms ({len(test_pairs)} pairs)")
    print(f"   Per pair: {bge_time/len(test_pairs):.1f}ms")
    
    print(f"\nüìä Scores by category:")
    print(f"   Similar pairs:")
    for pair, score in zip([p for p in test_pairs if p.category == "similar"], bge_similar):
        print(f"      {score:.4f} | {pair.description}")
    
    print(f"\n   Different topic pairs:")
    for pair, score in zip([p for p in test_pairs if p.category == "different"], bge_different):
        print(f"      {score:.4f} | {pair.description}")
    
    print(f"\n   Identical pairs:")
    for pair, score in zip([p for p in test_pairs if p.category == "identical"], bge_identical):
        print(f"      {score:.4f} | {pair.description}")
    
    analyze_threshold(bge_similar, bge_different, "BGE")
    
    # =========================================================================
    # Comparison Summary
    # =========================================================================
    print("\n" + "=" * 80)
    print("COMPARISON SUMMARY")
    print("=" * 80)
    
    # Separation quality
    cosine_separation = min(cosine_similar) - max(cosine_different)
    bge_separation = min(bge_similar) - max(bge_different)
    
    print(f"\nüìä Score Separation (min_similar - max_different):")
    print(f"   Cosine:  {cosine_separation:.4f} {'‚úÖ' if cosine_separation > 0 else '‚ùå OVERLAP'}")
    print(f"   BGE:     {bge_separation:.4f} {'‚úÖ' if bge_separation > 0 else '‚ùå OVERLAP'}")
    
    print(f"\n‚è±Ô∏è  Speed Comparison:")
    print(f"   Cosine:  {cosine_time:.1f}ms total, {cosine_time/len(test_pairs):.1f}ms/pair")
    print(f"   BGE:     {bge_time:.1f}ms total, {bge_time/len(test_pairs):.1f}ms/pair")
    print(f"   Winner:  {'Cosine' if cosine_time < bge_time else 'BGE'} ({abs(cosine_time - bge_time):.1f}ms faster)")
    
    print(f"\nüéØ Accuracy Comparison:")
    
    # For Cosine - try multiple thresholds
    print(f"   Cosine thresholds:")
    for t in [0.95, 0.90, 0.85, 0.80, 0.75]:
        similar_hit = sum(1 for s in cosine_similar if s >= t)
        false_pos = sum(1 for s in cosine_different if s >= t)
        print(f"      {t:.2f}: {similar_hit}/{len(cosine_similar)} similar, {false_pos}/{len(cosine_different)} false positives")
    
    # For BGE - scores are not normalized, need to find threshold
    print(f"\n   BGE thresholds:")
    bge_min = min(min(bge_similar), min(bge_different))
    bge_max = max(max(bge_similar), max(bge_different))
    for t in np.linspace(bge_min, bge_max, 6):
        similar_hit = sum(1 for s in bge_similar if s >= t)
        false_pos = sum(1 for s in bge_different if s >= t)
        print(f"      {t:.2f}: {similar_hit}/{len(bge_similar)} similar, {false_pos}/{len(bge_different)} false positives")
    
    # =========================================================================
    # Recommendation
    # =========================================================================
    print("\n" + "=" * 80)
    print("RECOMMENDATIONS")
    print("=" * 80)
    
    if bge_separation > cosine_separation:
        print(f"""
    ‚úÖ BGE Cross-Encoder BETTER for semantic cache:
       - Score separation: {bge_separation:.4f} (Cosine: {cosine_separation:.4f})
       - Can distinguish similar vs different topics more accurately
       
    ‚ö†Ô∏è  Trade-offs:
       - Speed: {'Slower' if bge_time > cosine_time else 'Faster'} by {abs(bge_time - cosine_time):.1f}ms
       - BGE runs locally (no API cost)
       - OpenAI requires API call
       
    üí° Suggested Implementation:
       1. For SMALL cache (< 100 entries): Use BGE directly
       2. For LARGE cache: Hybrid approach
          - First: Cosine filter to top 10-20 candidates
          - Then: BGE rerank for final decision
""")
    else:
        print(f"""
    ‚ÑπÔ∏è  Results inconclusive or Cosine performs similarly.
       
    Current issues:
       - Cosine separation: {cosine_separation:.4f}
       - BGE separation: {bge_separation:.4f}
       
    Consider:
       - Adding more diverse test cases
       - Fine-tuning BGE on bidding domain
       - Using hybrid approach
""")


if __name__ == "__main__":
    main()
