# Káº¾ HOáº CH MIGRATION Cá»¤ THá»‚ - KHÃ”NG Cáº¦N REPROCESS CHUNKS

**NgÃ y:** 2025-11-19  
**PhÃ¢n tÃ­ch:** Chunks hiá»‡n táº¡i ÄÃƒ CÃ“ Ä‘á»§ thÃ´ng tin, chá»‰ cáº§n UPDATE metadata

---

## ğŸ“Š PHÃ‚N TÃCH HIá»†N TRáº NG

### âœ… Chunk Files (data/processed/chunks/)
```json
{
  "document_id": "law_untitled",  // âŒ Generic
  "chunk_id": "law_untitled_khoan_0000",
  "document_type": "law",         // âœ… OK
  "content": "...",               // âœ… OK
  "hierarchy": [...],             // âœ… OK
  "section_title": "...",         // âœ… OK
  "chunk_index": 0,               // âœ… OK
  "total_chunks": 255             // âœ… OK
}
```

**Káº¿t luáº­n:** Chunks cÃ³ Ä‘áº§y Ä‘á»§ content, hierarchy, metadata â†’ **KHÃ”NG Cáº¦N reprocess**

### âŒ Database (langchain_pg_embedding)
```
5 collections (merged):
- LAW-Law/2025#cd5116 (1154 chunks) - Gá»™p 4 law files
- ND-Decree/2025#95b863 (595 chunks)
- TT-Circular/2025#3be8b6 (123 chunks)
- FORM-Bidding/2025#bee720 (2831 chunks) - Gá»™p 46 files
- DOC-Document/2025#787999 (5 chunks)
```

**Váº¥n Ä‘á»:**
- Generic document_ids (5 collections thay vÃ¬ 70 documents)
- KhÃ´ng cÃ³ source_file tracking
- KhÃ´ng thá»ƒ toggle tá»«ng document riÃªng láº»

---

## ğŸ¯ Káº¾ HOáº CH: UPDATE METADATA ONLY (2 ngÃ y)

### âœ… APPROACH: KhÃ´ng reprocess, chá»‰ update metadata

**LÃ½ do:**
1. Content Ä‘Ã£ Ä‘Ãºng, hierarchy Ä‘Ã£ tá»‘t
2. Embeddings Ä‘Ã£ cÃ³ trong database
3. Chá»‰ cáº§n map chunks â†’ documents má»›i
4. Tiáº¿t kiá»‡m 80% thá»i gian

---

## ğŸ“… CHI TIáº¾T Káº¾ HOáº CH

### **NGÃ€Y 1: Setup Documents Table + Mapping (4 giá»)**

#### SÃ¡ng (2 giá»)

**1. Create documents table** â±ï¸ 30 phÃºt
```bash
# Run migration scripts
python scripts/migration/002_extract_simple_metadata.py
python scripts/migration/003_populate_documents_table.py
```

**Expected:**
- âœ… Documents table vá»›i 70 records
- âœ… Mapping: 7 categories
- âœ… Document_ids generated

**2. Build mapping: chunk files â†’ document_id** â±ï¸ 1.5 giá»

```python
# scripts/migration/004_build_chunk_to_document_mapping.py

def build_mapping():
    """
    Map existing chunks to new document_ids
    Using metadata files as bridge
    """
    
    mapping = {}  # {old_chunk_path: new_document_id}
    
    # Load documents table
    documents = get_all_documents()
    
    for doc in documents:
        # Find corresponding metadata file
        metadata_file = find_metadata_file(doc.source_file)
        
        if metadata_file:
            # Get chunk file path from metadata
            chunk_file = metadata_file["output_file"]
            
            # Map chunk_file â†’ new document_id
            mapping[chunk_file] = doc.document_id
    
    return mapping

# Example output:
# {
#   "data/processed/chunks/Luat_so_90_2025.jsonl": "LUA-90-2025-QH15",
#   "data/processed/chunks/01A_Mau_HSYC.jsonl": "FORM-HSYC-XAYLAP-01A",
#   ...
# }
```

#### Chiá»u (2 giá»)

**3. Update database chunks with new document_ids** â±ï¸ 2 giá»

```python
# scripts/migration/005_update_chunk_document_ids.py

async def update_chunk_document_ids():
    """
    Update existing chunks in database vá»›i document_id má»›i
    KHÃ”NG reprocess, chá»‰ update metadata
    """
    
    # Load mapping
    mapping = load_chunk_to_document_mapping()
    
    # Load metadata files
    metadata_files = load_all_metadata()
    
    for meta_file in metadata_files:
        chunk_file = meta_file["output_file"]
        new_doc_id = mapping.get(chunk_file)
        
        if not new_doc_id:
            continue
        
        # Read chunks from file
        chunks = read_chunks_from_file(chunk_file)
        
        for chunk in chunks:
            old_chunk_id = chunk["chunk_id"]
            
            # Generate new chunk_id
            # "law_untitled_khoan_0000" â†’ "LUA-90-2025-QH15_khoan_0000"
            chunk_suffix = old_chunk_id.split("_", 2)[-1]  # "khoan_0000"
            new_chunk_id = f"{new_doc_id}_{chunk_suffix}"
            
            # Update database
            await db.execute(text("""
                UPDATE langchain_pg_embedding
                SET cmetadata = jsonb_set(
                    jsonb_set(
                        jsonb_set(
                            cmetadata,
                            '{document_id}',
                            to_jsonb(:new_doc_id::text)
                        ),
                        '{chunk_id}',
                        to_jsonb(:new_chunk_id::text)
                    ),
                    '{source_file}',
                    to_jsonb(:source_file::text)
                )
                WHERE cmetadata->>'chunk_id' = :old_chunk_id
            """), {
                "new_doc_id": new_doc_id,
                "new_chunk_id": new_chunk_id,
                "source_file": meta_file["source_file"],
                "old_chunk_id": old_chunk_id
            })
        
        print(f"âœ… Updated {len(chunks)} chunks for {new_doc_id}")
```

**Key Points:**
- âœ… Giá»¯ nguyÃªn embeddings (khÃ´ng recompute)
- âœ… Giá»¯ nguyÃªn content, hierarchy
- âœ… Chá»‰ update: document_id, chunk_id, source_file
- âœ… Fast: ~10-15 phÃºt cho 4700 chunks

---

### **NGÃ€Y 2: Verification + API Update (4 giá»)**

#### SÃ¡ng (2 giá»)

**4. Verify migration** â±ï¸ 1 giá»

```sql
-- Check document count
SELECT COUNT(DISTINCT cmetadata->>'document_id') 
FROM langchain_pg_embedding;
-- Expected: 70 (not 5!)

-- Check sample document_ids
SELECT DISTINCT cmetadata->>'document_id', COUNT(*) 
FROM langchain_pg_embedding 
GROUP BY cmetadata->>'document_id'
ORDER BY cmetadata->>'document_id'
LIMIT 10;
-- Expected: LUA-90-2025-QH15, LUA-57-2024-QH15, etc.

-- Verify source_file populated
SELECT COUNT(*) 
FROM langchain_pg_embedding 
WHERE cmetadata->>'source_file' IS NOT NULL;
-- Expected: 4708 (100%)

-- Check chunk_id format
SELECT cmetadata->>'chunk_id' 
FROM langchain_pg_embedding 
LIMIT 5;
-- Expected: LUA-90-2025-QH15_khoan_0000, etc.
```

**5. Update documents.total_chunks** â±ï¸ 30 phÃºt

```python
# Update chunk counts in documents table
async def update_chunk_counts():
    for doc in documents:
        count = await db.execute(text("""
            SELECT COUNT(*) 
            FROM langchain_pg_embedding
            WHERE cmetadata->>'document_id' = :doc_id
        """), {"doc_id": doc.document_id})
        
        await db.execute(text("""
            UPDATE documents
            SET total_chunks = :count
            WHERE document_id = :doc_id
        """), {"count": count, "doc_id": doc.document_id})
```

**6. Test retrieval** â±ï¸ 30 phÃºt

```python
# Test that retrieval still works
results = retriever.retrieve(
    query="luáº­t Ä‘áº¥u tháº§u 2025",
    filters={"document_id": "LUA-90-2025-QH15"}
)

# Should return chunks from Luáº­t 90/2025 only
```

#### Chiá»u (2 giá»)

**7. Update API endpoints** â±ï¸ 1 giá»

```python
# src/api/routers/documents_management.py

@router.get("/documents/catalog")
async def get_catalog(
    category: Optional[str] = None,
    status: str = "active",
    limit: int = 100,
    db: AsyncSession = Depends(get_db)
):
    """Get 70 documents (not 5!)"""
    query = select(Document).where(Document.status == status)
    
    if category:
        query = query.where(Document.category == category)
    
    result = await db.execute(query)
    docs = result.scalars().all()
    
    return {
        "total": len(docs),
        "documents": [
            {
                "document_id": doc.document_id,
                "document_name": doc.document_name,
                "category": doc.category,
                "total_chunks": doc.total_chunks,
                "status": doc.status
            }
            for doc in docs
        ]
    }

@router.patch("/documents/{document_id}/status")
async def toggle_status(
    document_id: str,
    status: str,
    db: AsyncSession = Depends(get_db)
):
    """Toggle specific document"""
    
    # Update documents table
    await db.execute(
        update(Document)
        .where(Document.document_id == document_id)
        .values(status=status)
    )
    
    # Update chunks (for filtering during retrieval)
    await db.execute(text("""
        UPDATE langchain_pg_embedding
        SET cmetadata = jsonb_set(
            cmetadata,
            '{status}',
            to_jsonb(:status::text)
        )
        WHERE cmetadata->>'document_id' = :doc_id
    """), {"status": status, "doc_id": document_id})
    
    await db.commit()
    return {"message": f"Updated {document_id} to {status}"}
```

**8. Update retriever to respect status** â±ï¸ 30 phÃºt

```python
# src/retrieval/retrievers/base.py

def retrieve(self, query: str, filters: dict = None):
    # Add status filter
    if filters is None:
        filters = {}
    
    if "status" not in filters:
        filters["status"] = "active"
    
    # Retrieval vá»›i filter
    results = self.vector_store.similarity_search(
        query,
        filter={"status": filters["status"]}
    )
    
    return results
```

**9. Integration testing** â±ï¸ 30 phÃºt

```bash
# Test catalog
curl http://localhost:8000/api/documents/catalog
# Expected: 70 documents

# Test toggle
curl -X PATCH http://localhost:8000/api/documents/LUA-90-2025-QH15/status \
  -d '{"status": "inactive"}'

# Test retrieval excludes inactive
curl -X POST http://localhost:8000/api/ask \
  -d '{"query": "luáº­t Ä‘áº¥u tháº§u 2025"}'
# Should NOT return chunks from LUA-90-2025-QH15
```

---

## ğŸ“Š SO SÃNH: REPROCESS vs UPDATE ONLY

| Aspect | Reprocess Approach | Update Metadata Only |
|--------|-------------------|---------------------|
| **Thá»i gian** | 3-4 ngÃ y | **2 ngÃ y** âœ… |
| **Embeddings** | Recompute (~2 giá») | Giá»¯ nguyÃªn âœ… |
| **Content** | Reprocess files | Giá»¯ nguyÃªn âœ… |
| **Risk** | High (cÃ³ thá»ƒ break) | Low (chá»‰ update IDs) |
| **Testing** | ToÃ n bá»™ pipeline | Chá»‰ API endpoints |
| **Rollback** | KhÃ³ (cáº§n backup) | Dá»… (chá»‰ revert IDs) |

**Recommendation:** **UPDATE METADATA ONLY** âœ…

---

## âœ… Káº¾T QUáº¢ MONG Äá»¢I

### TrÆ°á»›c Migration
```
Database: 5 "documents" (collections)
â”œâ”€â”€ LAW-Law/2025#cd5116 (1154 chunks - 4 files gá»™p)
â”œâ”€â”€ FORM-Bidding/2025#bee720 (2831 chunks - 46 files gá»™p)
â””â”€â”€ ... (3 more collections)

Catalog API: Returns 5 items
Toggle: KHÃ”NG THá»‚ toggle riÃªng láº»
source_file: NULL cho táº¥t cáº£ chunks
```

### Sau Migration (2 ngÃ y)
```
Database: 70 real documents
â”œâ”€â”€ LUA-90-2025-QH15 (255 chunks)
â”œâ”€â”€ LUA-57-2024-QH15 (280 chunks)
â”œâ”€â”€ FORM-HSYC-XAYLAP-01A (42 chunks)
â””â”€â”€ ... (67 more documents)

Catalog API: Returns 70 documents âœ…
Toggle: Toggle báº¥t ká»³ document nÃ o âœ…
source_file: Populated cho táº¥t cáº£ chunks âœ…
```

---

## ğŸ¯ EXECUTION CHECKLIST

### Day 1 Morning
- [ ] Run `002_extract_simple_metadata.py`
- [ ] Run `003_populate_documents_table.py`
- [ ] Verify: 70 documents in table

### Day 1 Afternoon
- [ ] Create `004_build_chunk_to_document_mapping.py`
- [ ] Run mapping script
- [ ] Verify: mapping.json with 70 entries

### Day 1 Evening
- [ ] Create `005_update_chunk_document_ids.py`
- [ ] Run update script
- [ ] Monitor progress (4708 chunks)

### Day 2 Morning
- [ ] Verify document_ids in database (should be 70)
- [ ] Verify source_file populated
- [ ] Update documents.total_chunks
- [ ] Test retrieval with new IDs

### Day 2 Afternoon
- [ ] Update catalog API endpoint
- [ ] Update toggle API endpoint
- [ ] Update retriever filter logic
- [ ] Integration testing
- [ ] Documentation

---

## ğŸš€ Báº®T Äáº¦U NGAY

```bash
# Step 1: Extract metadata
cd /home/sakana/Code/RAG-bidding
python scripts/migration/002_extract_simple_metadata.py

# Expected output:
# âœ… Found 70 documents
# ğŸ“ Há»“ sÆ¡ má»i tháº§u: 46 documents
# ğŸ“ Máº«u bÃ¡o cÃ¡o: 10 documents
# ...
# ğŸ’¾ Saved to: data/processed/documents_metadata.json
```

**XÃ¡c nháº­n Ä‘á»ƒ tiáº¿p tá»¥c?**
