# Simple Documents Schema Migration

**M·ª•c ƒë√≠ch:** T·∫°o b·∫£ng `documents` ƒë∆°n gi·∫£n ƒë·ªÉ qu·∫£n l√Ω 70 documents v·ªÅ ƒë·∫•u th·∫ßu

---

## üìä Database Schema

### Documents Table (9 fields - ƒë∆°n gi·∫£n)

```sql
documents (
    id                UUID PRIMARY KEY
    document_id       VARCHAR(255) UNIQUE    -- "LUA-90-2025-QH15"
    document_name     TEXT                    -- "Lu·∫≠t s·ªë 90/2025 v·ªÅ ƒê·∫•u th·∫ßu"
    category          VARCHAR(100)            -- 7 categories
    document_type     VARCHAR(50)             -- law, decree, circular, etc.
    source_file       TEXT                    -- "data/raw/Luat chinh/..."
    file_name         TEXT                    -- "Luat so 90 2025.docx"
    total_chunks      INTEGER                 -- S·ªë chunks sau khi process
    status            VARCHAR(50)             -- active/inactive/archived
    created_at        TIMESTAMP
    updated_at        TIMESTAMP
)
```

**Indexes:**
- `idx_documents_category` - Filter by category
- `idx_documents_type` - Filter by document type
- `idx_documents_status` - Toggle active/inactive
- `idx_documents_source` - Lookup by source file

---

## üóÇÔ∏è 7 Categories Mapping

| Category | Folder | Document Type | Files | Example document_id |
|----------|--------|---------------|-------|-------------------|
| Lu·∫≠t ch√≠nh | `Luat chinh/` | `law` | 4 | `LUA-90-2025-QH15` |
| Ngh·ªã ƒë·ªãnh | `Nghi dinh/` | `decree` | 1 | `ND-214-2025-CP` |
| Th√¥ng t∆∞ | `Thong tu/` | `circular` | 2 | `TT-00-QUYET-DINH` |
| Quy·∫øt ƒë·ªãnh | `Quyet dinh/` | `decision` | 1 | `QD-1667-BYT` |
| H·ªì s∆° m·ªùi th·∫ßu | `Ho so moi thau/` | `bidding_form` | 46 | `FORM-HSYC-XAYLAP-01A` |
| M·∫´u b√°o c√°o | `Mau bao cao/` | `report_template` | 10 | `TEMPLATE-BC-01` |
| C√¢u h·ªèi thi | `Cau hoi thi/` | `exam_question` | 6 | `EXAM-CAU-HOI-01` |

**Total:** 70 documents

---

## üöÄ Migration Steps (2 gi·ªù)

### Step 1: Extract Metadata (30 ph√∫t)

```bash
cd /home/sakana/Code/RAG-bidding

# Run extraction script
python scripts/migration/002_extract_simple_metadata.py
```

**Output:** `data/processed/documents_metadata.json`

Expected structure:
```json
[
  {
    "document_id": "LUA-90-2025-QH15",
    "document_name": "Luat so 90 2025-qh15",
    "category": "Lu·∫≠t ch√≠nh",
    "document_type": "law",
    "source_file": "data/raw/Luat chinh/Luat so 90 2025-qh15.docx",
    "file_name": "Luat so 90 2025-qh15.docx",
    "total_chunks": 0,
    "status": "active"
  },
  ...
]
```

### Step 2: Create Table & Populate (1 gi·ªù)

```bash
# Run migration script
python scripts/migration/003_populate_documents_table.py
```

**What it does:**
1. Creates `documents` table (if not exists)
2. Inserts 70 documents from metadata JSON
3. Verifies insertion with count queries

**Expected output:**
```
‚úÖ Inserted: 70 documents
üìä Total documents: 70

üìÅ Documents by category:
   H·ªì s∆° m·ªùi th·∫ßu: 46 documents
   M·∫´u b√°o c√°o: 10 documents
   C√¢u h·ªèi thi: 6 documents
   Lu·∫≠t ch√≠nh: 4 documents
   Th√¥ng t∆∞: 2 documents
   Ngh·ªã ƒë·ªãnh: 1 documents
   Quy·∫øt ƒë·ªãnh: 1 documents
```

### Step 3: Verify (30 ph√∫t)

```bash
# Connect to database
psql -U sakana -d rag_bidding_v2

# Check documents table
SELECT COUNT(*) FROM documents;
-- Expected: 70

# Check by category
SELECT category, COUNT(*) 
FROM documents 
GROUP BY category 
ORDER BY COUNT(*) DESC;

# Sample documents
SELECT document_id, document_name, category 
FROM documents 
LIMIT 10;
```

---

## üìù Usage Examples

### Query documents by category

```sql
-- Get all laws
SELECT * FROM documents 
WHERE category = 'Lu·∫≠t ch√≠nh' 
AND status = 'active';

-- Get all bidding forms
SELECT * FROM documents 
WHERE category = 'H·ªì s∆° m·ªùi th·∫ßu'
ORDER BY file_name;
```

### Toggle document status

```sql
-- Deactivate a document
UPDATE documents 
SET status = 'inactive', updated_at = NOW()
WHERE document_id = 'LUA-90-2025-QH15';

-- Reactivate
UPDATE documents 
SET status = 'active', updated_at = NOW()
WHERE document_id = 'LUA-90-2025-QH15';
```

### Update chunk count (after processing)

```sql
UPDATE documents 
SET total_chunks = 255
WHERE document_id = 'LUA-90-2025-QH15';
```

---

## üîÑ Next Steps (After migration)

### 1. Update Preprocessing Pipeline

Modify `src/preprocessing/document_processor.py` to:
- Load document info from `documents` table
- Use `document_id` from table (not generated)
- Update `total_chunks` after processing

### 2. Update Chunks Metadata

When inserting chunks to `langchain_pg_embedding`:
```json
{
  "document_id": "LUA-90-2025-QH15",  // From documents table
  "source_file": "data/raw/Luat chinh/...",
  "document_name": "Lu·∫≠t s·ªë 90/2025",
  "category": "Lu·∫≠t ch√≠nh",
  "status": "active"
}
```

### 3. Update API Endpoints

```python
# Get catalog from documents table
@router.get("/documents/catalog")
async def get_catalog(
    category: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    query = select(Document).where(Document.status == "active")
    if category:
        query = query.where(Document.category == category)
    
    result = await db.execute(query)
    docs = result.scalars().all()
    
    return {"documents": [doc.to_dict() for doc in docs]}

# Toggle document
@router.patch("/documents/{document_id}/status")
async def toggle_status(
    document_id: str,
    status: str,
    db: AsyncSession = Depends(get_db)
):
    await db.execute(
        update(Document)
        .where(Document.document_id == document_id)
        .values(status=status)
    )
    await db.commit()
    return {"message": f"Updated {document_id} to {status}"}
```

---

## ‚úÖ Benefits

1. **Simple:** Only 9 fields, easy to understand
2. **Fast:** Can migrate in 2 hours
3. **Flexible:** Can add more fields later if needed
4. **Category-based:** Matches your 7 folder structure
5. **Toggle-ready:** Status field for activate/deactivate
6. **Track chunks:** total_chunks for monitoring

---

## üéØ Timeline

| Task | Duration | Status |
|------|----------|--------|
| Extract metadata | 30 min | Ready to run |
| Create table & populate | 1 hour | Ready to run |
| Verify & test | 30 min | Ready to run |
| **Total** | **2 hours** | **Can start now** |

---

**Next:** Run `002_extract_simple_metadata.py` ƒë·ªÉ b·∫Øt ƒë·∫ßu?
