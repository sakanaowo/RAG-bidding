"""
Test script for Chunking Strategies
Tests HierarchicalChunker and SemanticChunker with real data
"""

import sys
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.preprocessing.loaders import DocxLoader, RawDocxContent
from src.preprocessing.chunking import HierarchicalChunker, SemanticChunker
from src.preprocessing.schema import DocType


def test_hierarchical_chunker():
    """Test 1: HierarchicalChunker with Law document"""
    print("\n" + "=" * 70)
    print("TEST 1: HierarchicalChunker")
    print("=" * 70)

    # Load test document
    loader = DocxLoader()
    test_file = (
        project_root / "data/raw/Luat chinh/Há»¢P NHáº¤T 126 2025 vá» Luáº­t Ä‘áº¥u tháº§u.docx"
    )

    print(f"\nğŸ“„ Loading: {test_file.name}")
    raw_content = loader.load(str(test_file))

    print(f"âœ… Loaded: {len(raw_content.structure)} structure elements")

    # Create chunker
    chunker = HierarchicalChunker(
        max_chunk_size=2000,
        min_chunk_size=300,
        overlap_size=150,
        preserve_parent_context=True,
        split_large_dieu=True,
    )

    # Chunk
    print(f"\nğŸ”„ Chunking...")
    base_metadata = {
        "doc_type": DocType.LAW,
        "doc_id": "126/VBHN-VPQH/2025",
    }

    chunks = chunker.chunk(
        raw_content=raw_content,
        doc_id="126/VBHN-VPQH/2025",
        base_metadata=base_metadata,
    )

    print(f"âœ… Created {len(chunks)} chunks")

    # Analyze chunks
    print(f"\nğŸ“Š Chunk Statistics:")
    total_chars = sum(chunk.char_count for chunk in chunks)
    avg_chars = total_chars / len(chunks) if chunks else 0
    min_chars = min(chunk.char_count for chunk in chunks) if chunks else 0
    max_chars = max(chunk.char_count for chunk in chunks) if chunks else 0

    print(f"   Total characters: {total_chars:,}")
    print(f"   Average chunk size: {avg_chars:.0f} chars")
    print(f"   Min chunk size: {min_chars:,} chars")
    print(f"   Max chunk size: {max_chars:,} chars")

    # Show first 3 chunks
    print(f"\nğŸ“ First 3 Chunks:")
    for i, chunk in enumerate(chunks[:3]):
        print(f"\n   Chunk {i+1}:")
        print(f"   - ID: {chunk.chunk_id}")
        print(f"   - Size: {chunk.char_count} chars")
        print(f"   - Level: {chunk.structure.level}")
        print(f"   - Hierarchy: {' > '.join(chunk.structure.hierarchy_path)}")
        print(f"   - Section: {chunk.structure.section_title}")
        preview = chunk.content[:100].replace("\n", " ")
        print(f"   - Preview: {preview}...")

    # Verify all chunks have required fields
    print(f"\nâœ“ Verification:")
    all_valid = True
    for i, chunk in enumerate(chunks):
        if not chunk.chunk_id:
            print(f"   âŒ Chunk {i} missing chunk_id")
            all_valid = False
        if not chunk.content:
            print(f"   âŒ Chunk {i} missing content")
            all_valid = False
        if chunk.char_count <= 0:
            print(f"   âŒ Chunk {i} invalid char_count: {chunk.char_count}")
            all_valid = False

    if all_valid:
        print(f"   âœ… All chunks valid")

    return all_valid


def test_semantic_chunker():
    """Test 2: SemanticChunker with plain text"""
    print("\n" + "=" * 70)
    print("TEST 2: SemanticChunker")
    print("=" * 70)

    # Create sample bidding document text
    sample_text = """
PHáº¦N I: THÃ”NG TIN CHUNG

TÃªn gÃ³i tháº§u: Mua sáº¯m trang thiáº¿t bá»‹ vÄƒn phÃ²ng
MÃ£ gÃ³i tháº§u: 2024-ÄT-001
Nguá»“n vá»‘n: NgÃ¢n sÃ¡ch nhÃ  nÆ°á»›c

A. YÃŠU Cáº¦U Vá»€ Sáº¢N PHáº¨M

CÃ¡c yÃªu cáº§u cá»¥ thá»ƒ vá» sáº£n pháº©m nhÆ° sau:

1. MÃ¡y tÃ­nh vÄƒn phÃ²ng

MÃ¡y tÃ­nh cáº§n Ä‘Ã¡p á»©ng cÃ¡c tiÃªu chuáº©n ká»¹ thuáº­t tá»‘i thiá»ƒu:
- CPU: Intel Core i5 tháº¿ há»‡ 11 trá»Ÿ lÃªn
- RAM: 16GB DDR4
- á»” cá»©ng: SSD 512GB
- MÃ n hÃ¬nh: 24 inch Full HD

2. MÃ¡y in laser

MÃ¡y in cáº§n cÃ³ cÃ¡c tÃ­nh nÄƒng:
- In Ä‘en tráº¯ng A4
- Tá»‘c Ä‘á»™ in: 30 trang/phÃºt
- Káº¿t ná»‘i: USB, WiFi, LAN
- Khay giáº¥y: 250 tá»

B. YÃŠU Cáº¦U Vá»€ Báº¢O HÃ€NH

NhÃ  tháº§u pháº£i Ä‘áº£m báº£o báº£o hÃ nh tá»‘i thiá»ƒu 36 thÃ¡ng cho táº¥t cáº£ thiáº¿t bá»‹.

PHáº¦N II: ÄIá»€U KIá»†N THAM Dá»° THáº¦U

1. Äiá»u kiá»‡n vá» nÄƒng lá»±c

NhÃ  tháº§u cáº§n cÃ³:
- Giáº¥y phÃ©p kinh doanh phÃ¹ há»£p
- Kinh nghiá»‡m tá»‘i thiá»ƒu 3 nÄƒm
- ÄÃ£ thá»±c hiá»‡n Ã­t nháº¥t 2 há»£p Ä‘á»“ng tÆ°Æ¡ng tá»±

2. Äiá»u kiá»‡n vá» tÃ i chÃ­nh

BÃ¡o cÃ¡o tÃ i chÃ­nh 2 nÄƒm gáº§n nháº¥t Ä‘Ã£ Ä‘Æ°á»£c kiá»ƒm toÃ¡n.
    """.strip()

    print(f"\nğŸ“„ Sample text: {len(sample_text)} characters")

    # Create chunker
    chunker = SemanticChunker(
        chunk_size=500,
        min_chunk_size=200,
        overlap_size=100,
        detect_sections=True,
    )

    # Chunk
    print(f"\nğŸ”„ Chunking...")
    base_metadata = {
        "doc_type": DocType.BIDDING,
        "doc_id": "2024-ÄT-001",
    }

    chunks = chunker.chunk(
        text=sample_text,
        doc_id="2024-DT-001",
        base_metadata=base_metadata,
    )

    print(f"âœ… Created {len(chunks)} chunks")

    # Analyze chunks
    print(f"\nğŸ“Š Chunk Statistics:")
    total_chars = sum(chunk.char_count for chunk in chunks)
    avg_chars = total_chars / len(chunks) if chunks else 0

    print(f"   Total characters: {total_chars:,}")
    print(f"   Average chunk size: {avg_chars:.0f} chars")

    # Show all chunks
    print(f"\nğŸ“ All Chunks:")
    for i, chunk in enumerate(chunks):
        print(f"\n   Chunk {i+1}:")
        print(f"   - ID: {chunk.chunk_id}")
        print(f"   - Size: {chunk.char_count} chars")
        print(f"   - Section: {chunk.structure.section_title or 'N/A'}")
        print(f"   - Method: {chunk.chunking.chunk_method}")
        preview = chunk.content[:80].replace("\n", " ")
        print(f"   - Preview: {preview}...")

    # Verify overlap
    print(f"\nâœ“ Overlap Verification:")
    has_overlap = False
    for i in range(len(chunks) - 1):
        chunk1_end = chunks[i].content[-50:]
        chunk2_start = chunks[i + 1].content[:50]

        # Check if there's any common text
        for j in range(len(chunk1_end)):
            if chunk1_end[j:] in chunk2_start:
                has_overlap = True
                print(f"   âœ… Overlap found between chunk {i+1} and {i+2}")
                break

    if not has_overlap and len(chunks) > 1:
        print(f"   âš ï¸  No overlap detected (might be OK for section boundaries)")

    return True


def test_chunker_comparison():
    """Test 3: Compare both chunkers on same document"""
    print("\n" + "=" * 70)
    print("TEST 3: Chunker Comparison")
    print("=" * 70)

    # Load test document
    loader = DocxLoader()
    test_file = (
        project_root / "data/raw/Luat chinh/Há»¢P NHáº¤T 126 2025 vá» Luáº­t Ä‘áº¥u tháº§u.docx"
    )

    raw_content = loader.load(str(test_file))

    # Test HierarchicalChunker
    print(f"\nğŸ”„ Testing HierarchicalChunker...")
    hierarchical_chunker = HierarchicalChunker(max_chunk_size=1500)
    hierarchical_chunks = hierarchical_chunker.chunk(
        raw_content=raw_content,
        doc_id="126/VBHN-VPQH/2025",
        base_metadata={},
    )

    # Test SemanticChunker
    print(f"ğŸ”„ Testing SemanticChunker...")
    semantic_chunker = SemanticChunker(chunk_size=1500)
    semantic_chunks = semantic_chunker.chunk(
        text=raw_content.text,
        doc_id="126/VBHN-VPQH/2025",
        base_metadata={},
    )

    # Compare
    print(f"\nğŸ“Š Comparison:")
    print(f"   HierarchicalChunker: {len(hierarchical_chunks)} chunks")
    print(f"   SemanticChunker: {len(semantic_chunks)} chunks")

    h_avg = sum(c.char_count for c in hierarchical_chunks) / len(hierarchical_chunks)
    s_avg = sum(c.char_count for c in semantic_chunks) / len(semantic_chunks)

    print(f"\n   Average chunk size:")
    print(f"   - Hierarchical: {h_avg:.0f} chars")
    print(f"   - Semantic: {s_avg:.0f} chars")

    print(f"\n   âœ… Hierarchical is better for legal docs (preserves structure)")
    print(f"   âœ… Semantic is better for free-form docs (flexible boundaries)")

    return True


def main():
    print("\n" + "=" * 80)
    print("ğŸ§ª CHUNKING STRATEGIES TEST SUITE")
    print("=" * 80)

    results = []

    # Run all tests
    results.append(("HierarchicalChunker", test_hierarchical_chunker()))
    results.append(("SemanticChunker", test_semantic_chunker()))
    results.append(("Chunker Comparison", test_chunker_comparison()))

    # Summary
    print("\n" + "=" * 80)
    print("ğŸ“Š TEST SUMMARY")
    print("=" * 80)

    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} - {test_name}")

    total = len(results)
    passed = sum(1 for _, r in results if r)

    print(f"\n{'='*80}")
    print(f"Total: {passed}/{total} tests passed")
    print("=" * 80)

    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
