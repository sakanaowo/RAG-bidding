# OpenAI Reranker - Setup & Usage Guide

**Created**: 2025-11-13  
**Status**: âœ… Implemented, Ready for Testing

## ğŸ¯ Overview

OpenAI Reranker lÃ  tÃ¹y chá»n reranking thá»© 2 bÃªn cáº¡nh BGE Reranker, sá»­ dá»¥ng GPT-4o-mini API Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ liÃªn quan cá»§a documents.

### So sÃ¡nh BGE vs OpenAI Reranker

| Feature | BGE Reranker (Default) | OpenAI Reranker |
|---------|------------------------|-----------------|
| **Model** | BAAI/bge-reranker-v2-m3 | GPT-4o-mini |
| **Type** | Cross-encoder (local) | API-based (cloud) |
| **Memory** | 1.2GB GPU/RAM | 0 (no model loading) |
| **Latency** | 25-50ms (GPU) | 200-500ms (network) |
| **Cost** | Free | ~$0.01-0.05 per 1000 tokens |
| **Quality** | Excellent (multilingual) | Excellent (GPT understanding) |
| **Offline** | âœ… Yes | âŒ No (requires internet) |
| **Rate Limit** | None | OpenAI API limits |

### Khi nÃ o dÃ¹ng BGE vs OpenAI?

**Use BGE (Default)** khi:
- Production deployment vá»›i high throughput
- Cáº§n low latency (<50ms)
- KhÃ´ng muá»‘n phá»¥ thuá»™c external API
- CÃ³ GPU available (RTX 3060+)

**Use OpenAI** khi:
- Testing/prototyping
- Low query volume (<100/day)
- KhÃ´ng cÃ³ GPU máº¡nh
- Cáº§n GPT-level understanding cho complex queries

---

## ğŸ“¦ Implementation Files

### 1. Core Implementation

**File**: `src/retrieval/ranking/openai_reranker.py` (240 lines)

```python
class OpenAIReranker(BaseReranker):
    """
    OpenAI-based reranker using GPT-4o-mini.
    
    Features:
    - Scores documents 0-10 based on query relevance
    - Vietnamese language support
    - Automatic truncation to avoid token limits
    - Error handling for API failures
    """
    
    def __init__(
        self,
        model_name: str = "gpt-4o-mini",  # Default model
        api_key: Optional[str] = None,
        temperature: float = 0.0,  # Deterministic
        max_tokens: int = 10,  # Only need score
    )
```

**Key Methods**:
- `rerank(query, documents, top_k)`: Main reranking function
- `_score_document(query, doc)`: Score single document 0-1
- `rerank_batch(...)`: Batch processing (sequential)

### 2. Integration Points

**Retriever Factory** - `src/retrieval/retrievers/__init__.py`:

```python
def create_retriever(
    mode: str = "balanced",
    enable_reranking: bool = True,
    reranker_type: Literal["bge", "openai"] = "bge",  # ğŸ†•
    ...
):
    if enable_reranking and reranker is None:
        if reranker_type == "bge":
            reranker = get_singleton_reranker()  # Singleton
        elif reranker_type == "openai":
            reranker = OpenAIReranker()  # New instance (no state)
```

**QA Chain** - `src/generation/chains/qa_chain.py`:

```python
def answer(
    question: str,
    mode: str | None = None,
    use_enhancement: bool = True,
    reranker_type: str = "bge",  # ğŸ†•
) -> Dict:
    retriever = create_retriever(
        mode=selected_mode,
        enable_reranking=enable_reranking,
        reranker_type=reranker_type,  # Pass through
    )
```

**API Endpoint** - `src/api/main.py`:

```python
class AskIn(BaseModel):
    question: str
    mode: Literal["fast", "balanced", "quality", "adaptive"] = "balanced"
    reranker: Literal["bge", "openai"] = "bge"  # ğŸ†•

@app.post("/ask")
def ask(body: AskIn):
    result = answer(
        body.question,
        mode=body.mode,
        reranker_type=body.reranker,  # ğŸ†•
    )
```

---

## ğŸ”§ Setup Instructions

### Step 1: Get OpenAI API Key

1. Go to https://platform.openai.com/api-keys
2. Create new secret key
3. Copy the key (starts with `sk-...`)

### Step 2: Set Environment Variable

**Option A: Temporary (current terminal session)**

```bash
export OPENAI_API_KEY=sk-your-key-here
```

**Option B: Permanent (.env file)**

```bash
# Add to .env file
OPENAI_API_KEY=sk-your-key-here
```

**Option C: Permanent (.bashrc/.zshrc)**

```bash
echo 'export OPENAI_API_KEY=sk-your-key-here' >> ~/.zshrc
source ~/.zshrc
```

### Step 3: Install Dependencies

OpenAI Python library should already be installed (used for LLM). Verify:

```bash
python3 -c "import openai; print('âœ… OpenAI installed')"
```

If not:

```bash
pip install openai
```

### Step 4: Restart Server

```bash
# Kill old server
pkill -f uvicorn

# Start with new env var
./start_server.sh
```

---

## ğŸš€ Usage Examples

### Example 1: Basic API Call with OpenAI Reranker

```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "quy trÃ¬nh Ä‘áº¥u tháº§u cÃ´ng khai",
    "mode": "balanced",
    "reranker": "openai"
  }'
```

**Response**:
```json
{
  "answer": "Quy trÃ¬nh Ä‘áº¥u tháº§u cÃ´ng khai Ä‘Æ°á»£c quy Ä‘á»‹nh...",
  "sources": ["Luáº­t Äáº¥u tháº§u 2023, Äiá»u 10", ...],
  "processing_time_ms": 3200
}
```

### Example 2: Compare BGE vs OpenAI

```python
import requests

url = "http://localhost:8000/ask"
query = "Ä‘iá»u kiá»‡n tham gia Ä‘áº¥u tháº§u"

# Test BGE
resp_bge = requests.post(url, json={
    "question": query,
    "mode": "balanced",
    "reranker": "bge"
})

# Test OpenAI
resp_openai = requests.post(url, json={
    "question": query,
    "mode": "balanced",
    "reranker": "openai"
})

print(f"BGE time:    {resp_bge.json()['processing_time_ms']}ms")
print(f"OpenAI time: {resp_openai.json()['processing_time_ms']}ms")
```

### Example 3: Programmatic Usage

```python
from src.retrieval.ranking import OpenAIReranker
from langchain_core.documents import Document

# Initialize reranker
reranker = OpenAIReranker(model_name="gpt-4o-mini")

# Mock documents
docs = [
    Document(page_content="Luáº­t Äáº¥u tháº§u quy Ä‘á»‹nh..."),
    Document(page_content="Nghá»‹ Ä‘á»‹nh 24/2024 hÆ°á»›ng dáº«n..."),
]

# Rerank
results = reranker.rerank(
    query="quy trÃ¬nh Ä‘áº¥u tháº§u",
    documents=docs,
    top_k=5
)

for doc, score in results:
    print(f"{score:.4f} - {doc.page_content[:60]}...")
```

---

## ğŸ§ª Testing

### Test Suite

**File**: `scripts/tests/test_openai_reranker.py`

```bash
# Run all tests
python3 scripts/tests/test_openai_reranker.py

# Run specific test
pytest scripts/tests/test_openai_reranker.py::test_openai_reranker_initialization -v
```

**Tests included**:
1. âœ… Initialization test
2. âœ… Scoring functionality
3. âœ… API integration
4. âœ… BGE vs OpenAI comparison

### Manual Testing

**Test 1: Check API Key**

```bash
python3 -c "
import os
from src.retrieval.ranking import OpenAIReranker

if not os.getenv('OPENAI_API_KEY'):
    print('âŒ API key not set')
else:
    print('âœ… API key found')
    reranker = OpenAIReranker()
    print(f'âœ… Reranker initialized: {reranker.model_name}')
"
```

**Test 2: Basic Reranking**

```bash
python3 -c "
from src.retrieval.ranking import OpenAIReranker
from langchain_core.documents import Document

reranker = OpenAIReranker()
docs = [
    Document(page_content='Luáº­t Äáº¥u tháº§u 2023 quy Ä‘á»‹nh quy trÃ¬nh Ä‘áº¥u tháº§u cÃ´ng khai.'),
    Document(page_content='Nghá»‹ Ä‘á»‹nh 24/2024 hÆ°á»›ng dáº«n chi tiáº¿t Luáº­t Äáº¥u tháº§u.'),
]

results = reranker.rerank('quy trÃ¬nh Ä‘áº¥u tháº§u cÃ´ng khai', docs, top_k=2)
for i, (doc, score) in enumerate(results, 1):
    print(f'[{i}] {score:.4f} - {doc.page_content[:50]}...')
"
```

**Test 3: API Endpoint**

```bash
# Ensure server is running
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "quy trÃ¬nh Ä‘áº¥u tháº§u cÃ´ng khai lÃ  gÃ¬?",
    "mode": "fast",
    "reranker": "openai"
  }' | python3 -m json.tool
```

---

## ğŸ’° Cost Estimation

### OpenAI Pricing (as of 2025-11)

**GPT-4o-mini** (recommended):
- Input: $0.15 / 1M tokens
- Output: $0.60 / 1M tokens

### Example Calculation

**Scenario**: 100 queries/day, 10 docs/query, avg 200 tokens/doc

```
Input tokens:
- Query: 50 tokens Ã— 100 queries = 5,000 tokens
- Documents: 200 tokens Ã— 10 docs Ã— 100 queries = 200,000 tokens
- Prompt overhead: ~50 tokens Ã— 10 Ã— 100 = 50,000 tokens
Total input: ~255,000 tokens/day

Output tokens:
- Score only: 5 tokens Ã— 10 Ã— 100 = 5,000 tokens/day

Daily cost:
- Input: 255k Ã— $0.15 / 1M = $0.038
- Output: 5k Ã— $0.60 / 1M = $0.003
Total: ~$0.04/day = $1.20/month
```

**For 1000 queries/day**: ~$12/month

### Cost Comparison

| Queries/Day | OpenAI Cost | BGE Cost | Savings with BGE |
|-------------|-------------|----------|------------------|
| 100 | $1.20/mo | $0 | $1.20 |
| 1,000 | $12/mo | $0 | $12 |
| 10,000 | $120/mo | $0 | $120 |

**Conclusion**: BGE is **free** and faster, but OpenAI offers easier setup (no GPU).

---

## ğŸ“Š Performance Characteristics

### Latency Breakdown

**BGE Reranker** (GPU):
```
Query enhancement:    200ms
Vector retrieval:     150ms
BGE reranking:        25-50ms   â† Fast!
LLM generation:       2000ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                2375-2400ms
```

**OpenAI Reranker** (API):
```
Query enhancement:    200ms
Vector retrieval:     150ms
OpenAI reranking:     200-500ms â† Slower (network)
LLM generation:       2000ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                2550-2850ms
```

**Impact**: +7-18% latency vs BGE

### Throughput

**BGE**: 
- Single instance: ~40 queries/sec (with GPU)
- Bottleneck: Vector retrieval, not reranking

**OpenAI**:
- Rate limit: 3,500 requests/min (GPT-4o-mini)
- Practical: ~10-20 queries/sec (network latency)
- Bottleneck: API rate limits

---

## ğŸ”§ Configuration Options

### Model Selection

```python
# Default: GPT-4o-mini (recommended)
reranker = OpenAIReranker(model_name="gpt-4o-mini")

# Premium: GPT-4 Turbo (higher quality, 10x cost)
reranker = OpenAIReranker(model_name="gpt-4-turbo-preview")

# Budget: GPT-3.5 Turbo (lower quality, cheaper)
reranker = OpenAIReranker(model_name="gpt-3.5-turbo")
```

### Temperature & Tokens

```python
reranker = OpenAIReranker(
    temperature=0.0,    # Deterministic (recommended for ranking)
    max_tokens=10,      # Only need score number
)
```

### Document Truncation

Default: 2000 characters (~500 tokens) per document

```python
# In openai_reranker.py
max_doc_chars = 2000  # Adjust if needed
```

---

## ğŸ› Troubleshooting

### Error 1: "OpenAI API key required"

```
ValueError: OpenAI API key required!
Set OPENAI_API_KEY environment variable or pass api_key parameter.
```

**Solution**:
```bash
export OPENAI_API_KEY=sk-your-key-here
./start_server.sh
```

### Error 2: "Invalid score format"

```
âš ï¸  Invalid score format: 'The document is relevant', using 0.0
```

**Cause**: GPT returned text instead of number

**Solution**: Already handled with try-except, defaults to 0.0

### Error 3: Rate Limit Exceeded

```
âŒ OpenAI API error: Rate limit exceeded
```

**Solution**:
- Reduce query volume
- Upgrade OpenAI tier
- Switch to BGE reranker

### Error 4: Network Timeout

```
âŒ OpenAI API error: Request timeout
```

**Solution**:
- Check internet connection
- Increase timeout in OpenAI client
- Use BGE for offline scenarios

---

## ğŸ”„ Migration from BGE to OpenAI

### Step 1: Identify Current Usage

```bash
grep -r "get_singleton_reranker" src/
grep -r "BGEReranker" src/
```

### Step 2: Update API Calls

**Before**:
```python
retriever = create_retriever(mode="balanced", enable_reranking=True)
# Uses BGE by default
```

**After**:
```python
retriever = create_retriever(
    mode="balanced",
    enable_reranking=True,
    reranker_type="openai"  # ğŸ†• Switch to OpenAI
)
```

### Step 3: Test Backward Compatibility

```python
# Default behavior unchanged (still uses BGE)
retriever = create_retriever(mode="balanced")  # âœ… Still uses BGE

# Explicit BGE
retriever = create_retriever(mode="balanced", reranker_type="bge")

# New OpenAI option
retriever = create_retriever(mode="balanced", reranker_type="openai")
```

---

## ğŸ“‹ Summary

âœ… **Implemented**:
- OpenAI reranker class (`openai_reranker.py`)
- API endpoint toggle (`reranker` parameter)
- Integration with retriever factory
- Comprehensive test suite
- Cost-effective defaults (gpt-4o-mini)

ğŸ¯ **Usage**:
- Simple: Add `"reranker": "openai"` to API calls
- Default: Still uses BGE (backward compatible)
- Setup: Just need `OPENAI_API_KEY` env var

ğŸ’¡ **Recommendation**:
- **Production**: Use BGE (faster, free, offline)
- **Testing**: Use OpenAI (easier setup, no GPU)
- **Hybrid**: BGE for high-volume, OpenAI for complex queries

---

**Next Steps**:
1. Set `OPENAI_API_KEY` environment variable
2. Restart server: `./start_server.sh`
3. Test with: `curl -X POST http://localhost:8000/ask -d '{"question":"...", "reranker":"openai"}'`
4. Monitor costs at https://platform.openai.com/usage

**Documentation**: See also
- `documents/technical/reranking-analysis/TOM_TAT_TIENG_VIET.md` - BGE reranker guide
- `documents/technical/reranking-analysis/RERANKING_STRATEGIES.md` - Strategy comparison
