# Options ƒë·ªÉ th√™m Document Name v√†o Metadata

## üìä Ph√¢n t√≠ch hi·ªán tr·∫°ng

### Data c√≥ s·∫µn:
1. ‚úÖ **Metadata files** (`data/processed/metadata/*.json`):
   - C√≥ `source_file`: `"data/raw/Ho so moi thau/6. T∆∞ v·∫•n/6. M·∫´u s·ªë 6C_E-TVCN t∆∞ v·∫•n c√° nh√¢n.docx"`
   - C√≥ `category`: `"6. T∆∞ v·∫•n"`
   - C√≥ mapping chunk_count, output_file

2. ‚ùå **Chunks** (`data/processed/chunks/*.jsonl`):
   - KH√îNG c√≥ `source_file`
   - KH√îNG c√≥ `document_name`

3. ‚ùå **Database** (langchain_pg_embedding):
   - `cmetadata` KH√îNG c√≥ `source_file`
   - `cmetadata` KH√îNG c√≥ `document_name`

### Document ID mapping:
```
chunk_id: "bidding_untitled_form_0102"
document_id: "FORM-Bidding/2025#bee720"
source_file: "data/raw/Ho so moi thau/..." (MISSING in DB)
```

---

## üéØ OPTIONS - T·ª´ nhanh ‚Üí to√†n di·ªán

### **OPTION 1: Quick Patch - Extract t·ª´ chunk_id (15 ph√∫t)** ‚ö°
**C√°ch l√†m**: Parse document name t·ª´ `chunk_id` pattern

**∆Øu ƒëi·ªÉm**:
- Nhanh nh·∫•t, kh√¥ng c·∫ßn ƒë·ªçc file
- Kh√¥ng c·∫ßn migrate data
- Ch·ªâ s·ª≠a 1 function trong API

**Nh∆∞·ª£c ƒëi·ªÉm**:
- T√™n kh√¥ng ch√≠nh x√°c (v√≠ d·ª•: "bidding_untitled_form" thay v√¨ t√™n file th·∫≠t)
- Ph·ª• thu·ªôc v√†o naming convention

**Implementation**:
```python
# src/api/routers/documents_management.py

def extract_title_from_metadata(cmetadata: dict) -> str:
    """Extract title t·ª´ chunk_id pattern"""
    chunk_id = cmetadata.get("chunk_id", "")
    
    # Pattern: {type}_{doc_name}_{section}_{index}
    # Example: "bidding_untitled_form_0102"
    parts = chunk_id.split("_")
    if len(parts) >= 3:
        doc_type = parts[0]  # "bidding"
        doc_name = "_".join(parts[1:-1])  # "untitled_form"
        
        # Clean up
        doc_name = doc_name.replace("_", " ").title()
        return f"{doc_type.title()}: {doc_name}"
    
    # Fallback
    return cmetadata.get("section_title", "Unknown Document")[:100]
```

**Result**: `"Bidding: Untitled Form"` (kh√¥ng ƒë·∫πp l·∫Øm)

---

### **OPTION 2: Mapping Table - T·∫°o document_id ‚Üí name lookup (30 ph√∫t)** üìã
**C√°ch l√†m**: T·∫°o mapping dict t·ª´ metadata files, store trong code ho·∫∑c file

**∆Øu ƒëi·ªÉm**:
- T√™n ch√≠nh x√°c t·ª´ source file
- Kh√¥ng c·∫ßn migrate database
- D·ªÖ maintain v√† update

**Nh∆∞·ª£c ƒëi·ªÉm**:
- C·∫ßn build mapping table tr∆∞·ªõc
- Ph·∫£i update khi c√≥ document m·ªõi

**Implementation**:

**Step 1**: Build mapping table
```python
# scripts/build_document_name_mapping.py

import json
import glob
from pathlib import Path

def build_mapping():
    """Build document_id -> document_name mapping"""
    mapping = {}
    
    # Read all metadata files
    for meta_file in glob.glob("data/processed/metadata/*.json"):
        with open(meta_file) as f:
            meta = json.load(f)
        
        source_file = meta.get("source_file", "")
        if not source_file:
            continue
        
        # Extract document name from source_file
        # "data/raw/Ho so moi thau/6. T∆∞ v·∫•n/6. M·∫´u s·ªë 6C_E-TVCN t∆∞ v·∫•n c√° nh√¢n.docx"
        # ‚Üí "6. M·∫´u s·ªë 6C_E-TVCN t∆∞ v·∫•n c√° nh√¢n"
        doc_name = Path(source_file).stem
        
        # Find matching chunks to get document_id
        chunk_file = meta.get("output_file", "")
        if chunk_file:
            # Read first chunk to get document_id
            with open(chunk_file) as cf:
                first_chunk = json.loads(cf.readline())
                doc_id = first_chunk.get("document_id")
                
                if doc_id:
                    mapping[doc_id] = {
                        "name": doc_name,
                        "category": meta.get("category", ""),
                        "source_file": source_file,
                        "document_type": meta.get("document_type", "")
                    }
    
    # Save mapping
    with open("src/config/document_name_mapping.json", "w") as f:
        json.dump(mapping, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Created mapping for {len(mapping)} documents")
    return mapping

if __name__ == "__main__":
    build_mapping()
```

**Step 2**: Use mapping in API
```python
# src/api/routers/documents_management.py

import json
from pathlib import Path

# Load mapping once at startup
MAPPING_FILE = Path(__file__).parent.parent.parent / "config" / "document_name_mapping.json"
DOCUMENT_NAME_MAPPING = {}

try:
    with open(MAPPING_FILE) as f:
        DOCUMENT_NAME_MAPPING = json.load(f)
except FileNotFoundError:
    logger.warning("Document name mapping not found. Run build_document_name_mapping.py")

def extract_title_from_metadata(cmetadata: dict) -> str:
    """Extract title using mapping table"""
    document_id = cmetadata.get("document_id")
    
    # Try mapping first
    if document_id and document_id in DOCUMENT_NAME_MAPPING:
        return DOCUMENT_NAME_MAPPING[document_id]["name"]
    
    # Fallback: section_title
    section_title = cmetadata.get("section_title", "")
    if section_title and len(section_title) < 100:
        return section_title
    
    # Last resort: truncate hierarchy
    if "hierarchy" in cmetadata:
        try:
            hierarchy = json.loads(cmetadata["hierarchy"])
            if hierarchy:
                return hierarchy[0][:100] + ("..." if len(hierarchy[0]) > 100 else "")
        except:
            pass
    
    return f"Document {document_id}"
```

**Result**: `"6. M·∫´u s·ªë 6C_E-TVCN t∆∞ v·∫•n c√° nh√¢n"` (ch√≠nh x√°c!)

---

### **OPTION 3: Backfill Database - Update metadata trong DB (1 gi·ªù)** üîÑ
**C√°ch l√†m**: Update t·∫•t c·∫£ records trong database ƒë·ªÉ th√™m `document_name`

**∆Øu ƒëi·ªÉm**:
- Data ƒë·∫ßy ƒë·ªß, kh√¥ng c·∫ßn lookup
- Permanent solution
- D·ªÖ query v√† filter

**Nh∆∞·ª£c ƒëi·ªÉm**:
- M·∫•t th·ªùi gian ch·∫°y migration
- C·∫ßn test k·ªπ
- Ph·∫£i reprocess n·∫øu c√≥ l·ªói

**Implementation**:

**Step 1**: Build update mapping (gi·ªëng Option 2)
```python
# scripts/migrate_add_document_name.py

import json
import glob
import asyncio
from pathlib import Path
from sqlalchemy import text
from src.config.database import get_session

async def build_document_name_mapping():
    """Same as Option 2"""
    mapping = {}
    
    for meta_file in glob.glob("data/processed/metadata/*.json"):
        with open(meta_file) as f:
            meta = json.load(f)
        
        source_file = meta.get("source_file", "")
        doc_name = Path(source_file).stem if source_file else ""
        
        chunk_file = meta.get("output_file", "")
        if chunk_file and Path(chunk_file).exists():
            with open(chunk_file) as cf:
                first_chunk = json.loads(cf.readline())
                doc_id = first_chunk.get("document_id")
                
                if doc_id:
                    mapping[doc_id] = {
                        "name": doc_name,
                        "category": meta.get("category", ""),
                        "source_file": source_file
                    }
    
    return mapping

async def update_database():
    """Update all chunks in database"""
    mapping = await build_document_name_mapping()
    
    async for db in get_session():
        try:
            # Get all unique document_ids
            result = await db.execute(
                text("SELECT DISTINCT cmetadata->>'document_id' as doc_id FROM langchain_pg_embedding")
            )
            doc_ids = [row[0] for row in result.fetchall()]
            
            updated_count = 0
            for doc_id in doc_ids:
                if doc_id not in mapping:
                    print(f"‚ö†Ô∏è  No mapping for {doc_id}")
                    continue
                
                doc_info = mapping[doc_id]
                
                # Update all chunks for this document
                await db.execute(
                    text("""
                        UPDATE langchain_pg_embedding
                        SET cmetadata = jsonb_set(
                            jsonb_set(
                                jsonb_set(
                                    cmetadata,
                                    '{document_name}',
                                    :doc_name
                                ),
                                '{source_file}',
                                :source_file
                            ),
                            '{category}',
                            :category
                        )
                        WHERE cmetadata->>'document_id' = :doc_id
                    """),
                    {
                        "doc_id": doc_id,
                        "doc_name": json.dumps(doc_info["name"]),
                        "source_file": json.dumps(doc_info["source_file"]),
                        "category": json.dumps(doc_info["category"])
                    }
                )
                
                updated_count += 1
                print(f"‚úÖ Updated {doc_id}: {doc_info['name']}")
            
            await db.commit()
            print(f"\nüéâ Updated {updated_count}/{len(doc_ids)} documents")
            
        except Exception as e:
            await db.rollback()
            print(f"‚ùå Error: {e}")
            raise

if __name__ == "__main__":
    asyncio.run(update_database())
```

**Step 2**: Update API to use new field
```python
# src/api/routers/documents_management.py

def extract_title_from_metadata(cmetadata: dict) -> str:
    """Extract title from metadata (now has document_name!)"""
    # NEW: Use document_name field
    if "document_name" in cmetadata and cmetadata["document_name"]:
        return cmetadata["document_name"]
    
    # Fallback: section_title
    return cmetadata.get("section_title", f"Document {cmetadata.get('document_id', 'Unknown')}")[:100]
```

**Result**: `"6. M·∫´u s·ªë 6C_E-TVCN t∆∞ v·∫•n c√° nh√¢n"` (permanent!)

---

### **OPTION 4: Reprocess Pipeline - Update preprocessing (3 gi·ªù)** üèóÔ∏è
**C√°ch l√†m**: S·ª≠a preprocessing pipeline ƒë·ªÉ lu√¥n l∆∞u `document_name` trong chunks

**∆Øu ƒëi·ªÉm**:
- Solution l√¢u d√†i nh·∫•t
- T·∫•t c·∫£ document m·ªõi s·∫Ω c√≥ `document_name`
- Chu·∫©n h√≥a data structure

**Nh∆∞·ª£c ƒëi·ªÉm**:
- M·∫•t nhi·ªÅu th·ªùi gian nh·∫•t
- Ph·∫£i reprocess ALL documents
- C·∫ßn test thoroughly

**Implementation**:

**Step 1**: Update chunk processing
```python
# src/preprocessing/base_processor.py (ho·∫∑c file t∆∞∆°ng t·ª±)

class DocumentProcessor:
    def process_document(self, file_path: str, **kwargs):
        """Process document and include source metadata"""
        
        # Extract document name from file path
        doc_name = Path(file_path).stem
        category = self._extract_category(file_path)
        
        # Process chunks
        chunks = self._split_into_chunks(file_path)
        
        for chunk in chunks:
            # ADD document_name and source_file to EVERY chunk
            chunk["document_name"] = doc_name
            chunk["source_file"] = file_path
            chunk["category"] = category
            
            yield chunk
    
    def _extract_category(self, file_path: str) -> str:
        """Extract category from folder structure"""
        # "data/raw/Ho so moi thau/6. T∆∞ v·∫•n/file.docx"
        # ‚Üí "6. T∆∞ v·∫•n"
        parts = Path(file_path).parts
        if len(parts) >= 3:
            return parts[-2]  # Parent folder
        return "Unknown"
```

**Step 2**: Reprocess all documents
```bash
# Backup current data first!
cp -r data/processed data/processed_backup_$(date +%Y%m%d)

# Reprocess
python scripts/batch_reprocess_all.py

# Reimport to database
python scripts/import_processed_chunks.py
```

**Result**: All future documents will have proper names automatically

---

## üìä Comparison Matrix

| Option | Time | Accuracy | Permanent | Complexity | Recommended |
|--------|------|----------|-----------|------------|-------------|
| **1. Quick Patch** | 15 min | ‚≠ê‚≠ê | ‚ùå | ‚≠ê | Testing only |
| **2. Mapping Table** | 30 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ (in code) | ‚≠ê‚≠ê | **YES - Best balance** |
| **3. Backfill DB** | 1 hour | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ (in DB) | ‚≠ê‚≠ê‚≠ê | If need DB queries |
| **4. Reprocess** | 3 hours | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ (everywhere) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Long-term only |

---

## üéØ Recommendation: **OPTION 2 + OPTION 3**

### Phase 1: Quick Win (30 ph√∫t)
1. Implement **Option 2** (Mapping Table)
   - Run `build_document_name_mapping.py`
   - Update API to use mapping
   - Test immediately

### Phase 2: Solidify (1 gi·ªù)
2. Run **Option 3** (Backfill Database)
   - Migrate data to add `document_name` field
   - Update API to prefer DB field over mapping
   - Keep mapping as fallback

### Phase 3: Future-proof (sau n√†y)
3. Update preprocessing pipeline (Option 4)
   - Only for new documents
   - No need to reprocess existing

---

## üöÄ Quick Start (Option 2)

```bash
# 1. Build mapping
python scripts/build_document_name_mapping.py

# 2. Restart server
./start_server.sh

# 3. Test
curl "http://localhost:8000/api/documents/catalog?limit=5" | jq '.[].title'
```

Expected output:
```
"6. M·∫´u s·ªë 6C_E-TVCN t∆∞ v·∫•n c√° nh√¢n"
"14A. M·∫´u BCƒêG PTV HH XL hop hop TBYT CGTT. quy tr√¨nh 1_1 tui"
...
```

---

## ‚ö†Ô∏è Notes

1. **Encoding**: Ensure UTF-8 for Vietnamese characters
2. **Missing mappings**: Some documents might not have metadata files
3. **Backup**: Always backup before Option 3 or 4
4. **Testing**: Test with small batch first

---

## üìù Next Steps

Sau khi implement xong, c√≥ th·ªÉ:
1. Add API endpoint ƒë·ªÉ manage document names
2. Add search by document name
3. Add filter by category
4. Export document catalog to Excel
