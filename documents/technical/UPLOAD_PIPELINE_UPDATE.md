# Upload Pipeline Update - Documents Table Integration

**Date:** November 20, 2025  
**Status:** ‚úÖ COMPLETED - Ready for testing

## üéØ Objective

Update preprocessing pipeline ƒë·ªÉ insert v√†o `documents` table sau khi upload file, ensuring consistency gi·ªØa vector DB v√† documents table.

## üìã Changes Made

### 1. Created DocumentIDGenerator ‚úÖ

**File:** `src/preprocessing/utils/document_id_generator.py`

**Features:**
- Generate structured document IDs from filenames
- Patterns supported:
  - `LUA-{s·ªë}-{nƒÉm}-{m√£}` - Lu·∫≠t (Laws)
  - `ND-{s·ªë}-{nƒÉm}-{m√£}` - Ngh·ªã ƒë·ªãnh (Decrees)
  - `TT-{s·ªë}-{nƒÉm}-{m√£}` - Th√¥ng t∆∞ (Circulars)
  - `QD-{s·ªë}-{nƒÉm}-{m√£}` - Quy·∫øt ƒë·ªãnh (Decisions)
  - `FORM-{t√™n}` - Bidding documents
  - `TEMPLATE-{t√™n}` - Report templates
  - `EXAM-{t√™n}` - Exam questions
- Fallback to sanitized filename if pattern not matched

**Examples:**
```python
generator = DocumentIDGenerator()

# Law file
generator.generate("Lu·∫≠t s·ªë 43-2024-QH15.docx", "law")
# ‚Üí "LUA-43-2024-QH15"

# Decree file
generator.generate("Ngh·ªã ƒë·ªãnh 78-2023-Nƒê-CP.docx", "decree")
# ‚Üí "ND-78-2023-ND-CP"

# Bidding document
generator.generate("HSMT-2024-001.docx", "bidding")
# ‚Üí "FORM-Bidding-HSMT-2024-001"
```

### 2. Updated Upload Pipeline ‚úÖ

**File:** `src/preprocessing/upload_pipeline.py`

**Changes:**
- Added `DocumentIDGenerator` import and initialization
- Generate `document_id` in Step 2 (after content extraction, before ProcessedDocument)
- Include `document_id` in ProcessedDocument metadata
- Ensure `document_id` propagates to all chunks
- Updated pipeline version to `v2.0`

**Key Code:**
```python
# Step 2: Generate document ID
document_id = self.doc_id_generator.generate(
    filename=file_path.name,
    doc_type=document_type,
    title=None
)

# Step 3: Create ProcessedDocument
processed_doc = ProcessedDocument(
    metadata={
        "document_id": document_id,  # ‚Üê NEW
        "source_file": file_path.name,
        "document_type": document_type,
        # ...
    },
    # ...
)
```

### 3. Updated Upload Service ‚úÖ

**File:** `src/api/services/upload_service.py`

**Changes:**
- Added `DocumentIDGenerator` import
- Added database imports: `get_db_sync`, `text` from SQLAlchemy
- Added `_insert_into_documents_table()` method
- Call insert method in `_process_single_file()` after vector store (Step 5.5)

**New Method:**
```python
def _insert_into_documents_table(
    self,
    document_id: str,
    document_name: str,
    document_type: str,
    category: str,
    file_name: str,
    source_file: str,
    total_chunks: int
):
    """
    Insert document record into documents table.
    Uses UPSERT to handle duplicates.
    """
    # INSERT ... ON CONFLICT DO UPDATE
    # Populates: document_id, document_name, document_type, 
    #            category, file_name, source_file, total_chunks,
    #            status='active', timestamps
```

**Integration Point:**
```python
# Step 5: Store in vector database
self.vector_store.add_documents(documents)

# Step 5.5: Insert into documents table ‚Üê NEW
self._insert_into_documents_table(
    document_id=document_id,
    document_name=document_name,
    document_type=doc_type,
    category=category,
    file_name=file_name,
    source_file=source_file,
    total_chunks=len(chunks)
)
```

### 4. Existing Endpoints (Already Working) ‚úÖ

**File:** `src/api/routers/documents_management.py`

These endpoints were already implemented and require NO changes:

- **GET `/documents/catalog`** - List all documents with filters
- **GET `/documents/catalog/{document_id}`** - Get document details
- **PATCH `/documents/catalog/{document_id}/status`** - Toggle status
- **GET `/documents/catalog/{document_id}/stats`** - Document statistics

## üîß How It Works

### Upload Flow (Updated)

```
1. User uploads file ‚Üí POST /upload/files
2. UploadProcessingService receives file
3. WorkingUploadPipeline.process_file():
   - Extract content (DocxLoader/DocLoader)
   - Generate document_id ‚Üê NEW
   - Create ProcessedDocument with document_id
   - Chunk content
   - Enrich chunks
4. UploadService._process_single_file():
   - Load file content
   - Classify document type
   - Process with pipeline
   - Generate embeddings
   - Store in vector DB
   - Insert into documents table ‚Üê NEW
5. Return upload_id for status tracking
```

### Documents Table Schema

```sql
CREATE TABLE documents (
    document_id TEXT PRIMARY KEY,           -- Generated by DocumentIDGenerator
    document_name TEXT NOT NULL,            -- From chunk title/filename
    document_type TEXT NOT NULL,            -- law, decree, circular, etc.
    category TEXT NOT NULL,                 -- Mapped from document_type
    file_name TEXT NOT NULL,                -- Original filename
    source_file TEXT NOT NULL,              -- Full file path
    total_chunks INTEGER NOT NULL,          -- Count from pipeline
    status TEXT DEFAULT 'active',           -- active, archived, pending
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

## üìù Testing Instructions

### Prerequisites

1. **Start server:**
   ```bash
   ./start_server.sh
   ```

2. **Verify database:**
   ```bash
   psql -h localhost -U sakana -d rag_bidding_v2 -c "SELECT COUNT(*) FROM documents;"
   ```

### Automated Test

Run test suite:
```bash
python scripts/tests/test_upload_workflow.py
```

**Test coverage:**
- ‚úÖ Upload file ‚Üí Verify documents table insert
- ‚úÖ GET /catalog ‚Üí Verify new document appears
- ‚úÖ PATCH status ‚Üí Toggle active/archived
- ‚úÖ Database verification

### Manual Testing

#### Test 1: Upload File

```bash
curl -X POST "http://localhost:8000/upload/files" \
  -F "files=@data/raw/Luat chinh/Luat-dau-thau-43-2013-QH13.docx" \
  -F "batch_name=manual_test" \
  -F "enable_enrichment=false"
```

**Expected:**
- Status 202 Accepted
- Returns `upload_id`
- Processing completes within 30s

**Verify in DB:**
```sql
-- Check documents table
SELECT document_id, document_name, document_type, total_chunks, status
FROM documents
WHERE document_id LIKE 'LUA-%'
ORDER BY created_at DESC
LIMIT 5;

-- Check vector DB
SELECT 
    cmetadata->>'document_id' as doc_id,
    COUNT(*) as chunks
FROM langchain_pg_embedding
WHERE cmetadata->>'document_id' LIKE 'LUA-%'
GROUP BY cmetadata->>'document_id'
ORDER BY MAX((cmetadata->>'chunk_index')::int) DESC
LIMIT 5;
```

#### Test 2: Catalog Listing

```bash
# Get all documents
curl "http://localhost:8000/documents/catalog?limit=20"

# Filter by type
curl "http://localhost:8000/documents/catalog?document_type=law&limit=10"

# Filter by status
curl "http://localhost:8000/documents/catalog?status=active&limit=10"
```

**Expected:**
- Returns array of DocumentSummary objects
- Each has: document_id, title, document_type, total_chunks, status
- Filters work correctly

#### Test 3: Toggle Status

```bash
# Get document_id from catalog
DOC_ID="LUA-43-2013-QH13"  # Replace with actual

# Set to archived
curl -X PATCH "http://localhost:8000/documents/catalog/$DOC_ID/status" \
  -H "Content-Type: application/json" \
  -d '{
    "status": "archived",
    "reason": "Manual test - archiving"
  }'

# Verify
curl "http://localhost:8000/documents/catalog/$DOC_ID"

# Restore to active
curl -X PATCH "http://localhost:8000/documents/catalog/$DOC_ID/status" \
  -H "Content-Type: application/json" \
  -d '{
    "status": "active",
    "reason": "Manual test - restoring"
  }'
```

**Expected:**
- Status updates successfully
- Returns chunks_updated count
- All chunks' metadata updated in vector DB

## ‚úÖ Verification Checklist

After testing, verify:

- [ ] Upload creates row in `documents` table
- [ ] `document_id` follows correct format (LUA-*, ND-*, FORM-*, etc.)
- [ ] `file_name` populated correctly
- [ ] `source_file` has full path
- [ ] `total_chunks` matches vector DB count
- [ ] `status` defaults to 'active'
- [ ] Catalog endpoint returns new document
- [ ] Status toggle updates both tables
- [ ] No duplicate document_id conflicts

## üêõ Known Issues

### None Currently

All files compiled without errors. DocumentIDGenerator patterns tested with regex.

### Potential Issues to Watch

1. **Document ID Collisions:**
   - Multiple files with same law number ‚Üí Same document_id
   - Mitigated by UPSERT (ON CONFLICT DO UPDATE)

2. **File Name Encoding:**
   - Vietnamese characters in filenames
   - Handled by `_sanitize_text()` with unidecode

3. **Database Connection:**
   - `get_db_sync()` used in upload_service
   - Ensure connection pool configured

## üìö Related Documentation

- **Migration Analysis:** `documents/migration/PREPROCESSING_UPDATE_ANALYSIS.md`
- **Database Schema:** `scripts/migration/001_simple_documents_schema.sql`
- **Fix Notebook:** `notebooks/migration/fix-migration-links.ipynb`
- **Catalog API:** `src/api/routers/documents_management.py`

## üéØ Next Steps

1. **Run automated test:**
   ```bash
   python scripts/tests/test_upload_workflow.py
   ```

2. **If tests pass:**
   - Update existing documents with document IDs (backfill)
   - Test with different document types (decree, circular, etc.)
   - Load test with multiple concurrent uploads

3. **If tests fail:**
   - Check server logs: `logs/server-log-deprecated.txt`
   - Check database connection
   - Verify file paths in test script

## üîÑ Rollback Plan

If issues occur:

1. **Revert code changes:**
   ```bash
   git checkout HEAD~1 -- src/preprocessing/upload_pipeline.py
   git checkout HEAD~1 -- src/api/services/upload_service.py
   ```

2. **Documents table is append-only, safe to keep**
   - No data loss
   - Can clean up later with DELETE queries

3. **Vector DB unaffected**
   - Pipeline still works without documents table insert
   - Only missing: documents table synchronization

## üìû Support

For issues or questions:
- Check logs in `logs/` directory
- Review error messages in test output
- Inspect database state with SQL queries above
