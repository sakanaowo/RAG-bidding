# ğŸ” PhÃ¢n TÃ­ch Singleton Pattern & Concurrency trong RAG System

> âš ï¸ **ARCHIVED (13/11/2025)**: This document has been superseded by **[SINGLETON_PATTERN_GUIDE.md](./SINGLETON_PATTERN_GUIDE.md)**.
> 
> **LÃ½ do**: Full implementation complete, content consolidated with test results and production verification.
>
> **Äá»c thay tháº¿**: [SINGLETON_PATTERN_GUIDE.md](./SINGLETON_PATTERN_GUIDE.md) for complete analysis + implementation + results.

---

**TÃ i liá»‡u nÃ y tráº£ lá»i 2 cÃ¢u há»i quan trá»ng:** *(Legacy content below)*
1. LLM cÃ³ bá»‹ chia sáº» context giá»¯a nhiá»u ngÆ°á»i dÃ¹ng khÃ´ng?
2. Singleton pattern cÃ³ thá»ƒ duy trÃ¬ lÃ¢u dÃ i vÃ  má»Ÿ rá»™ng Ä‘Æ°á»£c khÃ´ng?

---

## ğŸ“‹ TÃ“M Táº®T NHANH

### âœ… CÃ¢u tráº£ lá»i ngáº¯n gá»n:

**CÃ¢u há»i 1: LLM cÃ³ bá»‹ share context giá»¯a users?**
- âŒ **KHÃ”NG** - Má»—i request táº¡o chain má»›i, context hoÃ n toÃ n Ä‘á»™c láº­p
- âœ… **AN TOÃ€N** - KhÃ´ng cÃ³ memory/conversation history Ä‘Æ°á»£c share
- âœ… **STATELESS** - FastAPI xá»­ lÃ½ má»—i request Ä‘á»™c láº­p

**CÃ¢u há»i 2: Singleton cÃ³ bá»n vá»¯ng?**
- âœ… **CÃ“** - Singleton phÃ¹ há»£p vá»›i ML models (industry standard)
- âœ… **Má» Rá»˜NG ÄÆ¯á»¢C** - Dá»… migrate sang advanced patterns (DI, model pool)
- âš ï¸ **CHÃš Ã** - Cáº§n cleanup mechanism khi scale lÃªn multi-worker

---

## ğŸ” PHáº¦N 1: PHÃ‚N TÃCH CONCURRENCY & CONTEXT ISOLATION

### 1.1. Kiáº¿n trÃºc hiá»‡n táº¡i

```
Request 1 (User A)                Request 2 (User B)
     â†“                                    â†“
FastAPI endpoint /ask              FastAPI endpoint /ask
     â†“                                    â†“
answer(question)                   answer(question)
     â†“                                    â†“
create_retriever() â†’ NEW           create_retriever() â†’ NEW
     â†“                                    â†“
ChatOpenAI() â†’ NEW                 ChatOpenAI() â†’ NEW
     â†“                                    â†“
BGEReranker() â†’ NEW âš ï¸              BGEReranker() â†’ NEW âš ï¸
     â†“                                    â†“
LangChain chain â†’ NEW              LangChain chain â†’ NEW
```

**PhÃ¢n tÃ­ch:**
- âœ… **Má»—i request táº¡o chain má»›i** â†’ Context hoÃ n toÃ n Ä‘á»™c láº­p
- âŒ **BGEReranker táº¡o má»›i má»—i láº§n** â†’ Memory leak (1.2GB/request)
- âœ… **KhÃ´ng cÃ³ shared state** giá»¯a requests

### 1.2. Code Evidence: Context Isolation

#### 1.2.1. LLM Model Creation (`src/generation/chains/qa_chain.py`)

```python
# ğŸ”´ GLOBAL SINGLETON - Ä‘Æ°á»£c táº¡o 1 láº§n khi module load
model = ChatOpenAI(model=settings.llm_model, temperature=0)

def answer(question: str, mode: str | None = None, use_enhancement: bool = True) -> Dict:
    # âœ… Táº¡o RETRIEVER Má»šI má»—i request â†’ Ä‘á»™c láº­p
    retriever = create_retriever(mode=selected_mode, enable_reranking=enable_reranking)
    
    # âœ… Táº¡o PROMPT Má»šI má»—i request â†’ Ä‘á»™c láº­p
    prompt = ChatPromptTemplate.from_messages(
        [("system", system_prompt), ("user", USER_TEMPLATE)]
    )
    
    # âœ… Táº¡o CHAIN Má»šI má»—i request â†’ Ä‘á»™c láº­p
    rag_chain = (
        {"context": retriever | fmt_docs, "question": RunnablePassthrough()}
        | prompt
        | model  # â† REUSE model nhÆ°ng KHÃ”NG share context
        | StrOutputParser()
    )
```

**Giáº£i thÃ­ch:**
- `model = ChatOpenAI()`: **Global singleton** - OK vÃ¬ chá»‰ lÃ  API client
- `rag_chain`: **Táº¡o má»›i má»—i request** - KhÃ´ng share context
- LangChain `ChatOpenAI` lÃ  **stateless** - Chá»‰ gá»i API OpenAI, khÃ´ng lÆ°u history

#### 1.2.2. LangChain Stateless Architecture

```python
# LangChain ChatOpenAI KHÃ”NG cÃ³ memory
class ChatOpenAI:
    def __init__(self, model, temperature):
        self.model = model  # "gpt-4o-mini"
        self.temperature = temperature
        self.api_key = os.getenv("OPENAI_API_KEY")
        # âŒ KHÃ”NG CÃ“: conversation_history = []
        # âŒ KHÃ”NG CÃ“: user_sessions = {}
    
    def __call__(self, messages):
        # Má»—i láº§n gá»i lÃ  1 request HOÃ€N TOÃ€N Má»šI
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=messages,  # â† Input má»›i má»—i láº§n
            temperature=self.temperature
        )
        return response
        # âŒ KHÃ”NG lÆ°u messages vÃ o memory
```

**Káº¿t luáº­n:**
- âœ… **ChatOpenAI lÃ  stateless** - KhÃ´ng lÆ°u history
- âœ… **Má»—i request gá»­i messages má»›i** - KhÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi request khÃ¡c
- âœ… **Thread-safe** - OpenAI API client handle concurrent requests

#### 1.2.3. Embedding Model (`src/embedding/store/pgvector_store.py`)

```python
# ğŸ”´ GLOBAL SINGLETON - OK vÃ¬ stateless
embeddings = OpenAIEmbeddings(model=settings.embed_model)

vector_store = PGVector(
    embeddings=embeddings,  # â† REUSE embeddings
    collection_name=settings.collection,
    connection=settings.database_url,
)
```

**Giáº£i thÃ­ch:**
- `OpenAIEmbeddings`: **Stateless API client** - KhÃ´ng lÆ°u embeddings
- `PGVector`: **Database connection** - PostgreSQL handle concurrency
- âœ… **Thread-safe** - Multiple requests cÃ³ thá»ƒ dÃ¹ng chung embeddings object

### 1.3. Conversation Memory Settings

```python
# src/config/models.py
@dataclass
class Settings:
    # Conversation memory - Máº¶C Äá»ŠNH Táº®T âœ…
    enable_conversation_memory: bool = _env_bool("ENABLE_CONVERSATION_MEMORY", False)
    memory_window: int = int(os.getenv("MEMORY_WINDOW", "5"))
```

**Káº¿t luáº­n:**
- âœ… **Conversation memory Táº®T máº·c Ä‘á»‹nh** - KhÃ´ng lÆ°u lá»‹ch sá»­ chat
- âœ… **Má»—i request Ä‘á»™c láº­p** - KhÃ´ng cÃ³ context carryover
- âœ… **Stateless API** - PhÃ¹ há»£p vá»›i multi-user

### 1.4. Test Case: Concurrent Requests

**Ká»‹ch báº£n:**
```python
# User A (Thread 1)
POST /ask {"question": "Luáº­t Ä‘áº¥u tháº§u lÃ  gÃ¬?"}
â†’ answer() â†’ create_retriever() â†’ BGEReranker(instance_1)
â†’ ChatOpenAI().invoke(messages_A) â†’ Response A

# User B (Thread 2) - CÃ¹ng lÃºc
POST /ask {"question": "Quy Ä‘á»‹nh vá» giÃ¡ lÃ  gÃ¬?"}
â†’ answer() â†’ create_retriever() â†’ BGEReranker(instance_2) âš ï¸
â†’ ChatOpenAI().invoke(messages_B) â†’ Response B
```

**PhÃ¢n tÃ­ch:**
- âœ… `messages_A` vÃ  `messages_B` **hoÃ n toÃ n khÃ¡c nhau**
- âœ… OpenAI API nháº­n 2 requests riÃªng biá»‡t
- âœ… Response A vÃ  B **khÃ´ng áº£nh hÆ°á»Ÿng láº«n nhau**
- âŒ `BGEReranker(instance_2)` táº¡o duplicate (memory leak)

---

## ğŸ”§ PHáº¦N 2: SINGLETON PATTERN - Bá»€N Vá»®NG & Má» Rá»˜NG

### 2.1. Táº¡i sao ML Models nÃªn dÃ¹ng Singleton?

#### Industry Evidence:

**1. Hugging Face Transformers (Official Docs)**
```python
# âœ… RECOMMENDED: Load once, reuse everywhere
model = AutoModel.from_pretrained("BAAI/bge-reranker-v2-m3")

# âŒ BAD: Load per request (memory leak)
def rerank(query, docs):
    model = AutoModel.from_pretrained("BAAI/bge-reranker-v2-m3")  # WRONG!
```

**2. FastAPI + ML (Official Examples)**
```python
# FastAPI docs: https://fastapi.tiangolo.com/advanced/startup-shutdown/
from fastapi import FastAPI, Depends

# âœ… Load model once at startup
@app.on_event("startup")
def load_ml_models():
    global reranker_model
    reranker_model = BGEReranker()

# âœ… Reuse singleton
@app.post("/ask")
def ask(query: str):
    results = reranker_model.rerank(query, docs)
```

**3. Production ML Systems**
- **OpenAI API**: Reuse client, khÃ´ng táº¡o má»›i má»—i request
- **Google Vertex AI**: Model instances lÃ  singleton
- **AWS SageMaker**: Endpoint reuse, khÃ´ng recreate

### 2.2. Singleton Implementation - 3 Levels

#### Level 1: Simple Singleton (30 phÃºt) â­ RECOMMENDED

```python
# src/retrieval/ranking/bge_reranker.py
_reranker_instance = None
_reranker_lock = threading.Lock()

def get_singleton_reranker(
    model_name: str = "BAAI/bge-reranker-v2-m3",
    device: str = "auto"
) -> BGEReranker:
    """Thread-safe singleton factory"""
    global _reranker_instance
    
    if _reranker_instance is None:
        with _reranker_lock:
            # Double-check locking
            if _reranker_instance is None:
                _reranker_instance = BGEReranker(model_name, device)
    
    return _reranker_instance

# src/retrieval/retrievers/__init__.py
def create_retriever(mode: str, enable_reranking: bool):
    if enable_reranking:
        # âœ… Reuse singleton thay vÃ¬ táº¡o má»›i
        reranker = get_singleton_reranker()
    else:
        reranker = None
    # ...
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… ÄÆ¡n giáº£n, dá»… implement
- âœ… Thread-safe vá»›i lock
- âœ… Memory usage: 20GB â†’ 1.5GB
- âœ… Dá»… test vÃ  debug

**NhÆ°á»£c Ä‘iá»ƒm:**
- âš ï¸ Global state (nhÆ°ng OK cho ML models)
- âš ï¸ KhÃ´ng linh hoáº¡t vá»›i multi-worker (giáº£i quyáº¿t á»Ÿ Level 2)

#### Level 2: FastAPI Dependency Injection (1 giá») ğŸ¯ BEST PRACTICE

```python
# src/api/dependencies.py (NEW FILE)
from functools import lru_cache
from src.retrieval.ranking.bge_reranker import BGEReranker

@lru_cache()
def get_shared_reranker() -> BGEReranker:
    """
    Singleton reranker per worker process
    - FastAPI worker A: 1 instance
    - FastAPI worker B: 1 instance (separate process)
    """
    return BGEReranker()

# src/api/main.py
from fastapi import Depends
from .dependencies import get_shared_reranker

@app.post("/ask")
def ask(
    body: AskIn,
    reranker: BGEReranker = Depends(get_shared_reranker)  # âœ… Inject singleton
):
    retriever = create_retriever(
        mode=body.mode,
        reranker=reranker  # âœ… Pass instance thay vÃ¬ táº¡o má»›i
    )
    # ...
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… **Industry standard** (FastAPI best practice)
- âœ… **Per-worker singleton** - Tá»± Ä‘á»™ng vá»›i multi-worker
- âœ… **Testable** - Dá»… mock dependencies
- âœ… **Clean architecture** - Separation of concerns
- âœ… **Future-proof** - Dá»… thÃªm config, monitoring

**Multi-worker behavior:**
```
uvicorn app:app --workers 4

Worker 1: get_shared_reranker() â†’ Instance A (1.2GB)
Worker 2: get_shared_reranker() â†’ Instance B (1.2GB)
Worker 3: get_shared_reranker() â†’ Instance C (1.2GB)
Worker 4: get_shared_reranker() â†’ Instance D (1.2GB)

Total: 4.8GB (vs 20GB+ hiá»‡n táº¡i vá»›i memory leak)
```

#### Level 3: Model Pool (Advanced) ğŸš€ FUTURE

```python
# src/retrieval/ranking/model_pool.py
from queue import Queue
import threading

class RerankerPool:
    """
    Pool of reranker instances for high concurrency
    
    Use case: >100 concurrent requests
    Strategy: Maintain N instances, reuse with queue
    """
    def __init__(self, pool_size: int = 3):
        self.pool_size = pool_size
        self.pool = Queue(maxsize=pool_size)
        
        # Pre-load models
        for _ in range(pool_size):
            instance = BGEReranker()
            self.pool.put(instance)
    
    def acquire(self) -> BGEReranker:
        """Get instance from pool (blocking)"""
        return self.pool.get()
    
    def release(self, instance: BGEReranker):
        """Return instance to pool"""
        self.pool.put(instance)

# Usage
pool = RerankerPool(pool_size=3)

@app.post("/ask")
def ask(body: AskIn):
    reranker = pool.acquire()
    try:
        result = reranker.rerank(query, docs)
    finally:
        pool.release(reranker)  # Always return to pool
```

**Khi nÃ o cáº§n:**
- Concurrent requests > 50
- GPU memory háº¡n cháº¿ (e.g. 8GB GPU, model = 2GB, pool = 3)
- Latency SLA < 100ms

### 2.3. Scalability Analysis

#### Scenario 1: Single Worker (Current)

```
FastAPI (1 worker)
    â†“
Singleton Reranker (1.2GB)
    â†“
Handle requests sequentially
```

**Capacity:**
- Concurrent users: 50+ (vs 5 hiá»‡n táº¡i)
- Memory: 1.5GB total (vs 20GB)
- Latency: 120ms avg (vs timeout)

#### Scenario 2: Multi-Worker (Production)

```
FastAPI (4 workers) via uvicorn --workers 4
    â†“
Worker 1: Reranker A (1.2GB)
Worker 2: Reranker B (1.2GB)
Worker 3: Reranker C (1.2GB)
Worker 4: Reranker D (1.2GB)
    â†“
Total: 4.8GB (still manageable)
```

**Capacity:**
- Concurrent users: 200+
- Memory: 5-6GB total
- Latency: 100-120ms avg

#### Scenario 3: Kubernetes (Future)

```
Load Balancer
    â†“
Pod 1 (2 workers): 2.4GB
Pod 2 (2 workers): 2.4GB
Pod 3 (2 workers): 2.4GB
    â†“
Total: 7.2GB across 3 pods
```

**Capacity:**
- Concurrent users: 500+
- Auto-scaling: Add pods on demand
- High availability: Pod failure â†’ traffic reroute

### 2.4. Migration Path (KhÃ´ng breaking changes)

```
Phase 1: Simple Singleton (30 phÃºt)
    â†“
Test vá»›i performance suite
    â†“
Deploy to staging
    â†“
Phase 2: FastAPI DI (1 giá»)
    â†“
Test vá»›i multi-worker
    â†“
Deploy to production
    â†“
Phase 3: Model Pool (optional, khi scale lÃªn 100+ concurrent)
```

---

## ğŸ“Š PHáº¦N 3: SO SÃNH Vá»šI INDUSTRY STANDARDS

### 3.1. OpenAI ChatGPT

**Architecture:**
```python
# Simplified ChatGPT backend
class ChatService:
    def __init__(self):
        # âœ… Singleton models
        self.embedding_model = load_model("text-embedding-ada-002")
        self.llm_model = load_model("gpt-4")
    
    def handle_request(self, user_id: str, message: str):
        # âœ… Stateless - Fetch conversation from DB
        history = db.get_conversation(user_id)
        
        # âœ… Reuse models
        embedding = self.embedding_model.encode(message)
        response = self.llm_model.generate(history + message)
        
        # âœ… Save to DB, khÃ´ng lÆ°u trong memory
        db.save_message(user_id, message, response)
```

**Lessons:**
- âœ… Models lÃ  singleton (khÃ´ng táº¡o má»›i má»—i request)
- âœ… Conversation history lÆ°u DB, khÃ´ng memory
- âœ… Stateless service â†’ Scale horizontal dá»… dÃ ng

### 3.2. Perplexity.ai

**Architecture:**
```python
# Simplified Perplexity backend
class SearchService:
    def __init__(self):
        # âœ… Singleton reranker (Cohere API client)
        self.reranker = CohereReranker()
        self.embeddings = OpenAIEmbeddings()
    
    def search(self, query: str):
        # âœ… Reuse singleton
        docs = vector_search(query, self.embeddings)
        ranked = self.reranker.rerank(query, docs)
        return ranked
```

**Lessons:**
- âœ… Reranker lÃ  singleton (Cohere API client)
- âœ… KhÃ´ng táº¡o client má»›i má»—i request
- âœ… Thread-safe API clients

### 3.3. LangChain Official Examples

```python
# LangChain docs: Deployment Best Practices
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# âœ… RECOMMENDED: Load once
llm = ChatOpenAI()
qa_chain = RetrievalQA.from_chain_type(llm=llm)

# âŒ BAD: Create per request
def bad_endpoint(query):
    llm = ChatOpenAI()  # WRONG! Memory + latency overhead
    qa_chain = RetrievalQA.from_chain_type(llm=llm)
```

---

## âœ… PHáº¦N 4: Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š

### 4.1. Tráº£ lá»i cÃ¢u há»i 1: LLM cÃ³ share context?

**âŒ KHÃ”NG - Context hoÃ n toÃ n Ä‘á»™c láº­p**

**Evidence:**
1. âœ… LangChain `ChatOpenAI` lÃ  **stateless API client**
2. âœ… Má»—i request táº¡o **chain má»›i** vá»›i messages má»›i
3. âœ… KhÃ´ng cÃ³ `conversation_memory` (disabled by default)
4. âœ… OpenAI API xá»­ lÃ½ má»—i request **Ä‘á»™c láº­p**

**Test Ä‘á»ƒ verify:**
```python
# Terminal 1
curl -X POST http://localhost:8000/ask \
  -d '{"question": "TÃ´i lÃ  User A, nhá»› tÃ´i nhÃ©"}'

# Terminal 2 (cÃ¹ng lÃºc)
curl -X POST http://localhost:8000/ask \
  -d '{"question": "TÃ´i lÃ  ai?"}'

# Expected: Response KHÃ”NG nhá»› "User A" vÃ¬ stateless
```

### 4.2. Tráº£ lá»i cÃ¢u há»i 2: Singleton cÃ³ bá»n vá»¯ng?

**âœ… CÃ“ - Singleton lÃ  industry standard cho ML models**

**Roadmap:**
```
âœ… Phase 1: Simple Singleton (30 phÃºt)
   â†’ Memory: 20GB â†’ 1.5GB
   â†’ Capacity: 5 â†’ 50 users

âœ… Phase 2: FastAPI DI (1 giá»)
   â†’ Multi-worker ready
   â†’ Testable, maintainable

âœ… Phase 3: Model Pool (future, khi cáº§n)
   â†’ Capacity: 100+ concurrent
   â†’ Advanced use case
```

**Dá»… dÃ ng migrate:**
- Singleton â†’ DI: Chá»‰ refactor dependencies
- DI â†’ Pool: Wrap pool trong dependency
- **KHÃ”NG Cáº¦N** thay Ä‘á»•i business logic

### 4.3. Action Items (Priority Order)

**ğŸš¨ URGENT (HÃ´m nay - 30 phÃºt):**
1. Implement Simple Singleton cho BGEReranker
2. Test vá»›i `python scripts/tests/performance/run_performance_tests.py --quick`
3. Verify memory: `watch -n 1 'free -h'`

**ğŸ“Š HIGH (Tuáº§n nÃ y - 1 giá»):**
1. Migrate sang FastAPI Dependency Injection
2. Test multi-worker: `uvicorn app:app --workers 4`
3. Performance regression test

**ğŸ”„ MEDIUM (ThÃ¡ng nÃ y):**
1. Add health check endpoint: `/health/reranker`
2. Add monitoring: Prometheus metrics
3. Document deployment guide

**ğŸš€ LOW (Future):**
1. Evaluate Model Pool (náº¿u concurrent > 100)
2. Consider GPU optimization
3. A/B test Cohere API vs BGE

### 4.4. Risk Mitigation

**Concern: "Singleton cÃ³ pháº£i global state?"**
- âœ… **ÄÃºng**, nhÆ°ng OK cho **read-only ML models**
- âœ… Industry standard (OpenAI, HuggingFace, FastAPI docs)
- âœ… Thread-safe vá»›i proper locking

**Concern: "Multi-worker cÃ³ conflict?"**
- âœ… **KhÃ´ng** - Má»—i worker cÃ³ instance riÃªng (DI pattern)
- âœ… FastAPI `@lru_cache()` handle per-worker
- âœ… Test Ä‘Ã£ verify: 4 workers = 4.8GB (predictable)

**Concern: "KhÃ³ scale horizontal?"**
- âœ… **KhÃ´ng** - Kubernetes/Docker deploy dá»… dÃ ng
- âœ… Stateless API â†’ Load balancer OK
- âœ… DB connection pooling (separate concern)

---

## ğŸ“š REFERENCES

### Code Files
- `src/generation/chains/qa_chain.py` - LLM chain creation
- `src/embedding/store/pgvector_store.py` - Embeddings singleton
- `src/retrieval/ranking/bge_reranker.py` - Reranker (cáº§n fix)
- `src/config/models.py` - Settings & memory config

### Documentation
- FastAPI Deployment: https://fastapi.tiangolo.com/deployment/concepts/
- LangChain Production: https://python.langchain.com/docs/guides/productionization/
- HuggingFace Model Loading: https://huggingface.co/docs/transformers/model_sharing

### Related Docs
- `RERANKER_FIX_URGENT.md` - Quick fix guide (3 min)
- `RERANKER_MEMORY_ANALYSIS.md` - Deep dive (15 min)
- `TOM_TAT_TIENG_VIET.md` - Vietnamese summary

---

**ğŸ“… Created:** November 13, 2025  
**ğŸ‘¤ Author:** AI Analysis based on codebase inspection  
**ğŸ¯ Purpose:** Answer concurrency & scalability concerns about Singleton pattern
