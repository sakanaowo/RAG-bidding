# â“ FAQ: Concurrency & Singleton Pattern

> âš ï¸ **ARCHIVED (13/11/2025)**: This document has been superseded by **[SINGLETON_PATTERN_GUIDE.md](./SINGLETON_PATTERN_GUIDE.md) Section 7 (FAQ & Troubleshooting)**.
> 
> **LÃ½ do**: Full implementation complete, content consolidated into comprehensive guide.
>
> **Äá»c thay tháº¿**: [SINGLETON_PATTERN_GUIDE.md](./SINGLETON_PATTERN_GUIDE.md) for complete implementation + FAQ.

---

**TÃ i liá»‡u nÃ y tráº£ lá»i 2 cÃ¢u há»i quan trá»ng nháº¥t vá» concurrency vÃ  singleton pattern.** *(Legacy content below)*

---

## ğŸ“‹ CÃ¢u há»i 1: LLM cÃ³ bá»‹ share context giá»¯a nhiá»u ngÆ°á»i dÃ¹ng khÃ´ng?

### âœ… CÃ¢u tráº£ lá»i ngáº¯n gá»n: KHÃ”NG

**Giáº£i thÃ­ch:**
- Má»—i request táº¡o **LangChain chain má»›i** vá»›i context Ä‘á»™c láº­p
- `ChatOpenAI` chá»‰ lÃ  **API client** - khÃ´ng lÆ°u conversation history
- OpenAI API xá»­ lÃ½ má»—i request **hoÃ n toÃ n riÃªng biá»‡t**
- KhÃ´ng cÃ³ memory/session Ä‘Æ°á»£c share giá»¯a users

### ğŸ” Chi tiáº¿t ká»¹ thuáº­t:

#### Kiáº¿n trÃºc hiá»‡n táº¡i
```
User A: "Luáº­t Ä‘áº¥u tháº§u lÃ  gÃ¬?"
    â†“
FastAPI /ask endpoint
    â†“
answer(question) â†’ Táº¡o chain Má»šI
    â†“
ChatOpenAI().invoke(messages_A) â†’ API call Ä‘á»™c láº­p
    â†“
Response A
```

```
User B: "Quy Ä‘á»‹nh vá» giÃ¡?" (CÃ™ng LÃšC)
    â†“
FastAPI /ask endpoint
    â†“
answer(question) â†’ Táº¡o chain Má»šI (KHÃC User A)
    â†“
ChatOpenAI().invoke(messages_B) â†’ API call Ä‘á»™c láº­p (KHÃC User A)
    â†“
Response B (KHÃ”NG Bá»Š áº¢NH HÆ¯á»NG bá»Ÿi User A)
```

#### Code Evidence

**File: `src/generation/chains/qa_chain.py`**
```python
# Global model - CHá»ˆ lÃ  API client, KHÃ”NG lÆ°u context
model = ChatOpenAI(model=settings.llm_model, temperature=0)

def answer(question: str, mode: str | None = None, use_enhancement: bool = True) -> Dict:
    # âœ… Táº¡o RETRIEVER Má»šI cho má»—i request
    retriever = create_retriever(mode=selected_mode, enable_reranking=enable_reranking)
    
    # âœ… Táº¡o PROMPT Má»šI vá»›i question cá»§a user hiá»‡n táº¡i
    prompt = ChatPromptTemplate.from_messages(
        [("system", system_prompt), ("user", USER_TEMPLATE)]
    )
    
    # âœ… Táº¡o CHAIN Má»šI - Context hoÃ n toÃ n Ä‘á»™c láº­p
    rag_chain = (
        {"context": retriever | fmt_docs, "question": RunnablePassthrough()}
        | prompt
        | model  # â† Reuse model NHÆ¯NG khÃ´ng share context
        | StrOutputParser()
    )
    
    # Má»—i láº§n gá»i invoke() lÃ  1 request Má»šI tá»›i OpenAI
    result = chain.invoke(question)
```

**Giáº£i thÃ­ch code:**
1. `model = ChatOpenAI()`: Singleton OK vÃ¬ chá»‰ lÃ  **stateless API client**
2. `rag_chain`: Táº¡o **má»›i má»—i request** â†’ khÃ´ng share context
3. `chain.invoke(question)`: Má»—i láº§n gá»i gá»­i **messages má»›i** tá»›i OpenAI API

#### Conversation Memory Settings

**File: `src/config/models.py`**
```python
@dataclass
class Settings:
    # âœ… Conversation memory Máº¶C Äá»ŠNH Táº®T
    enable_conversation_memory: bool = _env_bool("ENABLE_CONVERSATION_MEMORY", False)
    memory_window: int = int(os.getenv("MEMORY_WINDOW", "5"))
```

**Káº¿t luáº­n:**
- âœ… Memory feature **Táº®T máº·c Ä‘á»‹nh**
- âœ… Má»—i request lÃ  **stateless** - khÃ´ng lÆ°u lá»‹ch sá»­
- âœ… PhÃ¹ há»£p vá»›i **multi-user** - khÃ´ng bá»‹ láº«n lá»™n context

### ğŸ§ª Test Ä‘á»ƒ verify

**Báº¡n cÃ³ thá»ƒ test nhÆ° sau:**

```bash
# Terminal 1: User A
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "TÃ´i lÃ  User A, nhá»› tÃ´i nhÃ©!"}'

# Terminal 2: User B (cÃ¹ng lÃºc)
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "TÃ´i lÃ  ai?"}'
```

**Expected result:**
- Response cá»§a User B sáº½ **KHÃ”NG** nhá»› "User A"
- Má»—i response Ä‘á»™c láº­p, khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi request khÃ¡c

### ğŸ“Š So sÃ¡nh vá»›i cÃ¡c há»‡ thá»‘ng khÃ¡c

#### ChatGPT (cÃ³ memory, nhÆ°ng per-user)
```python
class ChatGPT:
    def handle_request(self, user_id, message):
        # âœ… Load history Cá»¦A USER NÃ€Y tá»« database
        history = db.get_conversation(user_id)
        
        # âœ… Generate response vá»›i history cá»§a user
        response = llm.generate(history + message)
        
        # âœ… Save vÃ o DB vá»›i user_id
        db.save_message(user_id, message, response)
```

**KhÃ¡c biá»‡t:**
- ChatGPT: LÆ°u history **per-user** trong database
- RAG-bidding: **KhÃ´ng lÆ°u history** - má»—i request Ä‘á»™c láº­p

#### RAG-bidding (stateless)
```python
def answer(question: str):
    # âŒ KHÃ”NG load history tá»« database
    # âŒ KHÃ”NG save conversation
    # âœ… Chá»‰ process question hiá»‡n táº¡i
    
    chain = create_new_chain()  # Táº¡o má»›i má»—i láº§n
    result = chain.invoke(question)
    return result
```

### âœ… Káº¾T LUáº¬N

**LLM KHÃ”NG Bá»Š SHARE CONTEXT giá»¯a users vÃ¬:**

1. âœ… **LangChain `ChatOpenAI` lÃ  stateless** - Chá»‰ lÃ  API client
2. âœ… **Má»—i request táº¡o chain má»›i** vá»›i messages Ä‘á»™c láº­p
3. âœ… **Conversation memory Táº®T** - KhÃ´ng lÆ°u history
4. âœ… **OpenAI API xá»­ lÃ½ riÃªng biá»‡t** - Má»—i request lÃ  call má»›i
5. âœ… **FastAPI stateless** - KhÃ´ng shared state giá»¯a requests

**Báº¡n cÃ³ thá»ƒ yÃªn tÃ¢m:** NgÆ°á»i dÃ¹ng A khÃ´ng thá»ƒ tháº¥y context cá»§a ngÆ°á»i dÃ¹ng B! ğŸ”’

---

## ğŸ”§ CÃ¢u há»i 2: Singleton cÃ³ thá»ƒ duy trÃ¬ lÃ¢u vÃ  má»Ÿ rá»™ng khÃ´ng?

### âœ… CÃ¢u tráº£ lá»i ngáº¯n gá»n: CÃ“ - ÄÃ¢y lÃ  industry standard

**Giáº£i thÃ­ch:**
- Singleton lÃ  **best practice** cho ML models (OpenAI, HuggingFace, Google khuyáº¿n nghá»‹)
- **Dá»… dÃ ng migrate** sang advanced patterns (DI, model pool) khi cáº§n
- **Scalable** - Multi-worker, Kubernetes deploy OK
- **Thread-safe** vá»›i proper locking

### ğŸ­ Industry Evidence

#### 1. Hugging Face (Official Docs)
```python
# âœ… RECOMMENDED: Load once, reuse
model = AutoModel.from_pretrained("BAAI/bge-reranker-v2-m3")

# âŒ BAD: Load per request (memory leak nhÆ° hiá»‡n táº¡i)
def rerank(query, docs):
    model = AutoModel.from_pretrained("BAAI/bge-reranker-v2-m3")  # SAIIII!
```

#### 2. FastAPI (Official Docs)
```python
# âœ… Load model at startup
@app.on_event("startup")
def load_models():
    global reranker
    reranker = BGEReranker()  # Singleton

# âœ… Reuse singleton
@app.post("/ask")
def ask(query: str):
    results = reranker.rerank(query, docs)
```

#### 3. Production Systems
- **OpenAI API**: Client lÃ  singleton, reuse across requests
- **Perplexity.ai**: Reranker (Cohere client) lÃ  singleton
- **Google Vertex AI**: Model endpoints lÃ  singleton instances

### ğŸ“ˆ Scalability - 3 Levels

#### Level 1: Simple Singleton (30 phÃºt implement)

**PhÃ¹ há»£p cho:** 1 worker, <50 concurrent users

```python
# src/retrieval/ranking/bge_reranker.py
_reranker_instance = None
_reranker_lock = threading.Lock()

def get_singleton_reranker():
    global _reranker_instance
    
    if _reranker_instance is None:
        with _reranker_lock:
            if _reranker_instance is None:
                _reranker_instance = BGEReranker()
    
    return _reranker_instance
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… ÄÆ¡n giáº£n, dá»… implement (30 phÃºt)
- âœ… Thread-safe vá»›i lock
- âœ… Memory: 20GB â†’ 1.5GB
- âœ… Capacity: 5 â†’ 50 users

**Khi nÃ o dÃ¹ng:** Ngay bÃ¢y giá» Ä‘á»ƒ fix urgent issue!

#### Level 2: FastAPI Dependency Injection (1 giá» implement)

**PhÃ¹ há»£p cho:** Multi-worker, <200 concurrent users

```python
# src/api/dependencies.py
from functools import lru_cache

@lru_cache()
def get_shared_reranker() -> BGEReranker:
    """Singleton per worker process"""
    return BGEReranker()

# src/api/main.py
from fastapi import Depends

@app.post("/ask")
def ask(
    body: AskIn,
    reranker: BGEReranker = Depends(get_shared_reranker)
):
    # Reuse singleton
    retriever = create_retriever(mode=body.mode, reranker=reranker)
```

**Multi-worker behavior:**
```bash
uvicorn app:app --workers 4

Worker 1: BGEReranker instance A (1.2GB)
Worker 2: BGEReranker instance B (1.2GB)
Worker 3: BGEReranker instance C (1.2GB)
Worker 4: BGEReranker instance D (1.2GB)

Total: 4.8GB (vs 20GB+ hiá»‡n táº¡i)
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Industry standard (FastAPI best practice)
- âœ… Per-worker singleton â†’ Multi-worker ready
- âœ… Testable, maintainable
- âœ… Capacity: 50 â†’ 200+ users

**Khi nÃ o dÃ¹ng:** Sau khi test Level 1, migrate trong 1 tuáº§n

#### Level 3: Model Pool (Advanced)

**PhÃ¹ há»£p cho:** >500 concurrent users, GPU constraints

```python
from queue import Queue

class RerankerPool:
    def __init__(self, pool_size=3):
        self.pool = Queue(maxsize=pool_size)
        
        # Pre-load 3 instances
        for _ in range(pool_size):
            self.pool.put(BGEReranker())
    
    def acquire(self):
        return self.pool.get()  # Block if pool empty
    
    def release(self, instance):
        self.pool.put(instance)

# Usage
pool = RerankerPool(pool_size=3)

@app.post("/ask")
def ask(body: AskIn):
    reranker = pool.acquire()
    try:
        result = reranker.rerank(query, docs)
    finally:
        pool.release(reranker)
```

**Khi nÃ o cáº§n:**
- Concurrent requests > 100
- GPU memory háº¡n cháº¿ (8GB GPU, 2GB model â†’ pool = 3)
- Latency SLA < 100ms

### ğŸš€ Migration Path (KhÃ´ng breaking changes)

```
HIá»†N Táº I: Memory leak (20GB, 5 users)
    â†“ 30 phÃºt
Level 1: Simple Singleton (1.5GB, 50 users)
    â†“ Test 1 tuáº§n
    â†“ 1 giá» migrate
Level 2: FastAPI DI (4.8GB vá»›i 4 workers, 200 users)
    â†“ Production stable 1-2 thÃ¡ng
    â†“ Khi cáº§n scale lÃªn 500+ users
Level 3: Model Pool (tuá»³ chá»‰nh theo nhu cáº§u)
```

**Äáº·c Ä‘iá»ƒm:**
- âœ… **KhÃ´ng breaking changes** - Backward compatible
- âœ… **Incremental** - Test tá»«ng step
- âœ… **Rollback dá»…** - Má»—i level Ä‘á»™c láº­p

### ğŸ¢ Production Deployment Examples

#### Scenario 1: Single Server (Current target)

```
Server: 16GB RAM, 4 CPU cores
    â†“
FastAPI (1 worker)
    â†“
Singleton Reranker (1.2GB)
    â†“
Capacity: 50 concurrent users
Latency: 100-150ms
Memory: 2GB total
```

#### Scenario 2: Multi-worker (Next step)

```
Server: 32GB RAM, 8 CPU cores
    â†“
uvicorn --workers 4
    â†“
Worker 1-4: Each has BGEReranker (1.2GB)
    â†“
Total memory: 5-6GB
Capacity: 200 concurrent users
Latency: 100ms avg
```

#### Scenario 3: Kubernetes (Future)

```
Load Balancer
    â†“
Pod 1 (2 workers): 2.4GB
Pod 2 (2 workers): 2.4GB
Pod 3 (2 workers): 2.4GB
    â†“
Auto-scaling: Add pods on demand
Total capacity: 500+ users
High availability: Pod failure â†’ reroute
```

### âœ… Káº¾T LUáº¬N

**Singleton CÃ“ THá»‚ duy trÃ¬ lÃ¢u dÃ i vÃ  má»Ÿ rá»™ng vÃ¬:**

1. âœ… **Industry standard** - OpenAI, HuggingFace, FastAPI Ä‘á»u khuyáº¿n nghá»‹
2. âœ… **Dá»… migrate** - Singleton â†’ DI â†’ Pool (khÃ´ng breaking)
3. âœ… **Scalable** - Multi-worker, Kubernetes ready
4. âœ… **Thread-safe** - Vá»›i proper locking
5. âœ… **Proven** - Production systems (Perplexity, ChatGPT) dÃ¹ng pattern nÃ y

**Roadmap rÃµ rÃ ng:**
- âœ… Báº¯t Ä‘áº§u: Simple Singleton (30 phÃºt)
- âœ… Scale: FastAPI DI (1 giá»)
- âœ… Advanced: Model Pool (khi cáº§n >100 concurrent)

---

## ğŸ¯ TÃ“M Táº®T CUá»I CÃ™NG

### CÃ¢u há»i 1: LLM share context?

âŒ **KHÃ”NG** - Má»—i request Ä‘á»™c láº­p, khÃ´ng áº£nh hÆ°á»Ÿng láº«n nhau

**Why safe:**
- ChatOpenAI lÃ  stateless API client
- Má»—i request táº¡o chain má»›i
- Conversation memory disabled
- OpenAI API xá»­ lÃ½ riÃªng biá»‡t

### CÃ¢u há»i 2: Singleton bá»n vá»¯ng?

âœ… **CÃ“** - Industry standard, dá»… scale

**Why scalable:**
- Hugging Face, FastAPI, OpenAI Ä‘á»u dÃ¹ng
- Migrate path: Singleton â†’ DI â†’ Pool
- Multi-worker ready
- Kubernetes compatible

### ğŸš€ Next Action

**URGENT (30 phÃºt):**
1. Äá»c `RERANKER_FIX_URGENT.md`
2. Apply Simple Singleton fix
3. Test vá»›i performance suite
4. Verify: Memory 20GB â†’ 1.5GB

**THEN (1 tuáº§n):**
1. Monitor production
2. Migrate to FastAPI DI
3. Test multi-worker

**FUTURE (khi cáº§n):**
1. Evaluate Model Pool
2. Consider GPU optimization
3. Scale to 500+ users

---

## ğŸ“š Related Documents

- **SINGLETON_AND_CONCURRENCY_ANALYSIS.md** - Full technical deep-dive (30 min)
- **RERANKER_FIX_URGENT.md** - Quick fix guide (3 min)
- **TOM_TAT_TIENG_VIET.md** - Vietnamese comprehensive guide (15 min)

---

**ğŸ“… Created:** November 13, 2025  
**ğŸ‘¤ Purpose:** Answer 2 critical questions about concurrency & scalability  
**ğŸ¯ Audience:** Developers, architects, managers concerned about multi-user safety
