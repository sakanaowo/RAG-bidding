# ğŸ” PhÃ¢n TÃ­ch Váº¥n Äá» Memory - BGE Reranker

**NgÃ y**: 12/11/2025  
**Váº¥n Ä‘á»**: CUDA OOM vÃ  memory leak khi cháº¡y performance tests  
**TÃ¡c Ä‘á»™ng**: 20GB+ RAM, test timeout, khÃ´ng thá»ƒ scale concurrent users

---

## ğŸ“Š TÃ³m Táº¯t Váº¥n Äá»

### Log Error
```
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. 
GPU has a total capacity of 11.62 GiB of which 57.50 MiB is free. 
Including non-PyTorch memory, this process has 10.35 GiB memory in use.
Of the allocated memory 10.23 GiB is allocated by PyTorch.

[2025-11-08 08:55:35] [INFO] src.retrieval.ranking.bge_reranker: 
Initializing reranker: BAAI/bge-reranker-v2-m3
[2025-11-08 08:55:35] [INFO] src.retrieval.ranking.bge_reranker: 
GPU detected! Using CUDA for acceleration
```

### Memory Usage Pattern
- **Má»™t phiÃªn test**: 20GB+ RAM
- **Single BGE model**: ~1.2GB (BAAI/bge-reranker-v2-m3)
- **Concurrent test**: Multiple model instances â†’ Cumulative memory

---

## ğŸ” NguyÃªn NhÃ¢n Gá»‘c Rá»…

### 1. **Reranker ÄÆ°á»£c Táº¡o Má»›i Má»—i Request**

**File**: `src/api/main.py::ask()`
```python
@app.post("/ask", response_model=AskResponse)
def ask(body: AskIn):
    from src.retrieval.retrievers import create_retriever
    
    enable_reranking = settings.enable_reranking and body.mode != "fast"
    retriever = create_retriever(mode=body.mode, enable_reranking=enable_reranking)
    # âŒ Má»–I REQUEST Táº O RETRIEVER Má»šI!
```

**File**: `src/retrieval/retrievers/__init__.py::create_retriever()`
```python
def create_retriever(mode: str = "balanced", enable_reranking: bool = True, ...):
    if enable_reranking and reranker is None:
        reranker = BGEReranker()  # âŒ LOAD MODEL Má»šI Má»–I Láº¦N!
```

**File**: `src/generation/chains/qa_chain.py::answer()`
```python
def answer(question: str, mode: str | None = None, ...):
    retriever = create_retriever(mode=selected_mode, enable_reranking=enable_reranking)
    # âŒ CÅ¨NG Táº O RETRIEVER Má»šI!
```

### 2. **Model Loading Lifecycle**

```python
# src/retrieval/ranking/bge_reranker.py
class BGEReranker(BaseReranker):
    def __init__(self, model_name: str = "BAAI/bge-reranker-v2-m3", device=None):
        # Load 1.2GB model vÃ o memory
        self.model = CrossEncoder(
            model_name,
            device=device,  # CUDA náº¿u available
            max_length=512
        )
        # âŒ KHÃ”NG CÃ“ CLEANUP MECHANISM
        # âŒ KHÃ”NG CÃ“ MODEL CACHING
```

### 3. **Performance Test Amplifies Problem**

**File**: `scripts/tests/performance/run_performance_tests.py`
```python
# Test 1: Query Latency
# 15 queries Ã— 4 modes Ã— 3 runs = 180 requests
# â†’ 180 BGEReranker instances â†’ 216GB theoretical memory!

# Test 2: Multi-user Load  
# 10 concurrent users Ã— 3 queries = 30 simultaneous requests
# â†’ 30 BGEReranker instances â†’ 36GB memory!
```

**Actual Impact** (tá»« log):
```json
{
  "users_10": {
    "query_success_rate": 0.367,  // Chá»‰ 36.7% thÃ nh cÃ´ng
    "failed_queries": 19,          // 19/30 queries failed
    "avg_response_time_ms": 9620   // Gáº¥p 2.5x so vá»›i 5 users
  },
  "breaking_point_analysis": {
    "max_stable_concurrent_users": 5,  // Chá»‰ chá»‹u Ä‘Æ°á»£c 5 users!
    "breaking_point_users": 10
  }
}
```

---

## ğŸ­ Reranking Strategies - Industry Best Practices

### Production Systems Comparison

| System | Reranker Type | Caching Strategy | Notes |
|--------|---------------|------------------|-------|
| **Perplexity.ai** | Cohere Rerank API | API-managed | Cloud-based, no local memory |
| **You.com** | Custom reranker | Singleton pattern | Model cached per worker |
| **ChatGPT** | Proprietary | Distributed cache | Multi-tier caching |
| **RAG Bidding** âŒ | BGE (local) | **None** | Táº¡o má»›i má»—i request! |

### Standard RAG Pipeline vá»›i Reranking

```python
# Industry Standard Flow:
# 1. Retrieve nhiá»u docs (k=20-50)
retriever.search(query, k=20)

# 2. Rerank vá»›i CACHED model
reranker = get_cached_reranker()  # âœ… Singleton
top_docs = reranker.rerank(query, docs, top_k=5)

# 3. Use top-k cho generation
llm.generate(query, top_docs)
```

### CÃ¡c Reranker Phá»• Biáº¿n

**Commercial APIs** (no memory management needed):
- Cohere Rerank API
- Voyage AI Rerank
- Jina Reranker API

**Open-source Models** (cáº§n cache properly):
- `BAAI/bge-reranker-v2-m3` â­ (Ä‘ang dÃ¹ng, multilingual)
- `cross-encoder/ms-marco-MiniLM-L-6-v2` (English only)
- `vinai/phobert-base-v2` (Vietnamese, chÆ°a fine-tuned)

---

## âœ… Solutions

### Solution 1: Singleton Pattern (Quick Fix)

**File**: `src/retrieval/ranking/bge_reranker.py`
```python
import threading

_reranker_instance = None
_reranker_lock = threading.Lock()

def get_singleton_reranker(
    model_name: str = "BAAI/bge-reranker-v2-m3",
    device: str = None
):
    """Get or create singleton reranker instance."""
    global _reranker_instance
    
    if _reranker_instance is None:
        with _reranker_lock:
            if _reranker_instance is None:
                _reranker_instance = BGEReranker(
                    model_name=model_name,
                    device=device
                )
                logger.info("âœ… Created singleton BGEReranker")
    
    return _reranker_instance
```

**File**: `src/retrieval/retrievers/__init__.py`
```python
def create_retriever(mode: str = "balanced", enable_reranking: bool = True, ...):
    if enable_reranking and reranker is None:
        # âœ… Use singleton instead of creating new instance
        from src.retrieval.ranking.bge_reranker import get_singleton_reranker
        reranker = get_singleton_reranker()
    
    # ... rest of code
```

**Expected Impact**:
- Memory: 20GB â†’ 1.5GB (13x reduction)
- Concurrent users: 5 â†’ 50+ (10x improvement)
- Response time: 9.6s â†’ <2s (5x faster)

### Solution 2: FastAPI Dependency Injection (Recommended)

**File**: `src/api/dependencies.py` (new file)
```python
from functools import lru_cache
from src.retrieval.ranking import BGEReranker

@lru_cache()
def get_shared_reranker() -> BGEReranker:
    """
    FastAPI dependency: Singleton reranker per worker process.
    
    Benefits:
    - Automatic lifecycle management
    - Thread-safe by default
    - Compatible with uvicorn workers
    """
    return BGEReranker()
```

**File**: `src/api/main.py`
```python
from fastapi import Depends
from .dependencies import get_shared_reranker

@app.post("/ask")
def ask(
    body: AskIn,
    reranker: BGEReranker = Depends(get_shared_reranker)  # âœ… Injected
):
    retriever = create_retriever(
        mode=body.mode,
        enable_reranking=settings.enable_reranking,
        reranker=reranker  # âœ… Pass cached instance
    )
    # ... rest of code
```

**Benefits**:
- âœ… Automatic cleanup khi worker restart
- âœ… Compatible vá»›i multi-worker deployment
- âœ… Testable (cÃ³ thá»ƒ mock dependency)

### Solution 3: Manual Memory Cleanup (Temporary)

**File**: `src/retrieval/ranking/bge_reranker.py`
```python
import gc

class BGEReranker(BaseReranker):
    def __del__(self):
        """Cleanup when instance is destroyed."""
        if hasattr(self, 'model') and self.model is not None:
            del self.model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            logger.info("ğŸ§¹ Cleaned up BGEReranker resources")
    
    def rerank(self, query: str, documents: List[Document], top_k: int = 5):
        # ... existing code ...
        
        # Clear cache after heavy operation
        if self.device == "cuda" and len(documents) > 20:
            torch.cuda.empty_cache()
        
        return doc_scores[:top_k]
```

**Limitations**:
- âš ï¸ KhÃ´ng giáº£i quyáº¿t root cause
- âš ï¸ Váº«n load model nhiá»u láº§n
- âš ï¸ Chá»‰ giáº£m memory footprint, khÃ´ng cáº£i thiá»‡n performance

---

## ğŸ¯ Khuyáº¿n Nghá»‹ Triá»ƒn Khai

### Phase 1: Immediate Fix (1-2 hours)
1. âœ… Implement Solution 1 (Singleton pattern)
2. âœ… Add manual cleanup (`__del__`)
3. âœ… Test vá»›i performance suite
4. âœ… Monitor memory usage

### Phase 2: Production-ready (1 day)
1. âœ… Migrate to Solution 2 (FastAPI DI)
2. âœ… Add health check endpoint cho reranker status
3. âœ… Implement graceful degradation (fallback to no-rerank)
4. âœ… Add metrics (reranker latency, cache hit rate)

### Phase 3: Optimization (1 week)
1. âœ… Evaluate alternative rerankers (lighter models)
2. âœ… Implement result caching (cache reranked results)
3. âœ… Add connection pooling cho DB
4. âœ… Load testing vá»›i 50+ concurrent users

---

## ğŸ“ˆ Expected Performance Improvements

| Metric | Before | After (Singleton) | After (Full Optimization) |
|--------|--------|-------------------|---------------------------|
| **Memory per test** | 20GB | 1.5GB | 1GB |
| **Concurrent users** | 5 | 30 | 50+ |
| **Avg response time** | 9.6s | 3s | <2s |
| **Query success rate** | 36.7% | 90% | 95%+ |
| **GPU utilization** | OOM crash | 80% | 85% |

---

## ğŸ”— Related Files

**Problem Files**:
- `src/api/main.py::ask()` - Creates retriever per request
- `src/retrieval/retrievers/__init__.py::create_retriever()` - Creates reranker
- `src/generation/chains/qa_chain.py::answer()` - Also creates retriever

**Solution Files** (need to create/modify):
- `src/api/dependencies.py` - New file for FastAPI dependencies
- `src/retrieval/ranking/bge_reranker.py` - Add singleton getter
- `src/config/models.py` - Add reranker cache settings

**Test Files** (verify fix):
- `scripts/tests/performance/run_performance_tests.py`
- `scripts/tests/performance/test_multi_user_queries.py`

---

## ğŸ“š References

**Industry Articles**:
- [Building Scalable RAG Systems](https://www.deepset.ai/blog/scalable-rag)
- [Reranking Best Practices](https://cohere.com/blog/rerank)
- [Memory Management in PyTorch](https://pytorch.org/docs/stable/notes/cuda.html)

**Similar Issues**:
- [Hugging Face Forum: Model Caching](https://discuss.huggingface.co/t/how-to-cache-model)
- [FastAPI: Singleton Dependencies](https://fastapi.tiangolo.com/advanced/dependencies-with-cache/)

---

**Status**: ğŸš¨ CRITICAL - Blocking production scaling  
**Priority**: P0 - Fix immediately  
**Assignee**: Development team  
**Estimated effort**: 1-2 days for complete fix
