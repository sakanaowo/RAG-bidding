# ğŸ—ï¸ PREPROCESSING PIPELINE ARCHITECTURE
## Long-term Architecture Design for RAG-Bidding System

**NgÃ y táº¡o**: 31/10/2025  
**PhiÃªn báº£n**: 2.0  
**Status**: Architecture Design - Ready for Implementation

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Architecture Overview](#architecture-overview)
2. [Core Components](#core-components)
3. [Pipeline Design](#pipeline-design)
4. [Implementation Roadmap](#implementation-roadmap)
5. [Migration Strategy](#migration-strategy)

---

## ğŸ¯ ARCHITECTURE OVERVIEW

### NguyÃªn táº¯c thiáº¿t káº¿

**1. Modular & Extensible**
- Má»—i component Ä‘á»™c láº­p, dá»… test vÃ  maintain
- Plugin architecture cho document types má»›i
- Shared base classes giáº£m code duplication

**2. Unified Schema-driven**
- Single source of truth: UnifiedLegalChunk
- Type-safe vá»›i Pydantic validation
- Backward compatible migration

**3. Pipeline as Code**
- Declarative configuration (YAML/Python)
- Version controlled
- Reproducible preprocessing

**4. Quality-first**
- Validation á»Ÿ má»i stage
- Quality metrics tracking
- Automatic error recovery

---

## ğŸ›ï¸ ARCHITECTURE LAYERS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     APPLICATION LAYER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   CLI Tool   â”‚  â”‚  API Server  â”‚  â”‚  Batch Jobs  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ORCHESTRATION LAYER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           PipelineOrchestrator                           â”‚   â”‚
â”‚  â”‚  - Pipeline selection & routing                          â”‚   â”‚
â”‚  â”‚  - Parallel execution management                         â”‚   â”‚
â”‚  â”‚  - Error handling & retry logic                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE LAYER (7 pipelines)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”‚
â”‚  â”‚ Law  â”‚ â”‚Decreeâ”‚ â”‚Circ. â”‚ â”‚Decis.â”‚ â”‚Bidd. â”‚ â”‚Reportâ”‚ â”‚Examâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â”‚
â”‚     Each pipeline extends BaseLegalPipeline                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PROCESSING LAYER (Stages)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  1. Ingestion    â†’ Load documents                        â”‚   â”‚
â”‚  â”‚  2. Extraction   â†’ Parse content & metadata              â”‚   â”‚
â”‚  â”‚  3. Validation   â†’ Schema validation                     â”‚   â”‚
â”‚  â”‚  4. Chunking     â†’ Split into semantic chunks            â”‚   â”‚
â”‚  â”‚  5. Enrichment   â†’ Add metadata, semantic tags           â”‚   â”‚
â”‚  â”‚  6. Quality      â†’ Quality checks & scoring              â”‚   â”‚
â”‚  â”‚  7. Output       â†’ Save to database/files                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPONENT LAYER (Reusable)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Document  â”‚ â”‚  Metadata  â”‚ â”‚  Chunking  â”‚ â”‚  Validation â”‚  â”‚
â”‚  â”‚  Loaders   â”‚ â”‚ Extractors â”‚ â”‚ Strategies â”‚ â”‚   Engine    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Semantic  â”‚ â”‚  Quality   â”‚ â”‚ Hierarchy  â”‚ â”‚  Relation   â”‚  â”‚
â”‚  â”‚  Enricher  â”‚ â”‚  Analyzer  â”‚ â”‚  Builder   â”‚ â”‚  Resolver   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA LAYER                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Schema   â”‚ â”‚   Vector   â”‚ â”‚   Graph    â”‚ â”‚    Cache    â”‚  â”‚
â”‚  â”‚ (Pydantic) â”‚ â”‚    Store   â”‚ â”‚     DB     â”‚ â”‚   (Redis)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ CORE COMPONENTS

### 1. Schema Layer (`src/preprocessing/schema/`)

```python
# src/preprocessing/schema/__init__.py
from .unified_schema import (
    UnifiedLegalChunk,
    DocumentInfo,
    LegalMetadata,
    ContentStructure,
    Relationships,
    ProcessingMetadata,
    QualityMetrics,
    ExtendedMetadata
)
from .enums import DocType, LegalLevel, LegalStatus, RelationshipType
from .validators import SchemaValidator, ValidationResult

# src/preprocessing/schema/unified_schema.py
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any
from datetime import datetime

class UnifiedLegalChunk(BaseModel):
    """
    Unified schema for all 7 document types in Vietnamese legal system.
    
    Version: 2.0
    Compatible with: Law, Decree, Circular, Decision, Bidding, Report, Exam
    """
    
    document_info: DocumentInfo
    legal_metadata: LegalMetadata
    content_structure: ContentStructure
    relationships: Relationships
    processing_metadata: ProcessingMetadata
    quality_metrics: QualityMetrics
    extended_metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        validate_assignment = True
        extra = 'forbid'
        schema_extra = {
            "example": {
                "document_info": {
                    "doc_id": "law_43_2013_qh13_dieu_5_khoan_2_chunk_001",
                    "title": "Luáº­t Äáº¥u tháº§u",
                    "doc_type": "law",
                    "legal_code": "43/2013/QH13",
                    "source_file": "Luat_Dau_thau_43_2013.docx",
                    "source_url": "https://..."
                }
            }
        }
```

### 2. Base Pipeline (`src/preprocessing/base/`)

```python
# src/preprocessing/base/legal_pipeline.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from pathlib import Path
import logging

class BaseLegalPipeline(ABC):
    """
    Abstract base class for all document preprocessing pipelines.
    
    All 7 pipelines (Law, Decree, Circular, Decision, Bidding, Report, Exam)
    inherit from this base class.
    """
    
    def __init__(
        self,
        config: PipelineConfig,
        validator: SchemaValidator,
        logger: Optional[logging.Logger] = None
    ):
        self.config = config
        self.validator = validator
        self.logger = logger or logging.getLogger(self.__class__.__name__)
        
        # Components
        self.loader = self._init_loader()
        self.extractor = self._init_extractor()
        self.chunker = self._init_chunker()
        self.enricher = self._init_enricher()
        self.quality_analyzer = self._init_quality_analyzer()
        
    # ========================================
    # MAIN PIPELINE EXECUTION
    # ========================================
    
    def process(self, file_path: Path) -> PipelineResult:
        """
        Main pipeline execution method.
        
        Stages:
        1. Ingestion: Load document
        2. Extraction: Extract content & metadata
        3. Validation: Validate against schema
        4. Chunking: Split into chunks
        5. Enrichment: Add semantic metadata
        6. Quality: Quality checks
        7. Output: Save results
        """
        self.logger.info(f"Processing: {file_path}")
        
        try:
            # Stage 1: Ingestion
            document = self._stage_ingest(file_path)
            
            # Stage 2: Extraction
            raw_metadata, content = self._stage_extract(document)
            
            # Stage 3: Validation
            validated_metadata = self._stage_validate(raw_metadata)
            
            # Stage 4: Chunking
            chunks = self._stage_chunk(content, validated_metadata)
            
            # Stage 5: Enrichment
            enriched_chunks = self._stage_enrich(chunks)
            
            # Stage 6: Quality
            quality_checked = self._stage_quality_check(enriched_chunks)
            
            # Stage 7: Output
            result = self._stage_output(quality_checked)
            
            self.logger.info(f"Success: {len(quality_checked)} chunks created")
            return PipelineResult(
                success=True,
                chunks=quality_checked,
                metadata=result,
                errors=[]
            )
            
        except Exception as e:
            self.logger.error(f"Pipeline failed: {str(e)}")
            return PipelineResult(
                success=False,
                chunks=[],
                metadata={},
                errors=[str(e)]
            )
    
    # ========================================
    # PIPELINE STAGES (Template Method Pattern)
    # ========================================
    
    def _stage_ingest(self, file_path: Path) -> Document:
        """Stage 1: Load document from file"""
        self.logger.debug("Stage 1: Ingestion")
        return self.loader.load(file_path)
    
    def _stage_extract(self, document: Document) -> tuple:
        """Stage 2: Extract metadata and content"""
        self.logger.debug("Stage 2: Extraction")
        metadata = self.extract_metadata(document)
        content = self.extract_content(document)
        return metadata, content
    
    def _stage_validate(self, metadata: Dict) -> Dict:
        """Stage 3: Validate metadata against schema"""
        self.logger.debug("Stage 3: Validation")
        validation_result = self.validator.validate(
            metadata, 
            doc_type=self.get_doc_type()
        )
        
        if not validation_result.is_valid:
            raise ValidationError(validation_result.errors)
        
        return validation_result.validated_data
    
    def _stage_chunk(self, content: str, metadata: Dict) -> List[Chunk]:
        """Stage 4: Split content into semantic chunks"""
        self.logger.debug("Stage 4: Chunking")
        return self.chunker.chunk(content, metadata)
    
    def _stage_enrich(self, chunks: List[Chunk]) -> List[UnifiedLegalChunk]:
        """Stage 5: Enrich chunks with semantic metadata"""
        self.logger.debug("Stage 5: Enrichment")
        enriched = []
        for chunk in chunks:
            enriched_chunk = self.enricher.enrich(chunk)
            enriched.append(enriched_chunk)
        return enriched
    
    def _stage_quality_check(
        self, 
        chunks: List[UnifiedLegalChunk]
    ) -> List[UnifiedLegalChunk]:
        """Stage 6: Quality analysis and scoring"""
        self.logger.debug("Stage 6: Quality Check")
        for chunk in chunks:
            quality_score = self.quality_analyzer.analyze(chunk)
            chunk.quality_metrics.completeness_score = quality_score.completeness
            chunk.quality_metrics.confidence_score = quality_score.confidence
        return chunks
    
    def _stage_output(self, chunks: List[UnifiedLegalChunk]) -> Dict:
        """Stage 7: Save to database/files"""
        self.logger.debug("Stage 7: Output")
        return self.save_chunks(chunks)
    
    # ========================================
    # ABSTRACT METHODS (Must be implemented by subclasses)
    # ========================================
    
    @abstractmethod
    def extract_metadata(self, document: Document) -> Dict:
        """Extract document-specific metadata"""
        pass
    
    @abstractmethod
    def extract_content(self, document: Document) -> str:
        """Extract text content from document"""
        pass
    
    @abstractmethod
    def get_doc_type(self) -> DocType:
        """Return the document type for this pipeline"""
        pass
    
    @abstractmethod
    def get_extended_metadata(self, document: Document) -> Dict:
        """Extract extended metadata specific to document type"""
        pass
    
    # ========================================
    # COMPONENT INITIALIZATION (Override if needed)
    # ========================================
    
    def _init_loader(self) -> DocumentLoader:
        """Initialize document loader"""
        return DocxLoader()  # Default, override for PDF/HTML
    
    def _init_extractor(self) -> MetadataExtractor:
        """Initialize metadata extractor"""
        return MetadataExtractor()
    
    def _init_chunker(self) -> ChunkingStrategy:
        """Initialize chunking strategy"""
        return HierarchicalChunker(
            max_tokens=self.config.chunk_size,
            overlap=self.config.chunk_overlap
        )
    
    def _init_enricher(self) -> SemanticEnricher:
        """Initialize semantic enricher"""
        return SemanticEnricher()
    
    def _init_quality_analyzer(self) -> QualityAnalyzer:
        """Initialize quality analyzer"""
        return QualityAnalyzer()
    
    # ========================================
    # HELPER METHODS
    # ========================================
    
    def save_chunks(self, chunks: List[UnifiedLegalChunk]) -> Dict:
        """Save chunks to database and/or files"""
        # TODO: Implement database save
        output_path = self.config.output_dir / f"{chunks[0].document_info.doc_id}.jsonl"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for chunk in chunks:
                f.write(chunk.json() + '\n')
        
        return {
            "output_path": str(output_path),
            "chunk_count": len(chunks),
            "avg_quality": sum(c.quality_metrics.completeness_score for c in chunks) / len(chunks)
        }
```

### 3. Document Type Pipelines (`src/preprocessing/pipelines/`)

```python
# src/preprocessing/pipelines/law_pipeline.py
from ..base import BaseLegalPipeline
from ..schema import DocType, UnifiedLegalChunk

class LawPipeline(BaseLegalPipeline):
    """
    Pipeline for processing Vietnamese Laws (Luáº­t).
    
    Hierarchy: Pháº§n â†’ ChÆ°Æ¡ng â†’ Má»¥c â†’ Äiá»u â†’ Khoáº£n â†’ Äiá»ƒm
    Issuing body: Quá»‘c há»™i
    Legal level: 2 (highest in system)
    """
    
    def get_doc_type(self) -> DocType:
        return DocType.LAW
    
    def extract_metadata(self, document: Document) -> Dict:
        """
        Extract Law-specific metadata.
        
        Required fields:
        - law_number: e.g., "43/2013/QH13"
        - law_name: e.g., "Luáº­t Äáº¥u tháº§u"
        - promulgation_date
        - effective_date
        - issuing_body: "Quá»‘c há»™i"
        """
        metadata = {
            "document_info": {
                "doc_type": "law",
                "legal_code": self._extract_law_number(document),
                "title": self._extract_law_name(document),
                "source_file": document.file_path,
            },
            "legal_metadata": {
                "issued_by": "Quá»‘c há»™i",
                "issuing_level": 2,
                "promulgation_date": self._extract_promulgation_date(document),
                "effective_date": self._extract_effective_date(document),
                "status": self._determine_status(document),
            }
        }
        
        # Add extended metadata
        metadata["extended_metadata"] = {
            "law": self.get_extended_metadata(document)
        }
        
        return metadata
    
    def get_extended_metadata(self, document: Document) -> Dict:
        """
        Extract Law-specific extended metadata.
        """
        return {
            "law_name": self._extract_law_name(document),
            "law_category": self._extract_category(document),
            "session": self._extract_session_info(document),
            "vote_result": self._extract_vote_result(document),
            "subject_area": self._extract_subject_areas(document),
        }
    
    def extract_content(self, document: Document) -> str:
        """Extract text content with hierarchy preservation"""
        return self._extract_hierarchical_content(document)
    
    def _init_chunker(self) -> ChunkingStrategy:
        """Use hierarchical chunking for laws"""
        return HierarchicalChunker(
            max_tokens=512,
            overlap=50,
            hierarchy_levels=["Pháº§n", "ChÆ°Æ¡ng", "Má»¥c", "Äiá»u", "Khoáº£n", "Äiá»ƒm"]
        )
    
    # ========================================
    # PRIVATE HELPER METHODS
    # ========================================
    
    def _extract_law_number(self, document: Document) -> str:
        """Extract law number using regex: XX/YYYY/QHXX"""
        import re
        pattern = r'(\d+/\d{4}/QH\d+)'
        match = re.search(pattern, document.text)
        if match:
            return match.group(1)
        raise ValueError("Cannot extract law number")
    
    def _extract_law_name(self, document: Document) -> str:
        """Extract law name (usually in title)"""
        # Implementation here
        pass
    
    def _extract_promulgation_date(self, document: Document) -> str:
        """Extract promulgation date in ISO format"""
        # Implementation here
        pass
    
    def _extract_hierarchical_content(self, document: Document) -> str:
        """
        Extract content while preserving hierarchy markers.
        
        Example:
        PHáº¦N THá»¨ HAI
        CHÆ¯Æ NG II
        Äiá»u 15. ...
        """
        # Implementation here
        pass


# src/preprocessing/pipelines/decree_pipeline.py
class DecreePipeline(BaseLegalPipeline):
    """Pipeline for Nghá»‹ Ä‘á»‹nh (Government Decrees)"""
    
    def get_doc_type(self) -> DocType:
        return DocType.DECREE
    
    def extract_metadata(self, document: Document) -> Dict:
        metadata = {
            "document_info": {
                "doc_type": "decree",
                "legal_code": self._extract_decree_number(document),
                "title": self._extract_decree_name(document),
            },
            "legal_metadata": {
                "issued_by": "ChÃ­nh phá»§",
                "issuing_level": 3,
                "signer": self._extract_signer(document),
            }
        }
        
        # Add implements relationship
        metadata["relationships"] = {
            "implements": self._extract_parent_law(document)
        }
        
        return metadata


# src/preprocessing/pipelines/circular_pipeline.py
class CircularPipeline(BaseLegalPipeline):
    """Pipeline for ThÃ´ng tÆ° (Circulars)"""
    
    def get_doc_type(self) -> DocType:
        return DocType.CIRCULAR


# src/preprocessing/pipelines/decision_pipeline.py
class DecisionPipeline(BaseLegalPipeline):
    """Pipeline for Quyáº¿t Ä‘á»‹nh (Decisions) - NEW"""
    
    def get_doc_type(self) -> DocType:
        return DocType.DECISION
    
    def extract_metadata(self, document: Document) -> Dict:
        metadata = {
            "document_info": {
                "doc_type": "decision",
                "legal_code": self._extract_decision_number(document),
            },
            "legal_metadata": {
                "issued_by": self._extract_issuing_authority(document),
                "issuing_level": 4,
            }
        }
        
        metadata["extended_metadata"] = {
            "decision": {
                "decision_type": self._classify_decision_type(document),
                "subject_matter": self._extract_subject_matter(document),
                "execution_deadline": self._extract_deadline(document),
                "legal_basis": self._extract_legal_basis(document),
            }
        }
        
        return metadata


# src/preprocessing/pipelines/bidding_pipeline.py
class BiddingPipeline(BaseLegalPipeline):
    """Pipeline for Bidding Templates"""
    
    def get_doc_type(self) -> DocType:
        return DocType.BIDDING
    
    def extract_metadata(self, document: Document) -> Dict:
        metadata = {
            "document_info": {
                "doc_type": "bidding_template",
                "legal_code": None,  # No legal code for templates
            },
            "legal_metadata": {
                "issued_by": "Bá»™ Káº¿ hoáº¡ch vÃ  Äáº§u tÆ°",
                "issuing_level": 5,
                "status": "active",
            }
        }
        
        metadata["extended_metadata"] = {
            "bidding": {
                "template_type": self._classify_template_type(document),
                "template_code": self._extract_template_code(document),
                "procurement_method": self._extract_procurement_method(document),
            }
        }
        
        return metadata


# src/preprocessing/pipelines/report_pipeline.py
class ReportPipeline(BaseLegalPipeline):
    """Pipeline for Report Templates - NEW"""
    
    def get_doc_type(self) -> DocType:
        return DocType.REPORT


# src/preprocessing/pipelines/exam_pipeline.py
class ExamPipeline(BaseLegalPipeline):
    """Pipeline for Exam Questions - NEW"""
    
    def get_doc_type(self) -> DocType:
        return DocType.EXAM
```

---

## ğŸ”Œ REUSABLE COMPONENTS

### 1. Document Loaders (`src/preprocessing/loaders/`)

```python
# src/preprocessing/loaders/base.py
from abc import ABC, abstractmethod
from pathlib import Path

class DocumentLoader(ABC):
    """Base class for document loaders"""
    
    @abstractmethod
    def load(self, file_path: Path) -> Document:
        """Load document from file"""
        pass
    
    @abstractmethod
    def supports(self, file_path: Path) -> bool:
        """Check if this loader supports the file type"""
        pass


# src/preprocessing/loaders/docx_loader.py
from docx import Document as DocxDocument

class DocxLoader(DocumentLoader):
    """Loader for .docx files"""
    
    def load(self, file_path: Path) -> Document:
        docx = DocxDocument(file_path)
        
        # Extract text with formatting
        text = self._extract_text_with_hierarchy(docx)
        
        # Extract tables
        tables = self._extract_tables(docx)
        
        # Extract metadata
        metadata = self._extract_docx_metadata(docx)
        
        return Document(
            file_path=file_path,
            text=text,
            tables=tables,
            metadata=metadata,
            format="docx"
        )
    
    def supports(self, file_path: Path) -> bool:
        return file_path.suffix.lower() == '.docx'
    
    def _extract_text_with_hierarchy(self, docx: DocxDocument) -> str:
        """
        Extract text while preserving hierarchy markers.
        
        Identifies:
        - Pháº§n (by style or CAPS)
        - ChÆ°Æ¡ng (by style or pattern)
        - Äiá»u (by regex: "Äiá»u \d+")
        - Khoáº£n (by numbering)
        """
        text_parts = []
        
        for para in docx.paragraphs:
            # Check if this is a hierarchy marker
            if self._is_phan(para):
                text_parts.append(f"\n\n[PHAN] {para.text}\n")
            elif self._is_chuong(para):
                text_parts.append(f"\n[CHUONG] {para.text}\n")
            elif self._is_dieu(para):
                text_parts.append(f"\n[DIEU] {para.text}\n")
            else:
                text_parts.append(para.text)
        
        return '\n'.join(text_parts)


# src/preprocessing/loaders/pdf_loader.py
import fitz  # PyMuPDF

class PdfLoader(DocumentLoader):
    """Loader for PDF files"""
    
    def load(self, file_path: Path) -> Document:
        pdf = fitz.open(file_path)
        
        text = ""
        for page in pdf:
            text += page.get_text()
        
        return Document(
            file_path=file_path,
            text=text,
            format="pdf"
        )
    
    def supports(self, file_path: Path) -> bool:
        return file_path.suffix.lower() == '.pdf'
```

### 2. Chunking Strategies (`src/chunking/strategies/`)

```python
# src/chunking/strategies/hierarchical_chunker.py
from typing import List
import re

class HierarchicalChunker(ChunkingStrategy):
    """
    Chunk documents based on Vietnamese legal hierarchy.
    
    Hierarchy levels:
    0. Pháº§n (Part)
    1. ChÆ°Æ¡ng (Chapter)
    2. Má»¥c (Section)
    3. Äiá»u (Article)
    4. Khoáº£n (Paragraph)
    5. Äiá»ƒm (Point)
    """
    
    def __init__(
        self,
        max_tokens: int = 512,
        overlap: int = 50,
        hierarchy_levels: List[str] = None
    ):
        self.max_tokens = max_tokens
        self.overlap = overlap
        self.hierarchy_levels = hierarchy_levels or [
            "Pháº§n", "ChÆ°Æ¡ng", "Má»¥c", "Äiá»u", "Khoáº£n", "Äiá»ƒm"
        ]
    
    def chunk(self, content: str, metadata: Dict) -> List[Chunk]:
        """
        Split content into chunks based on hierarchy.
        
        Strategy:
        1. Split by Äiá»u (Article) first
        2. If Äiá»u > max_tokens, split by Khoáº£n (Paragraph)
        3. If Khoáº£n > max_tokens, split by sentences
        4. Maintain hierarchy context in each chunk
        """
        chunks = []
        
        # Parse hierarchy
        hierarchy = self._parse_hierarchy(content)
        
        # Split by articles
        articles = self._split_by_pattern(content, r'Äiá»u \d+\.')
        
        for article in articles:
            article_chunks = self._chunk_article(article, hierarchy)
            chunks.extend(article_chunks)
        
        return chunks
    
    def _parse_hierarchy(self, content: str) -> Dict:
        """
        Extract hierarchy structure from content.
        
        Returns:
        {
            "phan": "PHáº¦N THá»¨ HAI",
            "chuong": "CHÆ¯Æ NG II",
            "muc": "Má»¥c 1",
            "dieu": "Äiá»u 15"
        }
        """
        hierarchy = {}
        
        # Find Pháº§n
        phan_match = re.search(r'PHáº¦N (THá»¨ )?\w+', content)
        if phan_match:
            hierarchy["phan"] = phan_match.group(0)
        
        # Find ChÆ°Æ¡ng
        chuong_match = re.search(r'CHÆ¯Æ NG [IVXLC]+', content)
        if chuong_match:
            hierarchy["chuong"] = chuong_match.group(0)
        
        # Find Má»¥c
        muc_match = re.search(r'Má»¥c \d+', content)
        if muc_match:
            hierarchy["muc"] = muc_match.group(0)
        
        return hierarchy
    
    def _chunk_article(self, article: str, hierarchy: Dict) -> List[Chunk]:
        """Chunk a single article"""
        chunks = []
        
        # Extract article number
        article_match = re.match(r'Äiá»u (\d+)\.', article)
        article_num = article_match.group(1) if article_match else "0"
        
        # Split by paragraphs (Khoáº£n)
        paragraphs = self._split_by_pattern(article, r'\d+\.')
        
        for idx, para in enumerate(paragraphs):
            chunk = Chunk(
                text=para,
                hierarchy_path=[
                    hierarchy.get("phan", ""),
                    hierarchy.get("chuong", ""),
                    hierarchy.get("muc", ""),
                    f"Äiá»u {article_num}",
                    f"Khoáº£n {idx + 1}"
                ],
                hierarchy_level=4,
                metadata={
                    "article": f"Äiá»u {article_num}",
                    "paragraph": f"Khoáº£n {idx + 1}"
                }
            )
            chunks.append(chunk)
        
        return chunks


# src/chunking/strategies/semantic_chunker.py
class SemanticChunker(ChunkingStrategy):
    """
    Chunk based on semantic similarity.
    
    Uses sentence embeddings to group semantically related sentences.
    """
    
    def __init__(self, embedding_model, similarity_threshold: float = 0.7):
        self.embedding_model = embedding_model
        self.similarity_threshold = similarity_threshold
    
    def chunk(self, content: str, metadata: Dict) -> List[Chunk]:
        # Split into sentences
        sentences = self._split_sentences(content)
        
        # Compute embeddings
        embeddings = self.embedding_model.encode(sentences)
        
        # Group by similarity
        chunks = self._group_by_similarity(sentences, embeddings)
        
        return chunks
```

### 3. Metadata Enrichment (`src/preprocessing/enrichment/`)

```python
# src/preprocessing/enrichment/semantic_enricher.py
class SemanticEnricher:
    """
    Add semantic metadata to chunks.
    
    Features:
    - Named entity extraction
    - Legal concept tagging
    - Keyword extraction
    - Subject area classification
    """
    
    def __init__(self):
        self.ner_model = self._load_ner_model()
        self.concept_extractor = LegalConceptExtractor()
        self.keyword_extractor = KeywordExtractor()
    
    def enrich(self, chunk: Chunk) -> UnifiedLegalChunk:
        """Add semantic metadata to chunk"""
        
        # Extract named entities
        entities = self.ner_model.extract(chunk.text)
        
        # Extract legal concepts
        concepts = self.concept_extractor.extract(chunk.text)
        
        # Extract keywords
        keywords = self.keyword_extractor.extract(chunk.text)
        
        # Add to chunk metadata
        chunk.semantic_metadata = {
            "named_entities": entities,
            "legal_concepts": concepts,
            "keywords": keywords,
            "subject_areas": self._classify_subject_areas(concepts)
        }
        
        return chunk


# src/preprocessing/enrichment/relation_resolver.py
class RelationResolver:
    """
    Resolve relationships between documents.
    
    Features:
    - Find parent laws
    - Find superseded documents
    - Find related documents
    - Build citation graph
    """
    
    def __init__(self, document_database):
        self.db = document_database
    
    def resolve_relationships(
        self, 
        chunk: UnifiedLegalChunk
    ) -> UnifiedLegalChunk:
        """
        Resolve document relationships.
        
        For Decrees/Circulars:
        - Find parent law (implements)
        
        For all:
        - Find superseded documents
        - Find related documents via citations
        """
        
        # Find parent law
        if chunk.document_info.doc_type in ["decree", "circular"]:
            parent_law = self._find_parent_law(chunk)
            chunk.relationships.implements = parent_law
        
        # Find superseded documents
        superseded = self._find_superseded_documents(chunk)
        chunk.relationships.supersedes = superseded
        
        # Extract citations
        citations = self._extract_citations(chunk.text)
        chunk.relationships.related_documents = citations
        
        return chunk
```

### 4. Quality Analysis (`src/preprocessing/quality/`)

```python
# src/preprocessing/quality/quality_analyzer.py
class QualityAnalyzer:
    """
    Analyze chunk quality.
    
    Metrics:
    - Completeness: All required metadata present
    - Confidence: Extraction confidence
    - Consistency: Cross-field validation
    - Readability: Text quality
    """
    
    def analyze(self, chunk: UnifiedLegalChunk) -> QualityScore:
        scores = {
            "completeness": self._check_completeness(chunk),
            "confidence": self._check_confidence(chunk),
            "consistency": self._check_consistency(chunk),
            "readability": self._check_readability(chunk)
        }
        
        return QualityScore(
            overall=sum(scores.values()) / len(scores),
            **scores
        )
    
    def _check_completeness(self, chunk: UnifiedLegalChunk) -> float:
        """Check if all required fields are present"""
        required_fields = [
            "document_info.doc_id",
            "document_info.title",
            "legal_metadata.issued_by",
            "legal_metadata.effective_date",
            "content_structure.hierarchy_path"
        ]
        
        present = 0
        for field_path in required_fields:
            if self._has_field(chunk, field_path):
                present += 1
        
        return present / len(required_fields)
    
    def _check_confidence(self, chunk: UnifiedLegalChunk) -> float:
        """
        Check extraction confidence.
        
        Factors:
        - Regex match confidence
        - NER model confidence
        - Validation passed
        """
        confidence_scores = []
        
        # Legal code format validation
        if chunk.legal_metadata.legal_code:
            if re.match(r'^\d+/\d{4}/[A-ZÄ\-]+$', chunk.legal_metadata.legal_code):
                confidence_scores.append(1.0)
            else:
                confidence_scores.append(0.5)
        
        # Date format validation
        if chunk.legal_metadata.effective_date:
            if re.match(r'^\d{4}-\d{2}-\d{2}$', chunk.legal_metadata.effective_date):
                confidence_scores.append(1.0)
            else:
                confidence_scores.append(0.5)
        
        # Validation passed
        if chunk.quality_metrics.validation_passed:
            confidence_scores.append(1.0)
        else:
            confidence_scores.append(0.0)
        
        return sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
```

---

## ğŸ›ï¸ ORCHESTRATION

### Pipeline Orchestrator (`src/preprocessing/orchestrator.py`)

```python
class PipelineOrchestrator:
    """
    Orchestrate multiple preprocessing pipelines.
    
    Features:
    - Auto-detect document type
    - Route to appropriate pipeline
    - Parallel processing
    - Error recovery
    - Progress tracking
    """
    
    def __init__(self, config: OrchestratorConfig):
        self.config = config
        self.pipelines = self._init_pipelines()
        self.logger = logging.getLogger("Orchestrator")
    
    def _init_pipelines(self) -> Dict[DocType, BaseLegalPipeline]:
        """Initialize all 7 pipelines"""
        return {
            DocType.LAW: LawPipeline(self.config.law_config),
            DocType.DECREE: DecreePipeline(self.config.decree_config),
            DocType.CIRCULAR: CircularPipeline(self.config.circular_config),
            DocType.DECISION: DecisionPipeline(self.config.decision_config),
            DocType.BIDDING: BiddingPipeline(self.config.bidding_config),
            DocType.REPORT: ReportPipeline(self.config.report_config),
            DocType.EXAM: ExamPipeline(self.config.exam_config),
        }
    
    def process_batch(
        self, 
        file_paths: List[Path],
        parallel: bool = True
    ) -> BatchResult:
        """
        Process a batch of documents.
        
        Args:
            file_paths: List of document paths
            parallel: Enable parallel processing
        
        Returns:
            BatchResult with success/failure counts
        """
        self.logger.info(f"Processing {len(file_paths)} documents")
        
        if parallel:
            results = self._process_parallel(file_paths)
        else:
            results = self._process_sequential(file_paths)
        
        return BatchResult(
            total=len(file_paths),
            success=sum(1 for r in results if r.success),
            failed=sum(1 for r in results if not r.success),
            results=results
        )
    
    def _process_parallel(self, file_paths: List[Path]) -> List[PipelineResult]:
        """Process documents in parallel using multiprocessing"""
        from multiprocessing import Pool
        
        with Pool(processes=self.config.num_workers) as pool:
            results = pool.map(self._process_single, file_paths)
        
        return results
    
    def _process_single(self, file_path: Path) -> PipelineResult:
        """Process a single document"""
        try:
            # Auto-detect document type
            doc_type = self._detect_doc_type(file_path)
            
            # Get appropriate pipeline
            pipeline = self.pipelines[doc_type]
            
            # Process
            result = pipeline.process(file_path)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Failed to process {file_path}: {str(e)}")
            return PipelineResult(
                success=False,
                chunks=[],
                metadata={},
                errors=[str(e)]
            )
    
    def _detect_doc_type(self, file_path: Path) -> DocType:
        """
        Auto-detect document type from filename or content.
        
        Rules:
        - Path contains "Luat chinh" â†’ LAW
        - Path contains "Nghi dinh" â†’ DECREE
        - Path contains "Thong tu" â†’ CIRCULAR
        - Path contains "Quyet dinh" â†’ DECISION
        - Path contains "Ho so moi thau" â†’ BIDDING
        - Path contains "Mau bao cao" â†’ REPORT
        - Path contains "Cau hoi thi" â†’ EXAM
        """
        path_str = str(file_path).lower()
        
        if "luat" in path_str or "law" in path_str:
            return DocType.LAW
        elif "nghi dinh" in path_str or "decree" in path_str:
            return DocType.DECREE
        elif "thong tu" in path_str or "circular" in path_str:
            return DocType.CIRCULAR
        elif "quyet dinh" in path_str or "decision" in path_str:
            return DocType.DECISION
        elif "ho so" in path_str or "bidding" in path_str:
            return DocType.BIDDING
        elif "mau bao cao" in path_str or "report" in path_str:
            return DocType.REPORT
        elif "cau hoi" in path_str or "exam" in path_str:
            return DocType.EXAM
        
        # Default: try to detect from content
        return self._detect_from_content(file_path)
```

---

## ğŸ“ DIRECTORY STRUCTURE

```
src/preprocessing/
â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ schema/                          # Schema definitions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unified_schema.py           # UnifiedLegalChunk
â”‚   â”œâ”€â”€ enums.py                    # DocType, LegalStatus, etc.
â”‚   â”œâ”€â”€ validators.py               # SchemaValidator
â”‚   â””â”€â”€ models/                     # Pydantic models
â”‚       â”œâ”€â”€ document_info.py
â”‚       â”œâ”€â”€ legal_metadata.py
â”‚       â”œâ”€â”€ content_structure.py
â”‚       â”œâ”€â”€ relationships.py
â”‚       â”œâ”€â”€ processing_metadata.py
â”‚       â”œâ”€â”€ quality_metrics.py
â”‚       â””â”€â”€ extended/               # Extended metadata models
â”‚           â”œâ”€â”€ law.py
â”‚           â”œâ”€â”€ decree.py
â”‚           â”œâ”€â”€ circular.py
â”‚           â”œâ”€â”€ decision.py
â”‚           â”œâ”€â”€ bidding.py
â”‚           â”œâ”€â”€ report.py
â”‚           â””â”€â”€ exam.py
â”‚
â”œâ”€â”€ base/                           # Base classes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ legal_pipeline.py          # BaseLegalPipeline
â”‚   â”œâ”€â”€ document.py                # Document class
â”‚   â”œâ”€â”€ chunk.py                   # Chunk class
â”‚   â””â”€â”€ config.py                  # PipelineConfig
â”‚
â”œâ”€â”€ pipelines/                      # Document-specific pipelines
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ law_pipeline.py            # LawPipeline
â”‚   â”œâ”€â”€ decree_pipeline.py         # DecreePipeline
â”‚   â”œâ”€â”€ circular_pipeline.py       # CircularPipeline
â”‚   â”œâ”€â”€ decision_pipeline.py       # DecisionPipeline (NEW)
â”‚   â”œâ”€â”€ bidding_pipeline.py        # BiddingPipeline
â”‚   â”œâ”€â”€ report_pipeline.py         # ReportPipeline (NEW)
â”‚   â””â”€â”€ exam_pipeline.py           # ExamPipeline (NEW)
â”‚
â”œâ”€â”€ loaders/                        # Document loaders
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                    # DocumentLoader (ABC)
â”‚   â”œâ”€â”€ docx_loader.py             # DocxLoader
â”‚   â”œâ”€â”€ pdf_loader.py              # PdfLoader
â”‚   â”œâ”€â”€ html_loader.py             # HtmlLoader
â”‚   â””â”€â”€ excel_loader.py            # ExcelLoader (for exam questions)
â”‚
â”œâ”€â”€ extractors/                     # Metadata extractors
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metadata_extractor.py      # Base metadata extraction
â”‚   â”œâ”€â”€ legal_code_extractor.py    # Extract legal codes
â”‚   â”œâ”€â”€ date_extractor.py          # Extract dates
â”‚   â”œâ”€â”€ hierarchy_extractor.py     # Extract hierarchy
â”‚   â””â”€â”€ table_extractor.py         # Extract tables
â”‚
â”œâ”€â”€ chunking/                       # Chunking strategies
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hierarchical_chunker.py    # HierarchicalChunker
â”‚   â”œâ”€â”€ semantic_chunker.py        # SemanticChunker
â”‚   â”œâ”€â”€ fixed_size_chunker.py      # FixedSizeChunker
â”‚   â””â”€â”€ custom_chunker.py          # CustomChunker
â”‚
â”œâ”€â”€ enrichment/                     # Metadata enrichment
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ semantic_enricher.py       # SemanticEnricher
â”‚   â”œâ”€â”€ relation_resolver.py       # RelationResolver
â”‚   â”œâ”€â”€ concept_extractor.py       # LegalConceptExtractor
â”‚   â”œâ”€â”€ keyword_extractor.py       # KeywordExtractor
â”‚   â””â”€â”€ ner/                       # Named Entity Recognition
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ legal_ner.py           # Legal NER model
â”‚       â””â”€â”€ entity_linker.py       # Entity linking
â”‚
â”œâ”€â”€ quality/                        # Quality analysis
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ quality_analyzer.py        # QualityAnalyzer
â”‚   â”œâ”€â”€ validators/                # Specific validators
â”‚   â”‚   â”œâ”€â”€ date_validator.py
â”‚   â”‚   â”œâ”€â”€ code_validator.py
â”‚   â”‚   â””â”€â”€ hierarchy_validator.py
â”‚   â””â”€â”€ metrics/                   # Quality metrics
â”‚       â”œâ”€â”€ completeness.py
â”‚       â”œâ”€â”€ confidence.py
â”‚       â””â”€â”€ consistency.py
â”‚
â”œâ”€â”€ orchestrator.py                 # PipelineOrchestrator
â”œâ”€â”€ cli.py                         # CLI interface
â””â”€â”€ utils/                         # Utilities
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ text_utils.py              # Text processing utils
    â”œâ”€â”€ regex_patterns.py          # Vietnamese legal regex
    â””â”€â”€ file_utils.py              # File handling utils

```

---

## ğŸš€ IMPLEMENTATION ROADMAP

### Phase 1: Foundation (Week 1-2)
```python
Week 1:
â–¡ Setup schema module
  â–¡ Create unified_schema.py with all 21 core fields
  â–¡ Create enums.py with Vietnamese legal enums
  â–¡ Create Pydantic models for all 7 document types
  â–¡ Unit tests for schema validation

Week 2:
â–¡ Setup base module
  â–¡ Create BaseLegalPipeline with 7 stages
  â–¡ Create Document and Chunk classes
  â–¡ Create PipelineConfig
  â–¡ Integration tests for base pipeline
```

### Phase 2: Core Components (Week 3-4)
```python
Week 3:
â–¡ Implement loaders
  â–¡ DocxLoader with hierarchy preservation
  â–¡ PdfLoader with OCR support
  â–¡ ExcelLoader for exam questions
  
Week 4:
â–¡ Implement chunking strategies
  â–¡ HierarchicalChunker for legal documents
  â–¡ SemanticChunker for general text
  â–¡ Unit tests for all chunkers
```

### Phase 3: Pipeline Implementation (Week 5-8)
```python
Week 5: Law & Decree Pipelines
â–¡ LawPipeline
  â–¡ Metadata extraction
  â–¡ Hierarchy parsing
  â–¡ Extended metadata
  â–¡ Integration tests
  
â–¡ DecreePipeline
  â–¡ Similar to LawPipeline
  â–¡ Parent law detection
  
Week 6: Circular & Decision Pipelines
â–¡ CircularPipeline
â–¡ DecisionPipeline (NEW)
  
Week 7: Bidding & Report Pipelines
â–¡ BiddingPipeline (refactor existing)
â–¡ ReportPipeline (NEW)
  
Week 8: Exam Pipeline
â–¡ ExamPipeline (NEW)
  â–¡ Excel parsing
  â–¡ Question extraction
  â–¡ Answer key handling
```

### Phase 4: Enrichment & Quality (Week 9-10)
```python
Week 9:
â–¡ Semantic enrichment
  â–¡ Legal concept extraction
  â–¡ Named entity recognition
  â–¡ Keyword extraction
  
Week 10:
â–¡ Quality analysis
  â–¡ Completeness checker
  â–¡ Confidence scorer
  â–¡ Consistency validator
```

### Phase 5: Orchestration & Migration (Week 11-12)
```python
Week 11:
â–¡ Pipeline orchestrator
  â–¡ Auto document type detection
  â–¡ Parallel processing
  â–¡ Error recovery
  
Week 12:
â–¡ Data migration
  â–¡ Migrate existing 4 pipelines
  â–¡ Validate migrated data
  â–¡ Quality checks
```

### Phase 6: Testing & Documentation (Week 13-14)
```python
Week 13:
â–¡ End-to-end testing
  â–¡ Test all 7 pipelines
  â–¡ Performance benchmarks
  â–¡ Load testing
  
Week 14:
â–¡ Documentation
  â–¡ API documentation
  â–¡ User guide
  â–¡ Developer guide
â–¡ Production deployment
```

---

## ğŸ”„ MIGRATION STRATEGY

### Step 1: Parallel Run (Week 11)
```python
# Run both old and new pipelines
for document in documents:
    old_result = old_pipeline.process(document)
    new_result = new_pipeline.process(document)
    
    # Compare results
    differences = compare_results(old_result, new_result)
    
    # Log differences
    migration_logger.log(differences)
    
    # Use new result if validation passes
    if new_result.quality_metrics.validation_passed:
        use_result = new_result
    else:
        use_result = old_result
```

### Step 2: Gradual Cutover (Week 12)
```python
# Gradually increase traffic to new pipeline
traffic_split = {
    "old_pipeline": 80,  # Week 12, Day 1
    "new_pipeline": 20
}

# Monitor metrics
metrics = {
    "processing_time": [],
    "quality_scores": [],
    "error_rates": []
}

# Adjust split based on metrics
if metrics["error_rates"]["new_pipeline"] < 0.01:
    traffic_split["new_pipeline"] += 20
```

### Step 3: Full Cutover (Week 13)
```python
# Switch 100% to new pipeline
USE_NEW_PIPELINE = True

# Keep old pipeline for emergency rollback
ENABLE_ROLLBACK = True
ROLLBACK_THRESHOLD = 0.05  # 5% error rate
```

---

## ğŸ“Š MONITORING & METRICS

```python
# src/preprocessing/monitoring.py
class PipelineMonitor:
    """Monitor pipeline health and performance"""
    
    def __init__(self):
        self.metrics = {
            "documents_processed": 0,
            "chunks_created": 0,
            "avg_processing_time": 0.0,
            "avg_quality_score": 0.0,
            "error_count": 0,
            "validation_failures": 0,
        }
    
    def record_pipeline_run(self, result: PipelineResult):
        """Record metrics from pipeline run"""
        self.metrics["documents_processed"] += 1
        self.metrics["chunks_created"] += len(result.chunks)
        
        if result.success:
            avg_quality = sum(
                c.quality_metrics.completeness_score 
                for c in result.chunks
            ) / len(result.chunks)
            
            self.metrics["avg_quality_score"] = (
                self.metrics["avg_quality_score"] * 0.9 + avg_quality * 0.1
            )
        else:
            self.metrics["error_count"] += 1
    
    def get_dashboard_metrics(self) -> Dict:
        """Return metrics for dashboard"""
        return {
            "total_documents": self.metrics["documents_processed"],
            "total_chunks": self.metrics["chunks_created"],
            "success_rate": 1 - (self.metrics["error_count"] / max(self.metrics["documents_processed"], 1)),
            "avg_quality": self.metrics["avg_quality_score"],
            "health_status": self._get_health_status()
        }
    
    def _get_health_status(self) -> str:
        """Determine overall health status"""
        success_rate = 1 - (self.metrics["error_count"] / max(self.metrics["documents_processed"], 1))
        avg_quality = self.metrics["avg_quality_score"]
        
        if success_rate > 0.95 and avg_quality > 0.8:
            return "HEALTHY"
        elif success_rate > 0.9 and avg_quality > 0.7:
            return "WARNING"
        else:
            return "CRITICAL"
```

---

## ğŸ¯ SUCCESS CRITERIA

### Performance Metrics
- [ ] **Processing speed**: <100ms per chunk
- [ ] **Throughput**: 1000+ documents/hour
- [ ] **Memory**: <2GB for batch processing
- [ ] **Scalability**: Linear scaling with workers

### Quality Metrics
- [ ] **Validation success**: >98%
- [ ] **Completeness score**: >0.9
- [ ] **Confidence score**: >0.85
- [ ] **Data loss**: <1%

### Business Metrics
- [ ] **Coverage**: 100% of 7 document types
- [ ] **Consistency**: 100% unified schema adoption
- [ ] **Maintainability**: <50% code duplication
- [ ] **Extensibility**: New doc type in <1 week

---

## ğŸ” SECURITY & COMPLIANCE

```python
# src/preprocessing/security/

class DataProtection:
    """Protect sensitive data during preprocessing"""
    
    def __init__(self):
        self.encryption_key = self._load_encryption_key()
    
    def encrypt_sensitive_fields(self, chunk: UnifiedLegalChunk) -> UnifiedLegalChunk:
        """
        Encrypt sensitive fields.
        
        Sensitive fields:
        - Exam questions: correct_answer
        - Decisions: personal information
        """
        if chunk.document_info.doc_type == DocType.EXAM:
            # Encrypt answer key
            if "exam" in chunk.extended_metadata:
                chunk.extended_metadata["exam"]["correct_answer"] = self._encrypt(
                    chunk.extended_metadata["exam"]["correct_answer"]
                )
        
        return chunk
    
    def audit_log(self, operation: str, user: str, chunk_id: str):
        """Log all operations for audit trail"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "operation": operation,
            "user": user,
            "chunk_id": chunk_id
        }
        
        # Write to audit log
        self._write_audit_log(log_entry)
```

---

**Document Status**: âœ… Architecture Design Complete  
**Next Steps**: Begin Phase 1 Implementation  
**Dependencies**: DEEP_ANALYSIS_REPORT.md, Unified Schema

