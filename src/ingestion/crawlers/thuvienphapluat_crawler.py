#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Thuvienphapluat.vn Web Crawler Module

Module n√†y cung c·∫•p c√°c ch·ª©c nƒÉng ƒë·ªÉ crawl v√† tr√≠ch xu·∫•t n·ªôi dung t·ª´ website
thuvienphapluat.vn, sau ƒë√≥ export sang ƒë·ªãnh d·∫°ng Markdown.

Author: Generated from Jupyter Notebook
Date: 2025-09-29
"""

import argparse
import os
import re
import sys
import time
import requests
from datetime import datetime
from urllib.parse import urlparse
from bs4 import BeautifulSoup, NavigableString, Comment
from typing import Optional, List, Dict, Any


class ThuvienphapluatCrawler:
    """
    Class ch√≠nh ƒë·ªÉ crawl d·ªØ li·ªáu t·ª´ thuvienphapluat.vn
    """

    def __init__(
        self,
        user_agent: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        timeout: int = 30,
        default_output_dir: str = "app/data/crawler/outputs",
    ):
        """
        Kh·ªüi t·∫°o crawler

        Args:
            user_agent (str): User agent string ƒë·ªÉ tr√°nh b·ªã block
            timeout (int): Timeout cho requests (seconds)
            default_output_dir (str): Th∆∞ m·ª•c m·∫∑c ƒë·ªãnh ƒë·ªÉ l∆∞u file
        """
        self.headers = {"User-Agent": user_agent}
        self.timeout = timeout
        self.default_output_dir = default_output_dir

    def extract_content_from_div(self, content_div) -> str:
        """
        Tr√≠ch xu·∫•t n·ªôi dung t·ª´ div v√† chuy·ªÉn ƒë·ªïi sang markdown (phi√™n b·∫£n c∆° b·∫£n)

        Args:
            content_div: BeautifulSoup element ch·ª©a n·ªôi dung

        Returns:
            str: N·ªôi dung ƒë√£ chuy·ªÉn ƒë·ªïi sang markdown
        """
        if not content_div:
            return "Kh√¥ng t√¨m th·∫•y n·ªôi dung"

        markdown_content = []

        # Duy·ªát qua t·∫•t c·∫£ c√°c th·∫ª con
        for element in content_div.find_all(recursive=True):
            # B·ªè qua c√°c comment v√† navigable string
            if isinstance(element, (Comment, NavigableString)):
                continue

            # X·ª≠ l√Ω c√°c th·∫ª ti√™u ƒë·ªÅ
            if element.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
                level = int(element.name[1])
                text = element.get_text(strip=True)
                if text:
                    markdown_content.append("#" * level + " " + text + "\n")

            # X·ª≠ l√Ω th·∫ª paragraph
            elif element.name == "p":
                text = element.get_text(strip=True)
                if text:
                    markdown_content.append(text + "\n\n")

            # X·ª≠ l√Ω danh s√°ch c√≥ th·ª© t·ª±
            elif element.name == "ol":
                for i, li in enumerate(element.find_all("li"), 1):
                    text = li.get_text(strip=True)
                    if text:
                        markdown_content.append(f"{i}. {text}\n")
                markdown_content.append("\n")

            # X·ª≠ l√Ω danh s√°ch kh√¥ng th·ª© t·ª±
            elif element.name == "ul":
                for li in element.find_all("li"):
                    text = li.get_text(strip=True)
                    if text:
                        markdown_content.append(f"- {text}\n")
                markdown_content.append("\n")

            # X·ª≠ l√Ω c√°c th·∫ª strong/b
            elif element.name in ["strong", "b"]:
                text = element.get_text(strip=True)
                if text:
                    markdown_content.append(f"**{text}**")

            # X·ª≠ l√Ω c√°c th·∫ª em/i
            elif element.name in ["em", "i"]:
                text = element.get_text(strip=True)
                if text:
                    markdown_content.append(f"*{text}*")

            # X·ª≠ l√Ω th·∫ª div v·ªõi class ƒë·∫∑c bi·ªát
            elif element.name == "div":
                text = element.get_text(strip=True)
                # Ch·ªâ th√™m n·ªôi dung n·∫øu div kh√¥ng c√≥ th·∫ª con ph·ª©c t·∫°p
                if text and not element.find_all(
                    ["div", "p", "h1", "h2", "h3", "h4", "h5", "h6"]
                ):
                    markdown_content.append(text + "\n\n")

        return "".join(markdown_content)

    def extract_clean_content(self, content_div) -> str:
        """
        Tr√≠ch xu·∫•t n·ªôi dung s·∫°ch v√† chuy·ªÉn ƒë·ªïi sang markdown (phi√™n b·∫£n c·∫£i ti·∫øn)

        Args:
            content_div: BeautifulSoup element ch·ª©a n·ªôi dung

        Returns:
            str: N·ªôi dung ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† chuy·ªÉn ƒë·ªïi sang markdown
        """
        if not content_div:
            return "Kh√¥ng t√¨m th·∫•y n·ªôi dung"

        markdown_lines = []
        processed_elements = set()

        def clean_text(text):
            """L√†m s·∫°ch text, lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a"""
            return " ".join(text.split())

        def process_element(element, level=0):
            """X·ª≠ l√Ω t·ª´ng element m·ªôt c√°ch ƒë·ªá quy"""
            if element in processed_elements:
                return

            processed_elements.add(element)

            # X·ª≠ l√Ω ti√™u ƒë·ªÅ
            if element.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
                level_num = int(element.name[1])
                text = clean_text(element.get_text())
                if text and len(text) > 2:  # Ch·ªâ l·∫•y ti√™u ƒë·ªÅ c√≥ √Ω nghƒ©a
                    markdown_lines.append("#" * level_num + " " + text + "\n\n")

            # X·ª≠ l√Ω paragraph
            elif element.name == "p":
                text = clean_text(element.get_text())
                if text and len(text) > 10:  # Ch·ªâ l·∫•y ƒëo·∫°n vƒÉn c√≥ √Ω nghƒ©a
                    markdown_lines.append(text + "\n\n")

            # X·ª≠ l√Ω div c√≥ n·ªôi dung vƒÉn b·∫£n
            elif element.name == "div":
                # N·∫øu div kh√¥ng ch·ª©a c√°c th·∫ª block kh√°c, l·∫•y text
                if not element.find_all(
                    ["div", "p", "h1", "h2", "h3", "h4", "h5", "h6", "ul", "ol"]
                ):
                    text = clean_text(element.get_text())
                    if text and len(text) > 10:
                        markdown_lines.append(text + "\n\n")
                else:
                    # N·∫øu c√≥ th·∫ª con, x·ª≠ l√Ω ƒë·ªá quy
                    for child in element.children:
                        if hasattr(child, "name") and child.name:
                            process_element(child, level + 1)

            # X·ª≠ l√Ω danh s√°ch
            elif element.name == "ul":
                for li in element.find_all("li"):
                    text = clean_text(li.get_text())
                    if text:
                        markdown_lines.append(f"- {text}\n")
                markdown_lines.append("\n")

            elif element.name == "ol":
                for i, li in enumerate(element.find_all("li"), 1):
                    text = clean_text(li.get_text())
                    if text:
                        markdown_lines.append(f"{i}. {text}\n")
                markdown_lines.append("\n")

        # B·∫Øt ƒë·∫ßu x·ª≠ l√Ω t·ª´ root element
        process_element(content_div)

        # L√†m s·∫°ch k·∫øt qu·∫£
        content = "".join(markdown_lines)
        # Lo·∫°i b·ªè nhi·ªÅu d√≤ng tr·ªëng li√™n ti·∫øp
        content = re.sub(r"\n{3,}", "\n\n", content)

        return content.strip()

    def export_to_markdown(
        self, content: str, url: str, output_dir: Optional[str] = None
    ) -> Optional[str]:
        """
        Export n·ªôi dung ra file markdown

        Args:
            content (str): N·ªôi dung ƒë·ªÉ export
            url (str): URL g·ªëc
            output_dir (str, optional): Th∆∞ m·ª•c output. M·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng default_output_dir

        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ t·∫°o, ho·∫∑c None n·∫øu c√≥ l·ªói
        """
        if output_dir is None:
            output_dir = self.default_output_dir

        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs(output_dir, exist_ok=True)

        # T·∫°o t√™n file t·ª´ URL
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.split("/")

        # L·∫•y ph·∫ßn cu·ªëi c·ªßa URL l√†m t√™n file, lo·∫°i b·ªè extension
        filename_base = path_parts[-1].replace(".aspx", "").replace(".html", "")
        if not filename_base:
            filename_base = "crawled_content"

        # T·∫°o timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_base}_{timestamp}.md"
        filepath = os.path.join(output_dir, filename)

        # T·∫°o header cho file markdown
        header = f"""---
title: "N·ªôi dung t·ª´ {parsed_url.netloc}"
url: {url}
crawled_at: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
source: thuvienphapluat.vn
---

"""

        # Ghi file
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(header)
                f.write(content)

            print(f"‚úÖ ƒê√£ export th√†nh c√¥ng v√†o file: {filepath}")
            print(f"üìÑ K√≠ch th∆∞·ªõc file: {os.path.getsize(filepath):,} bytes")
            return filepath

        except Exception as e:
            print(f"‚ùå L·ªói khi export file: {str(e)}")
            return None

    def crawl_single_url(
        self, url: str, output_dir: Optional[str] = None
    ) -> Optional[str]:
        """
        Crawl m·ªôt URL ƒë∆°n l·∫ª v√† export sang markdown

        Args:
            url (str): URL c·∫ßn crawl
            output_dir (str, optional): Th∆∞ m·ª•c output

        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ t·∫°o, ho·∫∑c None n·∫øu c√≥ l·ªói
        """
        print(f"üîÑ B·∫Øt ƒë·∫ßu crawl: {url}")

        try:
            # 1. G·ª≠i request
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            print(f"üì° Status code: {response.status_code}")

            if response.status_code != 200:
                print(
                    f"‚ùå Kh√¥ng th·ªÉ truy c·∫≠p trang web. Status: {response.status_code}"
                )
                return None

            # 2. Parse HTML
            soup = BeautifulSoup(response.content, "html.parser")
            content_div = soup.find("div", class_="content1")

            if not content_div:
                print("‚ùå Kh√¥ng t√¨m th·∫•y div v·ªõi class 'content1'")
                return None

            print("‚úÖ T√¨m th·∫•y n·ªôi dung")

            # 3. Tr√≠ch xu·∫•t n·ªôi dung
            clean_content = self.extract_clean_content(content_div)
            print(f"üìù ƒê√£ tr√≠ch xu·∫•t {len(clean_content)} k√Ω t·ª±")

            # 4. Export ra markdown
            output_file = self.export_to_markdown(clean_content, url, output_dir)

            return output_file

        except requests.RequestException as e:
            print(f"‚ùå L·ªói khi request: {str(e)}")
            return None
        except Exception as e:
            print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}")
            return None

    def crawl_multiple_urls(
        self, urls: List[str], output_dir: Optional[str] = None, delay: int = 2
    ) -> List[Dict[str, Any]]:
        """
        Crawl nhi·ªÅu URL v·ªõi delay gi·ªØa c√°c request

        Args:
            urls (List[str]): Danh s√°ch c√°c URL c·∫ßn crawl
            output_dir (str, optional): Th∆∞ m·ª•c output
            delay (int): Delay gi·ªØa c√°c request (seconds)

        Returns:
            List[Dict]: Danh s√°ch k·∫øt qu·∫£ crawling
        """
        results = []
        total_urls = len(urls)

        print(f"üöÄ B·∫Øt ƒë·∫ßu crawl {total_urls} URL...")
        print(f"‚è±Ô∏è  Delay gi·ªØa c√°c request: {delay} gi√¢y")
        print("=" * 50)

        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{total_urls}] Processing: {url}")

            result = self.crawl_single_url(url, output_dir)
            results.append(
                {
                    "url": url,
                    "success": result is not None,
                    "output_file": result,
                    "index": i,
                }
            )

            # Delay gi·ªØa c√°c request ƒë·ªÉ tr√°nh b·ªã block
            if i < total_urls:
                print(f"‚è≥ Ch·ªù {delay} gi√¢y...")
                time.sleep(delay)

        # T√≥m t·∫Øt k·∫øt qu·∫£
        print("\n" + "=" * 50)
        print("üìä T·ªîNG K·∫æT:")
        successful = sum(1 for r in results if r["success"])
        failed = total_urls - successful

        print(f"‚úÖ Th√†nh c√¥ng: {successful}/{total_urls}")
        print(f"‚ùå Th·∫•t b·∫°i: {failed}/{total_urls}")

        if successful > 0:
            print(f"\nüìÅ C√°c file ƒë√£ t·∫°o:")
            for result in results:
                if result["success"]:
                    print(f"  - {result['output_file']}")

        return results


# Convenience functions ƒë·ªÉ s·ª≠ d·ª•ng d·ªÖ d√†ng h∆°n
def crawl_and_export_to_markdown(
    url: str, output_dir: str = "app/data/crawler/outputs/"
) -> Optional[str]:
    """
    Function ti·ªán √≠ch ƒë·ªÉ crawl m·ªôt URL v√† export sang markdown

    Args:
        url (str): URL c·∫ßn crawl
        output_dir (str): Th∆∞ m·ª•c output

    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ t·∫°o, ho·∫∑c None n·∫øu c√≥ l·ªói
    """
    crawler = ThuvienphapluatCrawler(default_output_dir=output_dir)
    return crawler.crawl_single_url(url, output_dir)


def batch_crawl_urls(
    urls: List[str], output_dir: str = "app/data/crawler/outputs/", delay: int = 2
) -> List[Dict[str, Any]]:
    """
    Function ti·ªán √≠ch ƒë·ªÉ crawl nhi·ªÅu URL

    Args:
        urls (List[str]): Danh s√°ch URL
        output_dir (str): Th∆∞ m·ª•c output
        delay (int): Delay gi·ªØa requests

    Returns:
        List[Dict]: K·∫øt qu·∫£ crawling
    """
    crawler = ThuvienphapluatCrawler(default_output_dir=output_dir)
    return crawler.crawl_multiple_urls(urls, output_dir, delay)


def build_arg_parser() -> argparse.ArgumentParser:
    """Create CLI parser for crawling script."""
    parser = argparse.ArgumentParser(
        description="Crawl n·ªôi dung t·ª´ thuvienphapluat.vn v√† l∆∞u v·ªÅ markdown"
    )
    parser.add_argument(
        "urls",
        nargs="+",
        help="URL ho·∫∑c danh s√°ch URL c·∫ßn crawl",
    )
    parser.add_argument(
        "-o",
        "--output-dir",
        default="app/data/crawler/outputs/",
        help="Th∆∞ m·ª•c l∆∞u file markdown (m·∫∑c ƒë·ªãnh: app/data/processed/)",
    )
    parser.add_argument(
        "--delay",
        type=int,
        default=2,
        help="Delay gi·ªØa c√°c request khi crawl nhi·ªÅu URL (gi√¢y)",
    )
    return parser


def main(argv: Optional[List[str]] = None) -> int:
    parser = build_arg_parser()
    args = parser.parse_args(argv)

    crawler = ThuvienphapluatCrawler(default_output_dir=args.output_dir)

    if len(args.urls) == 1:
        result = crawler.crawl_single_url(args.urls[0], args.output_dir)
        return 0 if result else 1

    results = crawler.crawl_multiple_urls(args.urls, args.output_dir, args.delay)
    return 0 if all(item["success"] for item in results) else 1


if __name__ == "__main__":
    sys.exit(main())
