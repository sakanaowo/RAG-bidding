"""
Intent Detection Module for RAG Pipeline

Detects query intent BEFORE attaching conversation context to avoid:
1. Gibberish queries polluting RAG with irrelevant context
2. Off-topic queries triggering unnecessary RAG retrieval
3. Casual queries being over-processed

Query Intents:
- CASUAL: Greetings, thanks, goodbyes
- ON_TOPIC: Valid questions about Ä‘áº¥u tháº§u
- OFF_TOPIC: Valid questions but not about Ä‘áº¥u tháº§u domain
- GIBBERISH: Random/meaningless text
- CONTEXT_FOLLOW_UP: References previous conversation ("nhÆ° Ä‘Ã£ nÃ³i", "á»Ÿ trÃªn")
"""

import re
import math
import logging
from enum import Enum
from dataclasses import dataclass
from typing import Optional, Tuple, List

logger = logging.getLogger(__name__)


class QueryIntent(Enum):
    """Classification of user query intent."""
    CASUAL = "casual"
    ON_TOPIC = "on_topic"
    OFF_TOPIC = "off_topic"
    GIBBERISH = "gibberish"
    CONTEXT_FOLLOW_UP = "context_follow_up"


@dataclass
class IntentResult:
    """Result of intent detection."""
    intent: QueryIntent
    confidence: float
    reason: str
    suggested_response: Optional[str] = None


class IntentDetector:
    """
    Detect user query intent for smarter RAG processing.
    
    Order of checks:
    1. Empty/too short â†’ GIBBERISH
    2. Entropy check â†’ GIBBERISH (random text)
    3. Casual patterns â†’ CASUAL
    4. Context follow-up patterns â†’ CONTEXT_FOLLOW_UP
    5. Domain keywords â†’ ON_TOPIC or OFF_TOPIC
    """
    
    # Äáº¥u tháº§u domain keywords (Vietnamese bidding law)
    DOMAIN_KEYWORDS = [
        # Core terms
        "Ä‘áº¥u tháº§u", "nhÃ  tháº§u", "gÃ³i tháº§u", "há»“ sÆ¡ má»i tháº§u", "há»“ sÆ¡ dá»± tháº§u",
        "chá»§ Ä‘áº§u tÆ°", "bÃªn má»i tháº§u", "Ä‘Ã¡nh giÃ¡ tháº§u", "xÃ©t tháº§u", "trÃºng tháº§u",
        "mua sáº¯m cÃ´ng", "mua sáº¯m táº­p trung", "Ä‘áº¥u giÃ¡", "chá»‰ Ä‘á»‹nh tháº§u",
        # Legal references
        "luáº­t Ä‘áº¥u tháº§u", "nghá»‹ Ä‘á»‹nh", "thÃ´ng tÆ°", "Ä‘iá»u", "khoáº£n", "Ä‘iá»ƒm",
        "quy Ä‘á»‹nh", "phÃ¡p luáº­t", "vÄƒn báº£n", "hÆ°á»›ng dáº«n",
        # Procedures
        "quy trÃ¬nh", "thá»§ tá»¥c", "há»“ sÆ¡", "Ä‘iá»u kiá»‡n", "tiÃªu chuáº©n", "tiÃªu chÃ­",
        "nÄƒng lá»±c", "kinh nghiá»‡m", "tÃ i chÃ­nh", "ká»¹ thuáº­t",
        # Entities
        "nhÃ  tháº§u phá»¥", "nhÃ  tháº§u chÃ­nh", "liÃªn danh", "tÆ° váº¥n", "giÃ¡m sÃ¡t",
        # Actions
        "ná»™p", "Ä‘Äƒng kÃ½", "tham gia", "khiáº¿u náº¡i", "xá»­ lÃ½ vi pháº¡m", "cháº¿ tÃ i",
        # E-procurement
        "há»‡ thá»‘ng máº¡ng", "Ä‘áº¥u tháº§u Ä‘iá»‡n tá»­", "Ä‘áº¥u tháº§u qua máº¡ng",
    ]
    
    # Context follow-up patterns (query references previous context)
    CONTEXT_PATTERNS = [
        r"(nhÆ°|giá»‘ng|tÆ°Æ¡ng tá»±)\s*(Ä‘Ã£\s*)?(nÃ³i|Ä‘á» cáº­p|trÃ¬nh bÃ y|giáº£i thÃ­ch)",
        r"(á»Ÿ|trong|táº¡i)\s*(trÃªn|trÆ°á»›c|pháº§n trÆ°á»›c)",
        r"(váº­y\s+thÃ¬|tháº¿\s+thÃ¬|nhÆ°\s+váº­y)",
        r"(tiáº¿p tá»¥c|nÃ³i\s+thÃªm|giáº£i thÃ­ch\s+rÃµ)",
        r"(cÃ¡i\s+)?Ä‘Ã³\s+(lÃ |cÃ³|Ä‘Æ°á»£c)",
        r"(Ä‘iá»u|khoáº£n|quy Ä‘á»‹nh)\s+(Ä‘Ã³|nÃ y|trÃªn)",
        r"^(rá»“i|váº­y|tháº¿|Ã |á»«|Ä‘Æ°á»£c)\s*\?*$",
        r"(chi tiáº¿t|cá»¥ thá»ƒ|rÃµ)\s+hÆ¡n",
        r"Ã½\s+(báº¡n|anh|chá»‹)\s+lÃ ",
        # NEW: Examples and clarifications
        r"vÃ­\s+dá»¥\s+(cá»¥\s+thá»ƒ|thá»±c\s+táº¿)?",  # "vÃ­ dá»¥", "vÃ­ dá»¥ cá»¥ thá»ƒ"
        r"(láº¥y|cho|nÃªu)\s+vÃ­\s+dá»¥",  # "láº¥y vÃ­ dá»¥", "cho vÃ­ dá»¥"
        r"cá»¥\s+thá»ƒ\s+(lÃ |nhÆ°)\s+(tháº¿\s+nÃ o|gÃ¬|sao)",  # "cá»¥ thá»ƒ lÃ  gÃ¬"
        r"^(nÃ³|Ä‘Ã³|váº­y|tháº¿)\s+(lÃ \s+)?(gÃ¬|sao)\s*\??$",  # "Ä‘Ã³ lÃ  gÃ¬", "nÃ³ lÃ  gÃ¬"
        r"(giáº£i\s+thÃ­ch|nÃ³i)\s+(rÃµ|ká»¹|thÃªm)",  # "giáº£i thÃ­ch rÃµ"
        r"(cÃ³\s+thá»ƒ\s+)?(cho\s+)?vÃ­\s+dá»¥",  # "cÃ³ thá»ƒ cho vÃ­ dá»¥"
    ]
    
    # Casual patterns (greetings, thanks, etc.)
    CASUAL_PATTERNS = {
        "greeting": [
            r"^(xin\s+)?chÃ o",
            r"^h+i+$",
            r"^h+e+l+o+$",
            r"^a+l+o+$",
            r"^hey+$",
            r"chÃ o\s+(buá»•i\s+)?(sÃ¡ng|trÆ°a|chiá»u|tá»‘i)",
            r"^good\s+(morning|afternoon|evening)$",
        ],
        "thanks": [
            r"(cáº£m|cÃ¡m)\s*Æ¡n",
            r"^thanks?$",
            r"^tks$",
            r"^ok\s+(cáº£m|cÃ¡m)\s*Æ¡n",
        ],
        "goodbye": [
            r"táº¡m\s+biá»‡t",
            r"^bye+$",
            r"^goodbye$",
            r"háº¹n\s+gáº·p\s+láº¡i",
        ],
        "identity": [
            r"báº¡n\s+lÃ \s+(ai|gÃ¬)",
            r"tÃªn\s+báº¡n",
            r"(ai|gÃ¬)\s+táº¡o\s+ra\s+báº¡n",
            r"báº¡n\s+(cÃ³\s+thá»ƒ\s+)?lÃ m\s+(Ä‘Æ°á»£c\s+)?gÃ¬",
        ],
        "confirmation": [
            r"^(ok|á»«|uh|Ä‘Æ°á»£c|rá»“i|vÃ¢ng|dáº¡|yes|no|khÃ´ng)$",
            r"^á»«$",  # Single char Vietnamese confirmation
        ],
    }
    
    # Direct responses for casual intents
    CASUAL_RESPONSES = {
        "greeting": "Xin chÃ o! ðŸ‘‹ TÃ´i lÃ  trá»£ lÃ½ chuyÃªn vá» phÃ¡p luáº­t Ä‘áº¥u tháº§u Viá»‡t Nam. Báº¡n cáº§n há»i gÃ¬ vá» Ä‘áº¥u tháº§u, tÃ´i sáºµn sÃ ng há»— trá»£!",
        "thanks": "KhÃ´ng cÃ³ gÃ¬! ðŸ˜Š Náº¿u báº¡n cÃ³ thÃªm cÃ¢u há»i vá» Ä‘áº¥u tháº§u, cá»© há»i nhÃ©!",
        "goodbye": "Táº¡m biá»‡t! ðŸ‘‹ Háº¹n gáº·p láº¡i báº¡n. ChÃºc báº¡n má»™t ngÃ y tá»‘t lÃ nh!",
        "identity": (
            "TÃ´i lÃ  trá»£ lÃ½ AI chuyÃªn vá» phÃ¡p luáº­t Ä‘áº¥u tháº§u Viá»‡t Nam. ðŸ“š\n\n"
            "TÃ´i cÃ³ thá»ƒ giÃºp báº¡n:\n"
            "- Tra cá»©u quy Ä‘á»‹nh trong Luáº­t Äáº¥u tháº§u, Nghá»‹ Ä‘á»‹nh, ThÃ´ng tÆ°\n"
            "- Giáº£i Ä‘Ã¡p tháº¯c máº¯c vá» quy trÃ¬nh Ä‘áº¥u tháº§u\n"
            "- TÃ¬m hiá»ƒu Ä‘iá»u kiá»‡n, tiÃªu chuáº©n cho nhÃ  tháº§u\n"
            "- HÆ°á»›ng dáº«n vá» há»“ sÆ¡ má»i tháº§u, Ä‘Ã¡nh giÃ¡ tháº§u\n\n"
            "HÃ£y Ä‘áº·t cÃ¢u há»i cá»¥ thá»ƒ vá» Ä‘áº¥u tháº§u Ä‘á»ƒ tÃ´i há»— trá»£ báº¡n!"
        ),
        "confirmation": "Báº¡n cÃ³ cÃ¢u há»i gÃ¬ khÃ¡c vá» Ä‘áº¥u tháº§u khÃ´ng? TÃ´i sáºµn sÃ ng há»— trá»£!",
    }
    
    # Gibberish response
    GIBBERISH_RESPONSE = (
        "Xin lá»—i, tÃ´i khÃ´ng hiá»ƒu cÃ¢u há»i cá»§a báº¡n. ðŸ¤”\n\n"
        "Báº¡n cÃ³ thá»ƒ Ä‘áº·t láº¡i cÃ¢u há»i rÃµ rÃ ng hÆ¡n vá» Ä‘áº¥u tháº§u Ä‘Æ°á»£c khÃ´ng?"
    )
    
    # Off-topic response
    OFF_TOPIC_RESPONSE = (
        "TÃ´i chá»‰ há»— trá»£ vá» phÃ¡p luáº­t Ä‘áº¥u tháº§u Viá»‡t Nam. ðŸ“‹\n\n"
        "Báº¡n cÃ³ thá»ƒ há»i tÃ´i vá»:\n"
        "- Quy trÃ¬nh Ä‘áº¥u tháº§u\n"
        "- Äiá»u kiá»‡n tham gia Ä‘áº¥u tháº§u\n"
        "- Há»“ sÆ¡ má»i tháº§u, Ä‘Ã¡nh giÃ¡ tháº§u\n"
        "- CÃ¡c quy Ä‘á»‹nh phÃ¡p luáº­t liÃªn quan"
    )
    
    def __init__(
        self,
        min_query_length: int = 3,  # Increased from 2 to filter 'ab' etc.
        max_entropy_threshold: float = 4.5,  # Higher = more random
        min_domain_confidence: float = 0.3,
    ):
        """
        Initialize IntentDetector.
        
        Args:
            min_query_length: Minimum characters for valid query
            max_entropy_threshold: Shannon entropy threshold for gibberish detection
            min_domain_confidence: Minimum keyword match ratio for on-topic
        """
        self.min_query_length = min_query_length
        self.max_entropy_threshold = max_entropy_threshold
        self.min_domain_confidence = min_domain_confidence
        
        # Compile regex patterns for performance
        self._context_patterns = [re.compile(p, re.IGNORECASE) for p in self.CONTEXT_PATTERNS]
        self._casual_patterns = {
            category: [re.compile(p, re.IGNORECASE) for p in patterns]
            for category, patterns in self.CASUAL_PATTERNS.items()
        }
    
    def detect(
        self,
        query: str,
        conversation_context: Optional[str] = None,
    ) -> IntentResult:
        """
        Detect the intent of a user query.
        
        Args:
            query: User query text
            conversation_context: Previous conversation context (optional)
        
        Returns:
            IntentResult with intent classification

        NOTE: conversation_context is provided for potential future use
        in context-aware intent detection, but currently not used.
        """
        if not query:
            return IntentResult(
                intent=QueryIntent.GIBBERISH,
                confidence=1.0,
                reason="Empty query",
                suggested_response=self.GIBBERISH_RESPONSE,
            )
        
        query_stripped = query.strip()
        query_lower = query_stripped.lower()
        
        # 1. Check casual patterns FIRST (before length check)
        # This allows single-char Vietnamese confirmations like "á»«" to be detected
        casual_result = self._check_casual(query_lower)
        if casual_result:
            return casual_result
        
        # 2. Check too short (after casual check)
        if len(query_stripped) < self.min_query_length:
            return IntentResult(
                intent=QueryIntent.GIBBERISH,
                confidence=0.9,
                reason=f"Query too short ({len(query_stripped)} chars)",
                suggested_response=self.GIBBERISH_RESPONSE,
            )
        
        # 3. Check gibberish (entropy-based and heuristics)
        if self._is_gibberish(query_stripped):
            return IntentResult(
                intent=QueryIntent.GIBBERISH,
                confidence=0.85,
                reason="High entropy / random text detected",
                suggested_response=self.GIBBERISH_RESPONSE,
            )
        
        # 4. Check context follow-up (before domain check)
        if self._is_context_follow_up(query_lower):
            return IntentResult(
                intent=QueryIntent.CONTEXT_FOLLOW_UP,
                confidence=0.8,
                reason="Query references previous context",
                suggested_response=None,  # Should use context
            )
        
        # 5. Check domain relevance
        domain_score = self._calculate_domain_score(query_lower)
        
        if domain_score >= self.min_domain_confidence:
            return IntentResult(
                intent=QueryIntent.ON_TOPIC,
                confidence=min(domain_score + 0.3, 1.0),
                reason=f"Domain keywords found (score: {domain_score:.2f})",
                suggested_response=None,  # Run RAG
            )
        
        # 6. If has Vietnamese question words but no domain keywords â†’ might be off-topic
        if self._has_question_pattern(query_lower) and domain_score < 0.1:
            return IntentResult(
                intent=QueryIntent.OFF_TOPIC,
                confidence=0.6,
                reason="Question pattern but no domain keywords",
                suggested_response=self.OFF_TOPIC_RESPONSE,
            )
        
        # 7. Default: Assume on-topic with lower confidence (let RAG handle it)
        return IntentResult(
            intent=QueryIntent.ON_TOPIC,
            confidence=0.5,
            reason="Default classification - no clear pattern",
            suggested_response=None,
        )
    
    def _is_gibberish(self, text: str) -> bool:
        """
        Detect gibberish using multiple heuristics.
        
        Heuristics:
        1. Shannon entropy (high entropy = random)
        2. Vowel ratio (Vietnamese should have ~40% vowels)
        3. Repeated character patterns
        4. No Vietnamese diacritics in long text
        5. All consonants check
        """
        # Check for repeated nonsense patterns first (quick check)
        if self._has_repeated_nonsense(text):
            logger.debug(f"Repeated nonsense pattern for '{text[:30]}...'")
            return True
        
        # Calculate Shannon entropy
        entropy = self._calculate_entropy(text)
        
        # Vietnamese text typically has entropy 3.5-4.2
        # Random Latin text has entropy around 3.8-4.3
        if entropy > self.max_entropy_threshold:
            logger.debug(f"High entropy detected: {entropy:.2f} for '{text[:30]}...'")
            return True
        
        # For longer text, check multiple heuristics
        if len(text) > 6:
            vowel_ratio = self._calculate_vowel_ratio(text)
            has_diacritics = self._has_vietnamese_diacritics(text)
            
            # Check if it's pure Latin text without Vietnamese characteristics
            # "iajsndijansd" has vowels (a, i) but looks random
            if not has_diacritics and self._is_pure_latin_gibberish(text):
                logger.debug(f"Latin gibberish detected for '{text[:30]}...'")
                return True
            
            # Pure consonants or very low vowel ratio = gibberish
            if vowel_ratio < 0.20:
                # If no Vietnamese diacritics and low vowels, likely gibberish
                if not has_diacritics:
                    logger.debug(f"Low vowel ratio ({vowel_ratio:.2f}) + no diacritics for '{text[:30]}...'")
                    return True
            
            # Check if text is only Latin letters without any Vietnamese characters
            # Vietnamese queries typically have at least some diacritics
            if len(text) > 10 and not has_diacritics:
                # Check if it looks like random keyboard mashing
                if self._looks_like_keyboard_mash(text):
                    logger.debug(f"Keyboard mash detected for '{text[:30]}...'")
                    return True
        
        return False
    
    def _is_pure_latin_gibberish(self, text: str) -> bool:
        """
        Detect pure Latin text that looks like random gibberish.
        
        Vietnamese text without diacritics still follows patterns:
        - Common word structures (consonant-vowel patterns)
        - Recognizable syllables
        - No unusual consonant clusters
        
        Random text lacks these patterns AND has unusual clusters.
        """
        text_lower = text.lower().replace(" ", "")
        
        if len(text_lower) < 7:
            return False
        
        # Check if all characters are basic Latin letters
        if not all(c.isalpha() and ord(c) < 128 for c in text_lower):
            return False  # Has non-Latin chars, let other checks handle
        
        # Check for unusual consonant clusters (not found in Vietnamese)
        # Vietnamese has max 2-3 consonant clusters: "tr", "th", "ch", "ng", "nh", "kh", "ph"
        consonants = set("bcdfghjklmnpqrstvwxz")
        
        # Find consonant cluster lengths
        max_cluster = 0
        current_cluster = 0
        cluster_count = 0
        
        for c in text_lower:
            if c in consonants:
                current_cluster += 1
            else:
                if current_cluster >= 3:
                    cluster_count += 1
                max_cluster = max(max_cluster, current_cluster)
                current_cluster = 0
        
        # Final cluster
        if current_cluster >= 3:
            cluster_count += 1
        max_cluster = max(max_cluster, current_cluster)
        
        # Vietnamese rarely has 3+ consonant clusters, never 4+
        if max_cluster >= 4:
            logger.debug(f"Long consonant cluster ({max_cluster}) detected")
            return True
        
        # Multiple 3-consonant clusters is suspicious
        if cluster_count >= 2:
            logger.debug(f"Multiple consonant clusters ({cluster_count}) detected")
            return True
        
        # Pattern-based check for shorter strings
        common_patterns = [
            # Common Vietnamese syllables (romanized)
            "an", "en", "in", "on", "un", "ang", "eng", "ong",
            "ai", "ao", "au", "ay", "eo", "ia", "ie", "iu", "oa", "oe", "oi", "ou", "ua", "ue", "ui", "uo", "uy",
            "anh", "inh", "ung", "uong",
            # Common Vietnamese word endings
            "nh", "ng", "ch",
            # Common Vietnamese beginnings  
            "th", "tr", "kh", "ph",
        ]
        
        pattern_count = sum(1 for p in common_patterns if p in text_lower)
        
        # Pattern density: patterns per character
        # Vietnamese text typically has pattern density > 0.15
        # Random text has lower density
        pattern_density = pattern_count / len(text_lower) if text_lower else 0
        
        # If a 10+ char text has low pattern density + consonant cluster, likely gibberish
        if len(text_lower) >= 10 and pattern_density < 0.15 and max_cluster >= 3:
            return True
        
        # If a 10+ char text has only 1-2 patterns, likely gibberish
        if len(text_lower) >= 10 and pattern_count <= 2:
            return True
        
        # If a 7-9 char text has 0 common patterns, likely gibberish
        if len(text_lower) >= 7 and pattern_count == 0:
            return True
        
        return False
    
    def _has_vietnamese_diacritics(self, text: str) -> bool:
        """Check if text contains Vietnamese diacritics."""
        diacritics = set("Ã Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘")
        return any(c in diacritics for c in text.lower())
    
    def _looks_like_keyboard_mash(self, text: str) -> bool:
        """
        Detect keyboard mashing patterns.
        
        Characteristics of keyboard mash:
        - Mix of random letters without structure
        - Often repeated key patterns
        - No meaningful Vietnamese words
        """
        text_lower = text.lower().replace(" ", "")
        
        if len(text_lower) < 6:
            return False
        
        # Check for keyboard row patterns (qwerty, asdf, etc.)
        keyboard_rows = ["qwertyuiop", "asdfghjkl", "zxcvbnm"]
        for row in keyboard_rows:
            row_chars = set(row)
            text_chars = set(text_lower)
            # If >70% chars from one keyboard row, likely mashing
            overlap = len(text_chars & row_chars) / len(text_chars) if text_chars else 0
            if overlap > 0.7 and len(text_lower) > 5:
                return True
        
        # Check for alternating patterns that aren't repeated (unlike asdasd)
        # Count unique consecutive pairs
        pairs = set()
        for i in range(len(text_lower) - 1):
            pairs.add(text_lower[i:i+2])
        
        # High pair diversity with no Vietnamese characteristics = gibberish
        pair_diversity = len(pairs) / (len(text_lower) - 1) if len(text_lower) > 1 else 0
        
        # Random text has high pair diversity (each pair unique)
        # Normal text has lower diversity (common letter combinations)
        if pair_diversity > 0.8 and len(text_lower) > 8:
            vowel_ratio = self._calculate_vowel_ratio(text)
            if vowel_ratio < 0.25:  # Low vowels + high diversity = gibberish
                return True
        
        return False
    
    def _calculate_entropy(self, text: str) -> float:
        """Calculate Shannon entropy of text."""
        if not text:
            return 0.0
        
        text_lower = text.lower()
        freq = {}
        for char in text_lower:
            freq[char] = freq.get(char, 0) + 1
        
        length = len(text_lower)
        entropy = 0.0
        for count in freq.values():
            p = count / length
            if p > 0:
                entropy -= p * math.log2(p)
        
        return entropy
    
    def _calculate_vowel_ratio(self, text: str) -> float:
        """Calculate ratio of vowels in text."""
        # Vietnamese vowels including diacritics
        vowels = set("aÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­eÃ¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡iÃ¬Ã­á»‰Ä©á»‹oÃ²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£uÃ¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±yá»³Ã½á»·á»¹á»µ")
        
        alpha_chars = [c for c in text.lower() if c.isalpha()]
        if not alpha_chars:
            return 0.0
        
        vowel_count = sum(1 for c in alpha_chars if c in vowels)
        return vowel_count / len(alpha_chars)
    
    def _has_repeated_nonsense(self, text: str) -> bool:
        """Check for repeated nonsense patterns like 'asdasd', 'qwqwqw'."""
        text_lower = text.lower().replace(" ", "")
        
        if len(text_lower) < 6:
            return False
        
        # Check for 2-3 char repeated patterns
        for pattern_len in [2, 3]:
            pattern = text_lower[:pattern_len]
            repeated = pattern * (len(text_lower) // pattern_len + 1)
            if text_lower in repeated:
                return True
        
        return False
    
    def _check_casual(self, query: str) -> Optional[IntentResult]:
        """Check if query matches casual patterns."""
        for category, patterns in self._casual_patterns.items():
            for pattern in patterns:
                if pattern.search(query):
                    return IntentResult(
                        intent=QueryIntent.CASUAL,
                        confidence=0.9,
                        reason=f"Matched casual pattern: {category}",
                        suggested_response=self.CASUAL_RESPONSES.get(category),
                    )
        return None
    
    def _is_context_follow_up(self, query: str) -> bool:
        """Check if query references previous context."""
        return any(pattern.search(query) for pattern in self._context_patterns)
    
    def _calculate_domain_score(self, query: str) -> float:
        """Calculate domain relevance score based on keyword matching."""
        if not query:
            return 0.0
        
        words = query.split()
        if not words:
            return 0.0
        
        matched_keywords = 0
        for keyword in self.DOMAIN_KEYWORDS:
            if keyword in query:
                matched_keywords += 1
        
        # Normalize by query length (longer queries need more keywords)
        # But cap at certain thresholds
        word_count = len(words)
        if word_count <= 5:
            threshold = 1
        elif word_count <= 15:
            threshold = 2
        else:
            threshold = 3
        
        return min(matched_keywords / threshold, 1.0)
    
    def _has_question_pattern(self, query: str) -> bool:
        """Check if query has Vietnamese question patterns."""
        question_patterns = [
            r"\?$",  # Ends with ?
            r"^(ai|gÃ¬|Ä‘Ã¢u|khi\s+nÃ o|bao\s+nhiÃªu|nhÆ°\s+tháº¿\s+nÃ o|táº¡i\s+sao|vÃ¬\s+sao)",
            r"(lÃ \s+gÃ¬|nhÆ°\s+tháº¿\s+nÃ o|ra\s+sao)",
            r"(cÃ³\s+thá»ƒ|Ä‘Æ°á»£c\s+khÃ´ng|pháº£i\s+khÃ´ng)",
        ]
        return any(re.search(p, query, re.IGNORECASE) for p in question_patterns)


# Singleton instance
_intent_detector: Optional[IntentDetector] = None


def get_intent_detector() -> IntentDetector:
    """Get singleton IntentDetector instance."""
    global _intent_detector
    if _intent_detector is None:
        _intent_detector = IntentDetector()
    return _intent_detector


# Quick helper function
def detect_intent(query: str, context: Optional[str] = None) -> IntentResult:
    """Quick function to detect query intent."""
    return get_intent_detector().detect(query, context)
