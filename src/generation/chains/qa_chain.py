import os
from typing import Dict, Literal
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import (
    RunnablePassthrough,
    RunnableLambda,
    RunnableParallel,
)
from src.generation.prompts.qa_prompts import (
    SYSTEM_PROMPT,
    SYSTEM_PROMPT_DETAILED,
    USER_TEMPLATE,
)
from src.retrieval.retrievers import create_retriever
from src.config.models import settings, apply_preset


model = ChatOpenAI(model=settings.llm_model, temperature=0)


def is_complex_query(question: str) -> bool:
    """
    Detect if query requires detailed analysis.

    Complex query indicators:
    - Contains keywords: ph√¢n t√≠ch, so s√°nh, t·ªïng h·ª£p, chi ti·∫øt, to√†n b·ªô
    - Multiple aspects (contains "v√†", "bao g·ªìm")
    - Long query (>100 chars)
    """
    question_lower = question.lower()

    # Keywords requiring detailed response
    detailed_keywords = [
        "ph√¢n t√≠ch",
        "so s√°nh",
        "t·ªïng h·ª£p",
        "chi ti·∫øt",
        "to√†n b·ªô",
        "ph√¢n bi·ªát",
        "kh√°c nhau",
        "gi·ªëng nhau",
        "∆∞u nh∆∞·ª£c ƒëi·ªÉm",
        "∆∞u ƒëi·ªÉm",
        "nh∆∞·ª£c ƒëi·ªÉm",
    ]

    # Check keywords
    if any(keyword in question_lower for keyword in detailed_keywords):
        return True

    # Check multiple aspects
    if ("bao g·ªìm" in question_lower or "v√†" in question_lower) and len(question) > 80:
        return True

    # Check length
    if len(question) > 150:
        return True

    return False


# Prompt will be created dynamically in answer() function
# prompt = ChatPromptTemplate.from_messages(
#     [("system", SYSTEM_PROMPT), ("user", USER_TEMPLATE)]
# )


def _get_document_statuses(docs) -> Dict[str, str]:
    """
    Get document statuses from documents table.

    This enriches retrieved documents with their validity status
    (active/expired/superseded) from the documents table.

    Args:
        docs: List of LangChain Documents with document_id in metadata

    Returns:
        Dict mapping document_id to status string
    """
    from src.models.base import SessionLocal
    from src.models.documents import Document
    import logging

    logger = logging.getLogger(__name__)
    statuses = {}

    # Extract unique document_ids from retrieved docs
    doc_ids = set()
    for d in docs:
        doc_id = d.metadata.get("document_id")
        if doc_id:
            doc_ids.add(doc_id)

    if not doc_ids:
        return statuses

    # Query documents table for statuses
    try:
        db = SessionLocal()
        try:
            # Query by document_id field (not UUID id)
            documents = (
                db.query(Document.document_id, Document.status)
                .filter(Document.document_id.in_(list(doc_ids)))
                .all()
            )

            for doc in documents:
                statuses[doc.document_id] = doc.status or "active"

            # Log if any expired documents found
            expired_docs = [d for d, s in statuses.items() if s != "active"]
            if expired_docs:
                logger.info(
                    f"üìã Found {len(expired_docs)} non-active documents in results: {expired_docs}"
                )

        finally:
            db.close()
    except Exception as e:
        logger.warning(f"Failed to fetch document statuses: {e}")

    return statuses


def fmt_docs(docs):
    lines = []
    for i, d in enumerate(docs, 1):
        lines.append(f"[#{i}]\n{d.page_content}\n")
    return "\n".join(lines)


def format_document_reference(doc, index: int, doc_status: str | None = None) -> str:
    """
    Format document reference v·ªõi th√¥ng tin chi ti·∫øt.

    Args:
        doc: LangChain Document
        index: Reference number
        doc_status: Status from documents table (active/expired/superseded)
    """
    meta = doc.metadata

    # L·∫•y th√¥ng tin c∆° b·∫£n
    title = meta.get("title", "T√†i li·ªáu")
    doc_type = meta.get("document_type", "")

    # Th√¥ng tin v·ªã tr√≠ trong t√†i li·ªáu
    hierarchy_parts = []

    # Th√™m ch∆∞∆°ng n·∫øu c√≥
    if meta.get("chuong"):
        hierarchy_parts.append(f"Ch∆∞∆°ng {meta['chuong']}")

    # Th√™m ƒëi·ªÅu
    if meta.get("dieu"):
        hierarchy_parts.append(f"ƒêi·ªÅu {meta['dieu']}")

    # Th√™m kho·∫£n n·∫øu c√≥
    if meta.get("khoan"):
        hierarchy_parts.append(f"Kho·∫£n {meta['khoan']}")

    # Th√™m ƒëi·ªÉm n·∫øu c√≥
    if meta.get("diem"):
        hierarchy_parts.append(f"ƒêi·ªÉm {meta['diem']}")

    # T·∫°o hierarchy string
    hierarchy = " ‚Üí ".join(hierarchy_parts) if hierarchy_parts else "N·ªôi dung chung"

    # URL ngu·ªìn n·∫øu c√≥
    url = meta.get("url", "")
    source_info = f"({url})" if url else ""

    # Preview n·ªôi dung
    content_preview = doc.page_content[:100].replace("\n", " ").strip()
    if len(doc.page_content) > 100:
        content_preview += "..."

    # Format final reference
    if doc_type:
        doc_type_str = f" - {doc_type}"
    else:
        doc_type_str = ""

    # Add status warning if document is not active
    status_warning = ""
    if doc_status and doc_status != "active":
        status_labels = {
            "expired": "‚ö†Ô∏è H·∫æT HI·ªÜU L·ª∞C",
            "superseded": "‚ö†Ô∏è ƒê√É ƒê∆Ø·ª¢C THAY TH·∫æ",
            "archived": "üìÅ ƒê√É L∆ØU TR·ªÆ",
            "draft": "üìù B·∫¢N NH√ÅP",
        }
        status_warning = f" {status_labels.get(doc_status, f'‚ö†Ô∏è {doc_status.upper()}')}"

    return (
        f"[#{index}] {hierarchy}{doc_type_str}{status_warning}\n    üìÑ {content_preview}\n    üîó {source_info}"
        if source_info
        else f"[#{index}] {hierarchy}{doc_type_str}{status_warning}\n    üìÑ {content_preview}"
    )


def answer(
    question: str,
    mode: str | None = None,
    reranker_type: Literal["bge", "openai"] = "openai",  # Default: OpenAI (API-based)
    filter_status: str | None = None,  # ‚ö†Ô∏è Deprecated - status not in embedding metadata
) -> Dict:
    """
    Answer a question using RAG pipeline.

    Args:
        question: User's question
        mode: RAG mode (fast/balanced/quality/adaptive)
        reranker_type: Reranker to use ("bge" or "openai")
        filter_status: ‚ö†Ô∏è DEPRECATED - Ignored. Status enrichment happens post-retrieval.

    Returns:
        Dict with answer, sources, and metadata
    """
    selected_mode = mode or settings.rag_mode or "balanced"
    apply_preset(selected_mode)

    # ‚úÖ Create retriever dynamically based on selected_mode and reranker_type
    enable_reranking = settings.enable_reranking and selected_mode != "fast"
    retriever = create_retriever(
        mode=selected_mode,
        enable_reranking=enable_reranking,
        reranker_type=reranker_type,
        # filter_status ignored - status enrichment happens post-retrieval
    )

    # ‚úÖ Select prompt based on query complexity
    use_detailed_prompt = is_complex_query(question)
    system_prompt = SYSTEM_PROMPT_DETAILED if use_detailed_prompt else SYSTEM_PROMPT

    import logging

    logger = logging.getLogger(__name__)
    if use_detailed_prompt:
        logger.info(
            "üîç Complex query detected ‚Üí Using DETAILED prompt for comprehensive analysis"
        )

    prompt = ChatPromptTemplate.from_messages(
        [("system", system_prompt), ("user", USER_TEMPLATE)]
    )

    # Build chain dynamically with the correct retriever and prompt
    rag_chain = (
        {"context": retriever | fmt_docs, "question": RunnablePassthrough()}
        | prompt
        | model
        | StrOutputParser()
    )

    chain = RunnableParallel(answer=rag_chain, source_documents=retriever)

    result = chain.invoke(question)

    # Enrich source documents with status from documents table
    doc_statuses = _get_document_statuses(result["source_documents"])

    # T·∫°o detailed source references
    src_lines = []
    detailed_sources = []
    has_expired_docs = False

    for i, d in enumerate(result["source_documents"], 1):
        # Get status from documents table (default to "active" if not found)
        doc_id = d.metadata.get("document_id", "")
        doc_status = doc_statuses.get(doc_id, "active")

        if doc_status != "active":
            has_expired_docs = True

        # T·∫°o reference chi ti·∫øt v·ªõi status
        detailed_ref = format_document_reference(d, i, doc_status)
        detailed_sources.append(detailed_ref)

        # T·∫°o source line ƒë∆°n gi·∫£n cho backward compatibility
        meta = d.metadata
        hierarchy_parts = []

        if meta.get("dieu"):
            hierarchy_parts.append(f"ƒêi·ªÅu {meta['dieu']}")
        if meta.get("khoan"):
            hierarchy_parts.append(f"Kho·∫£n {meta['khoan']}")
        if meta.get("diem"):
            hierarchy_parts.append(f"ƒêi·ªÉm {meta['diem']}")

        hierarchy = " ".join(hierarchy_parts) if hierarchy_parts else "VƒÉn b·∫£n"
        doc_title = meta.get("title", "T√†i li·ªáu ph√°p lu·∫≠t")

        # Add status to simple source line if not active
        status_suffix = f" [{doc_status.upper()}]" if doc_status != "active" else ""
        src_lines.append(f"[#{i}] {hierarchy} - {doc_title}{status_suffix}")

    # Build enhanced features list based on actual mode
    enhanced_features = []

    # Query Enhancement strategies (actual ones used)
    if selected_mode == "fast":
        # Fast mode: no enhancement
        pass
    elif selected_mode == "balanced":
        enhanced_features.append("Query Enhancement (Multi-Query, Step-Back)")
    elif selected_mode == "quality":
        enhanced_features.append(
            "Query Enhancement (Multi-Query, HyDE, Step-Back, Decomposition)"
        )
    elif selected_mode == "adaptive":
        enhanced_features.append("Query Enhancement (Multi-Query, Step-Back)")

    # RAG-Fusion (only quality mode)
    if selected_mode == "quality":
        enhanced_features.append("RAG-Fusion with RRF")

    # Adaptive K (only adaptive mode)
    if selected_mode == "adaptive":
        enhanced_features.append("Adaptive K Selection")

    # Document Reranking (all modes except fast)
    if selected_mode != "fast" and settings.enable_reranking:
        enhanced_features.append("Document Reranking (BGE)")

    # Add warning about expired documents in answer if needed
    answer_text = result["answer"].strip()
    if has_expired_docs:
        answer_text += "\n\n‚ö†Ô∏è **L∆∞u √Ω**: M·ªôt s·ªë t√†i li·ªáu tham kh·∫£o ƒë√£ h·∫øt hi·ªáu l·ª±c ho·∫∑c ƒë∆∞·ª£c thay th·∫ø. Vui l√≤ng ki·ªÉm tra vƒÉn b·∫£n hi·ªán h√†nh."

    return {
        "answer": answer_text,
        "sources": src_lines,
        "detailed_sources": detailed_sources,
        "source_documents_raw": [
            {
                "document_id": d.metadata.get("document_id", ""),
                "document_name": d.metadata.get(
                    "document_name", d.metadata.get("title", "T√†i li·ªáu")
                ),
                "chunk_id": d.metadata.get("chunk_id", ""),
                "content": d.page_content[:500],  # First 500 chars as citation
                "hierarchy": d.metadata.get("hierarchy", []),
                "section_title": d.metadata.get("section_title", ""),
                "document_type": d.metadata.get("document_type", ""),
                "category": d.metadata.get("category", ""),
                "dieu": d.metadata.get("dieu"),
                "khoan": d.metadata.get("khoan"),
                "diem": d.metadata.get("diem"),
                "status": doc_statuses.get(d.metadata.get("document_id", ""), "active"),
            }
            for d in result["source_documents"]
        ],
        "adaptive_retrieval": {
            "mode": selected_mode,
            "docs_retrieved": len(result["source_documents"]),
            "enhancement_enabled": selected_mode != "fast",
            "has_expired_docs": has_expired_docs,
        },
        "enhanced_features": enhanced_features,
        "document_statuses": doc_statuses,
    }
