"""
BGE Reranker for Vietnamese Legal Documents

S·ª≠ d·ª•ng BAAI/bge-reranker-v2-m3 - multilingual cross-encoder
ƒë√£ ƒë∆∞·ª£c fine-tuned cho reranking task.
"""

from sentence_transformers import CrossEncoder
from typing import List, Tuple, Optional
from langchain_core.documents import Document
import logging
import time
import torch
import threading

from .base_reranker import BaseReranker

logger = logging.getLogger(__name__)


# ===== SINGLETON PATTERN =====
# Global singleton instance v√† thread lock ƒë·ªÉ thread-safe
_reranker_instance: Optional["BGEReranker"] = None
_reranker_lock = threading.Lock()


def get_singleton_reranker(
    model_name: str = "BAAI/bge-reranker-v2-m3",
    device: str = "auto",
    max_length: int = 512,
    batch_size: int = 32,
) -> "BGEReranker":
    """
    Factory function ƒë·ªÉ l·∫•y singleton instance c·ªßa BGEReranker.

    Thread-safe implementation v·ªõi double-checked locking pattern.
    N·∫øu model ƒë√£ ƒë∆∞·ª£c load, s·∫Ω reuse instance thay v√¨ t·∫°o m·ªõi ‚Üí gi·∫£m memory.

    Args:
        model_name: Hugging Face model name (default: BAAI/bge-reranker-v2-m3)
        device: "auto", "cuda", ho·∫∑c "cpu"
        max_length: Max sequence length cho model
        batch_size: Batch size cho reranking (auto-adjust based on device)

    Returns:
        BGEReranker instance (singleton)

    Example:
        >>> reranker = get_singleton_reranker()  # L·∫ßn ƒë·∫ßu: load model (1.2GB)
        >>> reranker2 = get_singleton_reranker()  # L·∫ßn sau: reuse instance
        >>> assert reranker is reranker2  # True - c√πng instance
    """
    global _reranker_instance

    # Fast path: N·∫øu ƒë√£ c√≥ instance, return ngay (kh√¥ng c·∫ßn lock)
    if _reranker_instance is not None:
        return _reranker_instance

    # ‚úÖ Auto-detect device TR∆Ø·ªöC khi t·∫°o instance
    # CrossEncoder kh√¥ng ch·∫•p nh·∫≠n "auto", ch·ªâ ch·∫•p nh·∫≠n "cpu" ho·∫∑c "cuda"
    if device == "auto":
        try:
            if torch.cuda.is_available():
                device = "cuda"
                logger.info("üéÆ GPU detected! Using CUDA for acceleration")
            else:
                device = "cpu"
                logger.info("üíª No GPU detected, using CPU")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  CUDA check failed ({str(e)}), falling back to CPU")
            device = "cpu"

    # Slow path: T·∫°o instance m·ªõi (c·∫ßn lock)
    with _reranker_lock:
        # Double-check: C√≥ th·ªÉ thread kh√°c ƒë√£ t·∫°o xong trong l√∫c ch·ªù lock
        if _reranker_instance is None:
            logger.info(
                f"üîß Creating singleton BGEReranker instance "
                f"(model: {model_name}, device: {device})"
            )
            _reranker_instance = BGEReranker(
                model_name=model_name,
                device=device,  # Now guaranteed to be "cpu" or "cuda"
                max_length=max_length,
                batch_size=batch_size,
            )
        return _reranker_instance


def reset_singleton_reranker() -> None:
    """
    Reset singleton instance (CH·ªà d√πng cho testing).

    G·ªçi cleanup method n·∫øu c√≥, sau ƒë√≥ set instance v·ªÅ None.
    Cho ph√©p test cases t·∫°o reranker m·ªõi v·ªõi config kh√°c nhau.

    ‚ö†Ô∏è WARNING: KH√îNG g·ªçi trong production code!
    """
    global _reranker_instance

    with _reranker_lock:
        if _reranker_instance is not None:
            logger.warning("‚ö†Ô∏è Resetting singleton reranker (testing only)")
            # Cleanup n·∫øu c√≥ __del__ method
            if hasattr(_reranker_instance, "__del__"):
                _reranker_instance.__del__()
            _reranker_instance = None


class BGEReranker(BaseReranker):
    """
    BGE Multilingual Reranker cho vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam

    Default Model: BAAI/bge-reranker-v2-m3 ‚≠ê
    - ƒê√É FINE-TUNED cho reranking task
    - Multilingual (h·ªó tr·ª£ 180+ ng√¥n ng·ªØ, bao g·ªìm ti·∫øng Vi·ªát)
    - KH√îNG C√ì WARNING v·ªÅ uninitialized weights
    - Max sequence length: 512 tokens
    - State-of-the-art performance

    Alternative Models (n·∫øu c·∫ßn):
    - vinai/phobert-base-v2: Vietnamese-specific (ch∆∞a fine-tuned cho reranking)
    - vinai/phobert-large: Larger Vietnamese model (ch∆∞a fine-tuned)

    Performance:
    - Latency: ~100-150ms for 10 docs on CPU
    - Accuracy: Excellent cho ti·∫øng Vi·ªát v√† multilingual
    - Score separation: Clear distinction between relevant/irrelevant docs

    Note:
    - S·ª≠ d·ª•ng AutoTokenizer t·ª± ƒë·ªông (kh√¥ng xung ƒë·ªôt v·ªõi tiktoken cho chunking)
    - Device: cpu (c√≥ th·ªÉ chuy·ªÉn sang cuda n·∫øu c√≥ GPU ƒë·ªÉ tƒÉng t·ªëc)
    - Batch processing: H·ªó tr·ª£ batch inference ƒë·ªÉ t·ªëi ∆∞u throughput
    """

    # Model options for RERANKING
    BGE_RERANKER_M3 = "BAAI/bge-reranker-v2-m3"  # ‚≠ê DEFAULT (fine-tuned, multilingual)
    BGE_RERANKER_BASE = "BAAI/bge-reranker-base"  # Alternative BGE model
    PHOBERT_BASE = "vinai/phobert-base-v2"  # Vietnamese (not fine-tuned for reranking)

    # Note: huynguyen251/phobert-legal-qa-v2 is for QA task, NOT reranking
    # Use it separately in generation pipeline for answer extraction

    def __init__(
        self,
        model_name: str = BGE_RERANKER_M3,  # ‚≠ê Changed to fine-tuned model
        device: Optional[str] = None,  # ‚≠ê Auto-detect GPU
        max_length: int = 512,  # ‚≠ê BGE supports 512 tokens
        batch_size: int = 32,  # ‚≠ê Increased for GPU
        cache_dir: Optional[str] = None,
    ):
        """
        Args:
            model_name: Reranker model name
                Default: BAAI/bge-reranker-v2-m3 (fine-tuned, multilingual)
                Alternative: vinai/phobert-base-v2 (Vietnamese, not fine-tuned)
            device: "cpu", "cuda", or None (auto-detect GPU)
            max_length: Max tokens (BGE max = 512, PhoBERT max = 256)
            batch_size: Batch size for inference (32 for GPU, 16 for CPU)
            cache_dir: Model cache directory (default: ~/.cache/huggingface)
        """
        logger.info(f"üîß Initializing reranker: {model_name}")

        # Auto-detect device if not specified
        if device is None:
            try:
                if torch.cuda.is_available():
                    device = "cuda"
                    logger.info("üéÆ GPU detected! Using CUDA for acceleration")
                else:
                    device = "cpu"
                    logger.info("üíª No GPU detected, using CPU")
            except Exception as e:
                # Handle CUDA initialization errors gracefully
                logger.warning(f"‚ö†Ô∏è  CUDA check failed ({str(e)}), falling back to CPU")
                device = "cpu"

        self.model_name = model_name
        self.device = device
        self.max_length = max_length

        # Auto-adjust batch size for CPU
        if device == "cpu" and batch_size > 16:
            logger.info(f"‚öôÔ∏è  CPU detected, reducing batch_size from {batch_size} to 16")
            batch_size = 16

        self.batch_size = batch_size

        # Auto-adjust max_length based on model
        if "phobert" in model_name.lower() and max_length > 256:
            logger.warning(f"‚ö†Ô∏è  PhoBERT max length is 256, adjusting from {max_length}")
            self.max_length = 256

        # Load CrossEncoder (t·ª± ƒë·ªông load AutoTokenizer b√™n trong)
        try:
            self.model = CrossEncoder(
                model_name,
                device=device,
                max_length=self.max_length,
                model_kwargs={"cache_dir": cache_dir} if cache_dir else None,
            )
            logger.info(f"‚úÖ Model loaded on {device}")
            logger.info(f"üì¶ Max sequence length: {self.max_length} tokens")
        except Exception as e:
            logger.error(f"‚ùå Failed to load model: {e}")
            raise

    def rerank(
        self, query: str, documents: List[Document], top_k: int = 5
    ) -> List[Tuple[Document, float]]:
        """
        Rerank documents using BGE cross-encoder

        Args:
            query: User query (ti·∫øng Vi·ªát)
            documents: Retrieved documents
            top_k: Number of top documents to return

        Returns:
            List of (document, score) sorted by score descending
        """
        if not documents:
            logger.warning("‚ö†Ô∏è  Empty documents list")
            return []

        start_time = time.time()

        # Truncate n·∫øu c√≥ qu√° nhi·ªÅu docs (tr√°nh OOM)
        if len(documents) > 50:
            logger.warning(f"‚ö†Ô∏è  Too many docs ({len(documents)}), truncating to 50")
            documents = documents[:50]

        # Chu·∫©n b·ªã query-document pairs
        pairs = []
        for doc in documents:
            # Truncate content n·∫øu qu√° d√†i
            # BGE max 512 tokens, PhoBERT max 256 tokens
            # ∆Ø·ªõc t√≠nh: 1 token ‚âà 4 chars cho ti·∫øng Vi·ªát
            max_chars = (self.max_length - 50) * 4  # Reserve 50 tokens for query
            content = doc.page_content[:max_chars]
            pairs.append([query, content])

        # Predict relevance scores
        try:
            scores = self.model.predict(
                pairs, batch_size=self.batch_size, show_progress_bar=False
            )
        except Exception as e:
            logger.error(f"‚ùå Prediction error: {e}")
            # Fallback: return original order with dummy scores
            return [(doc, 1.0 - i * 0.1) for i, doc in enumerate(documents[:top_k])]

        # Zip documents with scores v√† sort
        doc_scores = list(zip(documents, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)

        # Log performance
        latency = (time.time() - start_time) * 1000
        top_score = doc_scores[0][1] if doc_scores else 0

        logger.info(
            f"üìä Reranked {len(documents)} docs in {latency:.1f}ms | "
            f"Top score: {top_score:.4f} | Returning top {top_k}"
        )

        # Debug: Log top 3 scores
        if logger.isEnabledFor(logging.DEBUG):
            for i, (doc, score) in enumerate(doc_scores[:3]):
                preview = doc.page_content[:80].replace("\n", " ")
                logger.debug(f"  [{i+1}] {score:.4f} - {preview}...")

        return doc_scores[:top_k]

    def rerank_batch(
        self, queries: List[str], documents_list: List[List[Document]], top_k: int = 5
    ) -> List[List[Tuple[Document, float]]]:
        """
        Batch reranking (t·ªëi ∆∞u h√≥a sau n·∫øu c·∫ßn)

        Hi·ªán t·∫°i: G·ªçi rerank() cho t·ª´ng query
        TODO: Implement true batch processing
        Hi·ªán t·∫°i API ch·ªâ x·ª≠ l√Ω 1 query/request ‚Üí kh√¥ng c·∫ßn batch
        """
        logger.info(f"üîÑ Batch reranking {len(queries)} queries...")

        results = []
        for query, docs in zip(queries, documents_list):
            result = self.rerank(query, docs, top_k)
            results.append(result)

        return results

    def __del__(self):
        """
        Cleanup method ƒë·ªÉ free GPU/CPU memory khi instance b·ªã destroy.

        G·ªçi torch.cuda.empty_cache() ƒë·ªÉ clear CUDA cache n·∫øu d√πng GPU.
        ƒê·∫£m b·∫£o model ƒë∆∞·ª£c unload khi kh√¥ng c√≤n d√πng (testing ho·∫∑c shutdown).
        """
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                logger.debug("üßπ Clearing CUDA cache for BGEReranker")
                torch.cuda.empty_cache()
        except Exception as e:
            # Ignore errors during cleanup (best effort)
            logger.warning(f"‚ö†Ô∏è Error during BGEReranker cleanup: {e}")
